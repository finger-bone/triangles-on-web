{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Triangles on Web This is a series of tutorial about computer graphics and WebGPU. What You Can Build 2D Basics In this section, we learn about the basic concepts of computer graphics and WebGPU. We do so by building a simple game called Life Game. Basic Setup Life Game Texture 3D Basics In this section, we learn about rendering 3D objects with WebGPU. We first learn about the basic concepts of 3D rendering. Orthographic Projection Perspective Projection Lighting WGSL Then we talk about the elephant in the room- the shader language, wgsl. WGSL 3D Models This part familiarize you with OBJ file format and how to load 3D models. OBJ Model Curves and Surfaces Next part is about curves and surfaces. B\u00e9zier Curve, B-spline Curve, and Surface","title":"Triangles on Web"},{"location":"#triangles-on-web","text":"This is a series of tutorial about computer graphics and WebGPU.","title":"Triangles on Web"},{"location":"#what-you-can-build","text":"","title":"What You Can Build"},{"location":"#2d-basics","text":"In this section, we learn about the basic concepts of computer graphics and WebGPU. We do so by building a simple game called Life Game. Basic Setup Life Game Texture","title":"2D Basics"},{"location":"#3d-basics","text":"In this section, we learn about rendering 3D objects with WebGPU. We first learn about the basic concepts of 3D rendering. Orthographic Projection Perspective Projection Lighting","title":"3D Basics"},{"location":"#wgsl","text":"Then we talk about the elephant in the room- the shader language, wgsl. WGSL","title":"WGSL"},{"location":"#3d-models","text":"This part familiarize you with OBJ file format and how to load 3D models. OBJ Model","title":"3D Models"},{"location":"#curves-and-surfaces","text":"Next part is about curves and surfaces. B\u00e9zier Curve, B-spline Curve, and Surface","title":"Curves and Surfaces"},{"location":"01/","text":"Triangles On Web Ch1 Draw Something This series introduces WebGPU, and computer graphics in general. Except for the basic knowledge of JS, no prior knowledge is needed. WebGPU is a relatively new API for the GPU. Albeit named as WebGPU, it can actually be considered a layer on top of Vulkan, DirectX 12, and Metal, OpenGL and WebGL. It is designed to be a low-level API, and is intended to be used for high-performance applications, such as games and simulations. In this chapter, we will draw something on the screen. The first part will refer to the Google Codelabs Tutorial . We will create a life game on the screen. Starting Point We will just create an empty vanilla JS project in vite with typescript enabled. Then clear all the extra codes, leaving only the main.ts . const main = async () => { console.log('Hello, world!') } main() Before actual coding, please check if your browser has WebGPU enabled. You can check it on WebGPU Samples . Chrome now defaults to enabled. On Safari, you should go to developer settings, flag settings and enable WebGPU. We also need to enable thee types for WebGPU, install @webgpu/types , and in tsc compiler options, add \"types\": [\"@webgpu/types\"] . Furthermore, we replace the <div id=\"app\"></div> with <canvas id=\"app\"></canvas> in the index.html . Drawing a Triangle There are many boilerplate codes to WebGPU, here is how it looks like. Requesting Device First we need access to the GPU. In WebGPU, it is done by the concept of an adapter , which is a bridge between the GPU and the browser. const adapter = await navigator.gpu.requestAdapter(); Then we need to request a device from the adapter. const device = await adapter.requestDevice(); console.log(device); Configure the Canvas We draw our triangle on the canvas. We need to get the canvas element and configure it. const canvas = document.getElementById('app') as HTMLCanvasElement; const context = canvas.getContext(\"webgpu\")!; const canvasFormat = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device: device, format: canvasFormat, }); Here, we use getContext to get relative information about the canvas. By specifying webgpu , we will get a context that is responsible for rendering with WebGPU. CanvasFormat is actually the color mode, for example, srgb . We usually just use the preferred format. Lastly, we configure the context with the device and the format. Understanding GPU Rendering Pipeline Before diving further into the engineering details, we first must understand how GPU handles rendering. The GPU rendering pipeline is a series of steps that the GPU takes to render an image. The application run on GPU is called a shader. The shader is a program that runs on the GPU. The shader has a special programming language that we will discuss later. The render pipeline has the following steps, CPU loads the data into the GPU. CPU may removed some invisible objects to save GPU resources. CPU sets all the colors, textures, and other data that the GPU needs to render the scene. CPU trigger a draw call to the GPU. GPU gets the data from the CPU and starts rendering the scene. GPU run into the geometry process, which processes the vertices of the scene. In the geometry process, the first step is the vertex shader, which processes the vertices of the scene. It may transform the vertices, change the color of the vertices, or do other things to the vertices. The next step is the tessellation shader, which processes the vertices of the scene. It performs subdivision of the vertices, whose purpose is to increase the detail of the scene. It also has many procedures but it's too complex to explain here. The next step is the geometry shader, which processes the vertices of the scene. In contrast to the vertex shader, where the developer could only define how one vertex is transformed, the geometry shader can define how multiple vertices are transformed. It can also create new vertices, which can be used to create new geometry. The last step of geometry process contains clipping, removing the undue parts that exceed the screen, and culling, removing the invisible parts that are not visible to the camera. The next step is the rasterization process, which converts the vertices into fragments. A fragment is a pixel that is going to be rendered on the screen. The next step is iteration of triangles, which iterates over the triangles of the scene. The next step is the fragment shader, which processes the fragments of the scene. It may change the color of the fragments, change the texture of the fragments, or do other things to the fragments. In this part, the depth test and stencil test are also performed. Depth test means to confer each fragment with the depth value, and the fragment with the smallest depth value will be rendered. Stencil test means to confer each fragment with the stencil value, and the fragment that passes the stencil test will be rendered. The stencil value is decided by the developer. The next step is the blending process, which blends the fragments of the scene. For example, if two fragments are overlapping, the blending process will blend the two fragments together. The last step is the output process, which outputs the fragments to the swap chain. The swap chain is a chain of images that are used to render the scene. To put it more simply, it is a buffer that holds the image that is going to be displayed on the screen. Depending on the primitives, the smallest unit that GPU can render, the pipeline may have different steps. Typically, we use triangles, which signals the GPU to treat every 3 group of vertices as a triangle. Creating Render Pass Render Pass is a step of the full GPU rendering. When a render pass is created, the GPU will start rendering the scene, and vice versa when it finishes. To create a render pass, we need to create an encoder that is responsible for compiling the render pass to GPU codes. const encoder = device.createCommandEncoder(); Then we create a render pass. const pass = encoder.beginRenderPass({ colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: \"clear\", storeOp: \"store\", }] }); Here, we create a render pass with a color attachment. Attachment is a concept in GPU that represents the image that is going to be rendered. An image may have many aspect which the GPU need to process, and each of them is an attachment. Here we only have one attachment, which is the color attachment. The view is the panel that the GPU will render on, here we set it to the texture of the canvas. loadOp is the operation that the GPU will do before the render pass, clear means GPU will first clear all the previously data from the last frame, and storeOp is the operation that the GPU will do after the render pass, store means GPU will store the data to the texture. loadOp can be load , which preserves the data from the last frame, or clear , which clears the data from the last frame. storeOp can be store , which stores the data to the texture, or discard , which discards the data. Now, just call pass.end() to end the render pass. Now, the command is saved in the command buffer of the GPU. To get the compiled command, use the following code, const commandBuffer = encoder.finish(); And, finally, submit the command to the render queue of the GPU. device.queue.submit([commandBuffer]); Now, you should see an ugly black canvas. Based on our stereotypical concepts about 3D, we would expect empty space to be a blue color. We can done that by setting the clear color. const pass = encoder.beginRenderPass({ colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: \"clear\", clearValue: { r: 0.1, g: 0.3, b: 0.8, a: 1.0 }, storeOp: \"store\", }] }); Drawing a Triangle Using Shader Now, we will draw a triangle on the canvas. We will use a shader to do that. The shader language will be wgsl, WebGPU Shading Language. Now, suppose we want to draw a triangle with the following coordinates, (-0.5, -0.5), (0.5, -0.5), (0.0, 0.5) As we stated before, to complete a render pipeline, we need a vertex shader and a fragment shader. Vertex Shader Use the following code to create shader modules. const cellShaderModule = device.createShaderModule({ label: \"shader\", code: ` // Shaders ` }); label here is simply a name, which is meant for debugging. code is the actual shader code. Vertex shader is a function that takes any parameter and returns the position of the vertex. However, contrary to what we might expect, the vertex shader returns a four dimensional vector, not a three dimensional vector. The fourth dimension is the w dimension, which is used for perspective division. We will discuss it later. Now, you can simply regard a four dimensional vector (x, y, z, w) as a three dimensional vector (x / w, y / w, z / w) . However, there is another problem- how to pass the data to the shader, and how to get the data out from the shader. To pass the data to the shader, we use the vertexBuffer , a buffer that contains the data of the vertices. We can create a buffer with the following code, const vertexBuffer = device.createBuffer({ size: 24, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, mappedAtCreation: true, }); Here we create a buffer with a size of 24 bytes, 6 floats, which is the size of the vertices. usage is the usage of the buffer, which is VERTEX for vertex data. GPUBufferUsage.COPY_DST means this buffer is valid as a copy destination. For all buffer whose data are written by the CPU, we need to set this flag. The map here means to map the buffer to the CPU, which means the CPU can read and write the buffer. The unmap means to unmap the buffer, which means the CPU can no longer read and write the buffer, and thus the content is available to the GPU. Now, we can write the data to the buffer. new Float32Array(vertexBuffer.getMappedRange()).set([ -0.5, -0.5, 0.5, -0.5, 0.0, 0.5, ]); vertexBuffer.unmap(); Here, we map the buffer to the CPU, and write the data to the buffer. Then we unmap the buffer. vertexBuffer.getMappedRange() will return the range of the buffer that is mapped to the CPU. We can use it to write the data to the buffer. However, these are just raw data, and the GPU doesn't know how to interpret them. We need to define the layout of the buffer. const vertexBufferLayout: GPUVertexBufferLayout = { arrayStride: 8, attributes: [{ format: \"float32x2\", offset: 0, shaderLocation: 0, }], }; Here, arrayStride is the number of bytes the GPU needs to skip forward in the buffer when it's looking for the next input. For example, if the arrayStride is 8, the GPU will skip 8 bytes to get the next input. Since here, we use float32x2 , the stride is 8 bytes, 4 bytes for each float, and 2 floats for each vertex. Now we can write the vertex shader. const shaderModule = device.createShaderModule({ label: \"shader\", code: ` @vertex fn vertexMain(@location(0) pos: vec2f) -> @builtin(position) vec4f { return vec4f(pos, 0, 1); } ` }); Here, @vertex means this is a vertex shader. @location(0) means the location of the attribute, which is 0, as previously defined. Please note that in the shader language, you are dealing with the layout of the buffer, so whenever you pass a value, you need to pass either a struct, whose fields had defined @location , or just a value with @location . vec2f is a two dimensional float vector, and vec4f is a four dimensional float vector. Since vertex shader is required to return a vec4f position, we need to annotate that with @builtin(position) . Fragment Shader Fragment shader, similarly, is something that takes the interpolated vertex output and output the attachments, color in this case. The interpolated means that although only certain pixel on the vertices have decided value, for every other pixel, the values are interpolated, either linear, averaged, or other means. The color of fragment is a four dimensional vector, which is the color of the fragment, respectively red, green, blue, and alpha. Please note that the color is in the range of 0 to 1, not 0 to 255. In addition that, fragment shader defines the color of every vertex, not the color of the triangle. The color of the triangle is determined by the color of the vertices, by interpolation. Since we currently does not bother to control the color of the fragment, we can simply return a constant color. const shaderModule = device.createShaderModule({ label: \"shader\", code: ` @vertex fn vertexMain(@location(0) pos: vec2f) -> @builtin(position) vec4f { return vec4f(pos, 0, 1); } @fragment fn fragmentMain() -> vec4<f32> { return vec4<f32>(1.0, 1.0, 0.0, 1.0); } ` }); Render Pipeline Then we define the customized render pipeline by replacing the vertex and fragment shader. const pipeline = device.createRenderPipeline({ label: \"pipeline\", layout: \"auto\", vertex: { module: shaderModule, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout] }, fragment: { module: shaderModule, entryPoint: \"fragmentMain\", targets: [{ format: canvasFormat }] } }); Note that in fragment shader, we need to specify the format of the target, which is the format of the canvas. Draw Call Before render pass ends, we add the draw call. pass.setPipeline(pipeline); pass.setVertexBuffer(0, vertexBuffer); pass.draw(3); Here, in setVertexBuffer , the first parameter is the index of the buffer, in the pipeline definition field buffers , and the second parameter is the buffer itself. When calling draw , the parameter is the number of vertices to draw. Since we have 3 vertices, we draw 3. Now, you should see a yellow triangle on the canvas. Draw Life Game Cells Now we tweak our codes a bit- since we want to build a life game, so we need to draw squares instead of triangles. A square is actually two triangles, so we need to draw 6 vertices. The changes here are simple and you don't need a detailed explanation. const main = async () => { const adapter = await navigator.gpu.requestAdapter(); if (!adapter) { console.error('WebGPU not supported'); return; } const device = await adapter.requestDevice(); console.log(device); const canvas = document.getElementById('app') as HTMLCanvasElement; const context = canvas.getContext(\"webgpu\")!; const canvasFormat = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device: device, format: canvasFormat, }); const vertices = [ 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, ] const vertexBuffer = device.createBuffer({ size: vertices.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, mappedAtCreation: true, }); new Float32Array(vertexBuffer.getMappedRange()).set(vertices); const vertexBufferLayout: GPUVertexBufferLayout = { arrayStride: 8, attributes: [{ format: \"float32x2\", offset: 0, shaderLocation: 0, }], }; const shaderModule = device.createShaderModule({ label: \"shader\", code: ` @vertex fn vertexMain(@location(0) pos: vec2f) -> @builtin(position) vec4f { return vec4f(pos, 0, 1); } @fragment fn fragmentMain() -> vec4<f32> { return vec4<f32>(1.0, 1.0, 0.0, 1.0); } ` }); vertexBuffer.unmap(); const pipeline = device.createRenderPipeline({ label: \"Cell pipeline\", layout: \"auto\", vertex: { module: shaderModule, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout] }, fragment: { module: shaderModule, entryPoint: \"fragmentMain\", targets: [{ format: canvasFormat }] } }); const encoder = device.createCommandEncoder(); const pass = encoder.beginRenderPass({ colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: \"clear\", clearValue: { r: 0.1, g: 0.3, b: 0.8, a: 1.0 }, storeOp: \"store\", }] }); pass.setPipeline(pipeline); pass.setVertexBuffer(0, vertexBuffer); pass.draw(vertices.length / 2); pass.end(); const commandBuffer = encoder.finish(); device.queue.submit([commandBuffer]); } Now, you should see a yellow square on the canvas. Coordinate System We didn't discuss the coordinate system of the GPU. It is, well, rather simple. The actual coordinate system of the GPU is a right-handed coordinate system, which means the x-axis points to the right, the y-axis points up, and the z-axis points out of the screen. The range of the coordinate system is from -1 to 1. The origin is at the center of the screen. z-axis is from 0 to 1, 0 is the near plane, and 1 is the far plane. However, z-axis is for depth. When you do 3D rendering, you can not just use z-axis to determine the position of the object, you need to use the perspective division. This is called the NDC, normalized device coordinate. For example, if you want to draw a square at the top left corner of the screen, the vertices are (-1, 1), (-1, 0), (0, 1), (0, 0), though you need to use two triangles to draw it.","title":"Basic Setup"},{"location":"01/#triangles-on-web-ch1-draw-something","text":"This series introduces WebGPU, and computer graphics in general. Except for the basic knowledge of JS, no prior knowledge is needed. WebGPU is a relatively new API for the GPU. Albeit named as WebGPU, it can actually be considered a layer on top of Vulkan, DirectX 12, and Metal, OpenGL and WebGL. It is designed to be a low-level API, and is intended to be used for high-performance applications, such as games and simulations. In this chapter, we will draw something on the screen. The first part will refer to the Google Codelabs Tutorial . We will create a life game on the screen.","title":"Triangles On Web Ch1 Draw Something"},{"location":"01/#starting-point","text":"We will just create an empty vanilla JS project in vite with typescript enabled. Then clear all the extra codes, leaving only the main.ts . const main = async () => { console.log('Hello, world!') } main() Before actual coding, please check if your browser has WebGPU enabled. You can check it on WebGPU Samples . Chrome now defaults to enabled. On Safari, you should go to developer settings, flag settings and enable WebGPU. We also need to enable thee types for WebGPU, install @webgpu/types , and in tsc compiler options, add \"types\": [\"@webgpu/types\"] . Furthermore, we replace the <div id=\"app\"></div> with <canvas id=\"app\"></canvas> in the index.html .","title":"Starting Point"},{"location":"01/#drawing-a-triangle","text":"There are many boilerplate codes to WebGPU, here is how it looks like.","title":"Drawing a Triangle"},{"location":"01/#requesting-device","text":"First we need access to the GPU. In WebGPU, it is done by the concept of an adapter , which is a bridge between the GPU and the browser. const adapter = await navigator.gpu.requestAdapter(); Then we need to request a device from the adapter. const device = await adapter.requestDevice(); console.log(device);","title":"Requesting Device"},{"location":"01/#configure-the-canvas","text":"We draw our triangle on the canvas. We need to get the canvas element and configure it. const canvas = document.getElementById('app') as HTMLCanvasElement; const context = canvas.getContext(\"webgpu\")!; const canvasFormat = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device: device, format: canvasFormat, }); Here, we use getContext to get relative information about the canvas. By specifying webgpu , we will get a context that is responsible for rendering with WebGPU. CanvasFormat is actually the color mode, for example, srgb . We usually just use the preferred format. Lastly, we configure the context with the device and the format.","title":"Configure the Canvas"},{"location":"01/#understanding-gpu-rendering-pipeline","text":"Before diving further into the engineering details, we first must understand how GPU handles rendering. The GPU rendering pipeline is a series of steps that the GPU takes to render an image. The application run on GPU is called a shader. The shader is a program that runs on the GPU. The shader has a special programming language that we will discuss later. The render pipeline has the following steps, CPU loads the data into the GPU. CPU may removed some invisible objects to save GPU resources. CPU sets all the colors, textures, and other data that the GPU needs to render the scene. CPU trigger a draw call to the GPU. GPU gets the data from the CPU and starts rendering the scene. GPU run into the geometry process, which processes the vertices of the scene. In the geometry process, the first step is the vertex shader, which processes the vertices of the scene. It may transform the vertices, change the color of the vertices, or do other things to the vertices. The next step is the tessellation shader, which processes the vertices of the scene. It performs subdivision of the vertices, whose purpose is to increase the detail of the scene. It also has many procedures but it's too complex to explain here. The next step is the geometry shader, which processes the vertices of the scene. In contrast to the vertex shader, where the developer could only define how one vertex is transformed, the geometry shader can define how multiple vertices are transformed. It can also create new vertices, which can be used to create new geometry. The last step of geometry process contains clipping, removing the undue parts that exceed the screen, and culling, removing the invisible parts that are not visible to the camera. The next step is the rasterization process, which converts the vertices into fragments. A fragment is a pixel that is going to be rendered on the screen. The next step is iteration of triangles, which iterates over the triangles of the scene. The next step is the fragment shader, which processes the fragments of the scene. It may change the color of the fragments, change the texture of the fragments, or do other things to the fragments. In this part, the depth test and stencil test are also performed. Depth test means to confer each fragment with the depth value, and the fragment with the smallest depth value will be rendered. Stencil test means to confer each fragment with the stencil value, and the fragment that passes the stencil test will be rendered. The stencil value is decided by the developer. The next step is the blending process, which blends the fragments of the scene. For example, if two fragments are overlapping, the blending process will blend the two fragments together. The last step is the output process, which outputs the fragments to the swap chain. The swap chain is a chain of images that are used to render the scene. To put it more simply, it is a buffer that holds the image that is going to be displayed on the screen. Depending on the primitives, the smallest unit that GPU can render, the pipeline may have different steps. Typically, we use triangles, which signals the GPU to treat every 3 group of vertices as a triangle.","title":"Understanding GPU Rendering Pipeline"},{"location":"01/#creating-render-pass","text":"Render Pass is a step of the full GPU rendering. When a render pass is created, the GPU will start rendering the scene, and vice versa when it finishes. To create a render pass, we need to create an encoder that is responsible for compiling the render pass to GPU codes. const encoder = device.createCommandEncoder(); Then we create a render pass. const pass = encoder.beginRenderPass({ colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: \"clear\", storeOp: \"store\", }] }); Here, we create a render pass with a color attachment. Attachment is a concept in GPU that represents the image that is going to be rendered. An image may have many aspect which the GPU need to process, and each of them is an attachment. Here we only have one attachment, which is the color attachment. The view is the panel that the GPU will render on, here we set it to the texture of the canvas. loadOp is the operation that the GPU will do before the render pass, clear means GPU will first clear all the previously data from the last frame, and storeOp is the operation that the GPU will do after the render pass, store means GPU will store the data to the texture. loadOp can be load , which preserves the data from the last frame, or clear , which clears the data from the last frame. storeOp can be store , which stores the data to the texture, or discard , which discards the data. Now, just call pass.end() to end the render pass. Now, the command is saved in the command buffer of the GPU. To get the compiled command, use the following code, const commandBuffer = encoder.finish(); And, finally, submit the command to the render queue of the GPU. device.queue.submit([commandBuffer]); Now, you should see an ugly black canvas. Based on our stereotypical concepts about 3D, we would expect empty space to be a blue color. We can done that by setting the clear color. const pass = encoder.beginRenderPass({ colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: \"clear\", clearValue: { r: 0.1, g: 0.3, b: 0.8, a: 1.0 }, storeOp: \"store\", }] });","title":"Creating Render Pass"},{"location":"01/#drawing-a-triangle-using-shader","text":"Now, we will draw a triangle on the canvas. We will use a shader to do that. The shader language will be wgsl, WebGPU Shading Language. Now, suppose we want to draw a triangle with the following coordinates, (-0.5, -0.5), (0.5, -0.5), (0.0, 0.5) As we stated before, to complete a render pipeline, we need a vertex shader and a fragment shader.","title":"Drawing a Triangle Using Shader"},{"location":"01/#vertex-shader","text":"Use the following code to create shader modules. const cellShaderModule = device.createShaderModule({ label: \"shader\", code: ` // Shaders ` }); label here is simply a name, which is meant for debugging. code is the actual shader code. Vertex shader is a function that takes any parameter and returns the position of the vertex. However, contrary to what we might expect, the vertex shader returns a four dimensional vector, not a three dimensional vector. The fourth dimension is the w dimension, which is used for perspective division. We will discuss it later. Now, you can simply regard a four dimensional vector (x, y, z, w) as a three dimensional vector (x / w, y / w, z / w) . However, there is another problem- how to pass the data to the shader, and how to get the data out from the shader. To pass the data to the shader, we use the vertexBuffer , a buffer that contains the data of the vertices. We can create a buffer with the following code, const vertexBuffer = device.createBuffer({ size: 24, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, mappedAtCreation: true, }); Here we create a buffer with a size of 24 bytes, 6 floats, which is the size of the vertices. usage is the usage of the buffer, which is VERTEX for vertex data. GPUBufferUsage.COPY_DST means this buffer is valid as a copy destination. For all buffer whose data are written by the CPU, we need to set this flag. The map here means to map the buffer to the CPU, which means the CPU can read and write the buffer. The unmap means to unmap the buffer, which means the CPU can no longer read and write the buffer, and thus the content is available to the GPU. Now, we can write the data to the buffer. new Float32Array(vertexBuffer.getMappedRange()).set([ -0.5, -0.5, 0.5, -0.5, 0.0, 0.5, ]); vertexBuffer.unmap(); Here, we map the buffer to the CPU, and write the data to the buffer. Then we unmap the buffer. vertexBuffer.getMappedRange() will return the range of the buffer that is mapped to the CPU. We can use it to write the data to the buffer. However, these are just raw data, and the GPU doesn't know how to interpret them. We need to define the layout of the buffer. const vertexBufferLayout: GPUVertexBufferLayout = { arrayStride: 8, attributes: [{ format: \"float32x2\", offset: 0, shaderLocation: 0, }], }; Here, arrayStride is the number of bytes the GPU needs to skip forward in the buffer when it's looking for the next input. For example, if the arrayStride is 8, the GPU will skip 8 bytes to get the next input. Since here, we use float32x2 , the stride is 8 bytes, 4 bytes for each float, and 2 floats for each vertex. Now we can write the vertex shader. const shaderModule = device.createShaderModule({ label: \"shader\", code: ` @vertex fn vertexMain(@location(0) pos: vec2f) -> @builtin(position) vec4f { return vec4f(pos, 0, 1); } ` }); Here, @vertex means this is a vertex shader. @location(0) means the location of the attribute, which is 0, as previously defined. Please note that in the shader language, you are dealing with the layout of the buffer, so whenever you pass a value, you need to pass either a struct, whose fields had defined @location , or just a value with @location . vec2f is a two dimensional float vector, and vec4f is a four dimensional float vector. Since vertex shader is required to return a vec4f position, we need to annotate that with @builtin(position) .","title":"Vertex Shader"},{"location":"01/#fragment-shader","text":"Fragment shader, similarly, is something that takes the interpolated vertex output and output the attachments, color in this case. The interpolated means that although only certain pixel on the vertices have decided value, for every other pixel, the values are interpolated, either linear, averaged, or other means. The color of fragment is a four dimensional vector, which is the color of the fragment, respectively red, green, blue, and alpha. Please note that the color is in the range of 0 to 1, not 0 to 255. In addition that, fragment shader defines the color of every vertex, not the color of the triangle. The color of the triangle is determined by the color of the vertices, by interpolation. Since we currently does not bother to control the color of the fragment, we can simply return a constant color. const shaderModule = device.createShaderModule({ label: \"shader\", code: ` @vertex fn vertexMain(@location(0) pos: vec2f) -> @builtin(position) vec4f { return vec4f(pos, 0, 1); } @fragment fn fragmentMain() -> vec4<f32> { return vec4<f32>(1.0, 1.0, 0.0, 1.0); } ` });","title":"Fragment Shader"},{"location":"01/#render-pipeline","text":"Then we define the customized render pipeline by replacing the vertex and fragment shader. const pipeline = device.createRenderPipeline({ label: \"pipeline\", layout: \"auto\", vertex: { module: shaderModule, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout] }, fragment: { module: shaderModule, entryPoint: \"fragmentMain\", targets: [{ format: canvasFormat }] } }); Note that in fragment shader, we need to specify the format of the target, which is the format of the canvas.","title":"Render Pipeline"},{"location":"01/#draw-call","text":"Before render pass ends, we add the draw call. pass.setPipeline(pipeline); pass.setVertexBuffer(0, vertexBuffer); pass.draw(3); Here, in setVertexBuffer , the first parameter is the index of the buffer, in the pipeline definition field buffers , and the second parameter is the buffer itself. When calling draw , the parameter is the number of vertices to draw. Since we have 3 vertices, we draw 3. Now, you should see a yellow triangle on the canvas.","title":"Draw Call"},{"location":"01/#draw-life-game-cells","text":"Now we tweak our codes a bit- since we want to build a life game, so we need to draw squares instead of triangles. A square is actually two triangles, so we need to draw 6 vertices. The changes here are simple and you don't need a detailed explanation. const main = async () => { const adapter = await navigator.gpu.requestAdapter(); if (!adapter) { console.error('WebGPU not supported'); return; } const device = await adapter.requestDevice(); console.log(device); const canvas = document.getElementById('app') as HTMLCanvasElement; const context = canvas.getContext(\"webgpu\")!; const canvasFormat = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device: device, format: canvasFormat, }); const vertices = [ 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, ] const vertexBuffer = device.createBuffer({ size: vertices.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, mappedAtCreation: true, }); new Float32Array(vertexBuffer.getMappedRange()).set(vertices); const vertexBufferLayout: GPUVertexBufferLayout = { arrayStride: 8, attributes: [{ format: \"float32x2\", offset: 0, shaderLocation: 0, }], }; const shaderModule = device.createShaderModule({ label: \"shader\", code: ` @vertex fn vertexMain(@location(0) pos: vec2f) -> @builtin(position) vec4f { return vec4f(pos, 0, 1); } @fragment fn fragmentMain() -> vec4<f32> { return vec4<f32>(1.0, 1.0, 0.0, 1.0); } ` }); vertexBuffer.unmap(); const pipeline = device.createRenderPipeline({ label: \"Cell pipeline\", layout: \"auto\", vertex: { module: shaderModule, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout] }, fragment: { module: shaderModule, entryPoint: \"fragmentMain\", targets: [{ format: canvasFormat }] } }); const encoder = device.createCommandEncoder(); const pass = encoder.beginRenderPass({ colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: \"clear\", clearValue: { r: 0.1, g: 0.3, b: 0.8, a: 1.0 }, storeOp: \"store\", }] }); pass.setPipeline(pipeline); pass.setVertexBuffer(0, vertexBuffer); pass.draw(vertices.length / 2); pass.end(); const commandBuffer = encoder.finish(); device.queue.submit([commandBuffer]); } Now, you should see a yellow square on the canvas.","title":"Draw Life Game Cells"},{"location":"01/#coordinate-system","text":"We didn't discuss the coordinate system of the GPU. It is, well, rather simple. The actual coordinate system of the GPU is a right-handed coordinate system, which means the x-axis points to the right, the y-axis points up, and the z-axis points out of the screen. The range of the coordinate system is from -1 to 1. The origin is at the center of the screen. z-axis is from 0 to 1, 0 is the near plane, and 1 is the far plane. However, z-axis is for depth. When you do 3D rendering, you can not just use z-axis to determine the position of the object, you need to use the perspective division. This is called the NDC, normalized device coordinate. For example, if you want to draw a square at the top left corner of the screen, the vertices are (-1, 1), (-1, 0), (0, 1), (0, 0), though you need to use two triangles to draw it.","title":"Coordinate System"},{"location":"02/","text":"Triangles On Web Ch2 Creating the Life Game We will build a cellular automaton that simulates the game of life. The game of life is a zero-player game, meaning that its evolution is determined by its initial state, requiring no further input. One interacts with the Game of Life by creating an initial configuration and observing how it evolves. In the last chapter, we learnt about WebGPU basics, and now, we will use that knowledge to create a simple game of life simulation. We will use the GPU to perform the calculations and render the game of life. The Game of Life The Game of Life is a cellular automaton devised by the British mathematician John Horton Conway in 1970. Basically, there is a gird of any size, and each node in the grid can either be empty or alive. The game evolves in steps, and at each step, the following rules are applied to each node: If the cell is alive, then it stays alive if it has either 2 or 3 live neighbors, otherwise it dies. If the cell is empty, then it springs to life only in the case that it has 3 live neighbors, otherwise it remains empty. By neighboring, we mean the 8 cells surrounding the cell. Clean Up the Code Before we start, let's clean up the code from the last chapter. It's just a bit refactoring. const requestDevice = async (): Promise<[GPUAdapter, GPUDevice] | null> => { const adapter = await navigator.gpu.requestAdapter(); if (!adapter) { console.error('WebGPU not supported'); return null; } const device = await adapter.requestDevice(); console.log(device); return [adapter, device]; } const getContext = async (device: GPUDevice): Promise<[GPUCanvasContext, GPUTextureFormat]> => { const canvas = document.getElementById('app') as HTMLCanvasElement; const context = canvas.getContext(\"webgpu\")!; const canvasFormat = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device: device, format: canvasFormat, }); return [context, canvasFormat]; } const getShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"shader\", code: ` @vertex fn vertexMain(@location(0) pos: vec2f) -> @builtin(position) vec4f { return vec4f(pos, 0, 1); } @fragment fn fragmentMain() -> @location(0) vec4<f32> { return vec4<f32>(1.0, 1.0, 0.0, 1.0); } ` }); } const getBuffer = async (device: GPUDevice): Promise<[GPUBuffer, GPUVertexBufferLayout]> => { const vertices = [ 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, ] const vertexBuffer = device.createBuffer({ size: vertices.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, mappedAtCreation: true, }); new Float32Array(vertexBuffer.getMappedRange()).set(vertices); const vertexBufferLayout: GPUVertexBufferLayout = { arrayStride: 8, attributes: [{ format: \"float32x2\", offset: 0, shaderLocation: 0, }], }; vertexBuffer.unmap(); return [vertexBuffer, vertexBufferLayout]; } const main = async () => { const [_, device] = (await requestDevice())!; const [context, canvasFormat] = await getContext(device); const shaderModule = await getShaderModule(device); const [vertexBuffer, vertexBufferLayout] = await getBuffer(device); const pipeline = device.createRenderPipeline({ label: \"Cell pipeline\", layout: \"auto\", vertex: { module: shaderModule, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout] }, fragment: { module: shaderModule, entryPoint: \"fragmentMain\", targets: [{ format: canvasFormat }] } }); const encoder = device.createCommandEncoder(); const pass = encoder.beginRenderPass({ colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: \"clear\", clearValue: { r: 0.1, g: 0.3, b: 0.8, a: 1.0 }, storeOp: \"store\", }] }); pass.setPipeline(pipeline); pass.setVertexBuffer(0, vertexBuffer); pass.draw(6); pass.end(); const commandBuffer = encoder.finish(); device.queue.submit([commandBuffer]); } main() Draw the Grid We will start by drawing all the cells on the screen, which forms a grid. First we generate all the points for each grid cell. A grid is a 2D array of cells, and each cell is a square. We will draw the grid as a set of triangles. Each cell will be represented by two triangles. const grid_length = 64; const getBuffer = async (device: GPUDevice): Promise<[GPUBuffer, GPUVertexBufferLayout]> => { const vertexBuffer = device.createBuffer({ size: grid_length * grid_length * 4 * 12, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, mappedAtCreation: true, }); const vertices = new Float32Array(grid_length * grid_length * 12); // starts from -1 to 1 const step = 2 / grid_length; const padding = step / 4; for(let i = 0; i < grid_length; i++) { for(let j = 0; j < grid_length; j++) { const top_left_x = -1 + i * step; const top_left_y = 1 - j * step; const bottom_right_x = top_left_x + step; const bottom_right_y = top_left_y - step; const index = (i * grid_length + j) * 12; vertices[index] = top_left_x + padding; vertices[index + 1] = top_left_y - padding; vertices[index + 2] = bottom_right_x - padding; vertices[index + 3] = top_left_y - padding; vertices[index + 4] = top_left_x + padding; vertices[index + 5] = bottom_right_y + padding; vertices[index + 6] = top_left_x + padding; vertices[index + 7] = bottom_right_y + padding; vertices[index + 8] = bottom_right_x - padding; vertices[index + 9] = top_left_y - padding; vertices[index + 10] = bottom_right_x - padding; vertices[index + 11] = bottom_right_y + padding; } } new Float32Array(vertexBuffer.getMappedRange()).set(vertices); const vertexBufferLayout: GPUVertexBufferLayout = { arrayStride: 8, attributes: [{ format: \"float32x2\", offset: 0, shaderLocation: 0, }], }; vertexBuffer.unmap(); return [vertexBuffer, vertexBufferLayout]; } Then change the draw pass to the correct number, pass.draw(grid_length * grid_length * 6); Now, you should see a grid of cells on the screen. Handle the States Now we need to deal with the states of each cell. Here is our first issue- how can we pass the states of each cell to the GPU? Previously, we learnt about passing data to GPU as shader parameters. But in this case, we can't do that since every cell needs access to the state of all other cells. We can't pass the state of all cells as a parameter to the shader. The solution is a bind group. A bind group is a collection of resources that are bound to the pipeline. We can directly create an array and bind it to a buffer, then pass it to the shader. We can simply use u32 to represent the state of each cell. We will use 0 to represent an empty cell and 1 to represent a live cell, please note that this is definitely not the most economical way to represent the state of a cell, but it's the simplest way for now. To create a bind group, first we initialize the data for it. Here, we will use a random pattern, where for each cell, there is a 25% chance that it will be alive. const states = new Uint32Array(grid_length * grid_length); for(let i = 0; i < grid_length * grid_length; i++) { states[i] = Math.random() < 0.25 ? 1 : 0; } Then we create a buffer and bind group for it. The buffer for vertex shader input is vertex buffer, and for now, we just need a buffer that stores the states of each cell, so we use a storage buffer. const statesStorageBuffer = device.createBuffer({ size: grid_length * grid_length * 4, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }); Previously, we used the map function to get the mapped range of the buffer, but this time, we can use device.queue.writeBuffer , which is another way to write data to the buffer. For mapped range, it is a complete map and thus suitable for constantly updating, small data, while writeBuffer is suitable for writing data once. device.queue.writeBuffer(statesStorageBuffer, 0, states.buffer); The second parameter is an offset, obviously, it should be a zero here. To access the buffer in the shader, we need to create a bind group layout. A bind group layout is a description of the resources that a bind group will contain. const bindGroup = device.createBindGroup({ label: \"bind group\", layout: pipeline.getBindGroupLayout(0), entries: [{ binding: 0, resource: { buffer: statesStorageBuffer }, }], }); Here we create a bind group with a single entry, which is a buffer that stores the states of each cell. Typically, you need to manually create a layout. But we created pipeline with layout: \"auto\" , so we can just use pipeline.getBindGroupLayout(0) to get the layout. It will look for the shader and create the layout for you. Then we need to update the render pass to bind the bind group. pass.setBindGroup(0, bindGroup); So we create a bind group, which contains a buffer that stores the states of each cell, and bind it to the pipeline. To access the buffer in the shader, we need to update the shader code. const getShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"shader\", code: ` @group(0) @binding(0) var<storage> states: array<u32>; @vertex fn vertexMain(@location(0) pos: vec2f) -> @builtin(position) vec4f { return vec4f(pos, 0, 1); } @fragment fn fragmentMain() -> @location(0) vec4<f32> { return vec4<f32>(1.0, 1.0, 0.0, 1.0); } ` }); } Here, the var<storage> is used to declare a storage buffer, and array<u32> is used to declare an array of u32 . The @group(0) @binding(0) is used to bind the buffer to the bind group. Now we can access the buffer in the shader. However, there is another issue. How can we knew the index of the cell to be rendered? We just can just pass the index of the cell to the vertex shader. To do so, we can add a new parameter to the vertex shader. @group(0) @binding(0) var<storage> states: array<u32>; @vertex fn vertexMain(@location(0) pos: vec2f, @location(1) cell: vec2f) -> @builtin(position) vec4f { return vec4f(pos, 0, 1); } Then we need to update the vertex buffer layout. const vertexBufferLayout: GPUVertexBufferLayout = { arrayStride: 12, attributes: [{ format: \"float32x2\", offset: 0, shaderLocation: 0, }, { format: \"float32x2\", offset: 8, shaderLocation: 1, }], }; However, a simpler way would be using vertex index. The vertex index is the index of the vertex in the vertex buffer. We can use the vertex index to calculate the cell index. @group(0) @binding(0) var<storage> states: array<u32>; @vertex fn vertexMain(@location(0) pos: vec2f, @builtin(vertex_index) vertexIndex: u32) -> @builtin(position) vec4f { let cell_index = vertexIndex / 6; return vec4f(pos, 0, 1); } But the problem is, vertex shader can only take parameters and return position, so we can not discard certain vertices in vertex shader. We need to use the fragment shader to do that. But how we can pass the state to the fragment shader? Actually, you can just use a struct. So long that the returned value of the vertex shader is a struct with a field of type @builtin(position) vec4f , the fragment shader can access the struct. struct VertexOutput { @builtin(position) pos: vec4<f32>, @location(0) @interpolate(flat) cell: u32, }; @group(0) @binding(0) var<storage> states: array<u32>; @vertex fn vertexMain(@location(0) pos: vec2f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let cell_index = vertexIndex / 6; var output: VertexOutput; output.pos = vec4f(gridPos, 0, 1); output.cell = cell; return output; } Please note that, fragment shader is responsible for rendering every pixel, but vertex shader output only the vertex. When the vertices are passed, they are first ensemble into a primitive, then the fragment shader is called for each pixel in the primitive. This is why we need a @interpolate(flat) for the cell. This tells the pipeline that when dealing with the value of pixels that are not vertices, it should use the flatten value of the primitive- that is, the value of the vertex. Previously, we passed no parameters to the fragment shader. Actually, it just accepts the output from the vertex shader, so if you want the position, if vertex only return position, then you can just use pos in the fragment shader. @fragment fn fragmentMain(@builtin(position) pos: vec4f) -> vec4<f32> { return vec4<f32>(1.0, 1.0, 0.0, 1.0); } But now, since we have a struct, we can access the field of the struct in the fragment shader. In the fragment, you can discard some nodes based on the state. The discarded nodes will not be rendered. However, for example, if in a triangle, one vertex is discarded, the whole triangle will be discarded. @fragment fn fragmentMain(input: VertexOutput) -> @location(0) vec4<f32> { if(states[input.cell] == 0u) { discard; } return vec4<f32>(1.0, 1.0, 0.0, 1.0); } Now, you should only see part of the grid, where the cells are alive initially. Update the States However, our cells are not alive- we have to update the states of each cell. To run computation on GPU, instead of vertex shader and fragment shader, we can use compute shader. A compute shader is a shader stage that is used to perform general-purpose computation on the GPU. It is used to perform calculations that are not necessarily related to rendering. For computation shader, we need to create a new shader module. const getComputationShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"computation shader\", code: ` @group(0) @binding(0) var<storage> states: array<u32>; @compute @workgroup_size(${workgroup_size}, ${workgroup_size}) fn computeMain() { } ` }); } We have something unique for the compute shader- @workgroup_size . The workgroup size is the number of threads in a workgroup. The workgroup size is a 3D vector, and the typical maximum size is 256x256x64, may vary depending on the device, in my Mac, it's only capable of up to 16 . If you want to do, for example, computation over grid_length by grid_length cells, if it exceeds the workgroup size, you can make the workgroup size smaller. When the actual computation is done, it's done by a workgroup_size times workgroup_size every time. It slides over the grid. global_invocation_id is still the actual index of the cell over the whole grid. By omitting the parameters, it is the same as setting that dimension to 1. Then the same pipeline and pass, const computationShaderModule = await getComputationShaderModule(device); const computationPipeline = device.createComputePipeline({ label: \"computation pipeline\", layout: \"auto\", compute: { module: computationShaderModule, entryPoint: \"computeMain\", } }); const computationEncoder = device.createCommandEncoder(); const computationPass = computationEncoder.beginComputePass(); Before going further, we need to introduce a two-buffer system for updating states. Basically, since rendering is a long-lasting process, we can't update the states while rendering. So we need to use two buffers, one for the current state and one for the next state. After each frame, we swap the buffers. Now we extract our code that is responsible for rendering, we isolate them and pass an extra parameter, the step. const render = async (step: number) => { const encoder = device.createCommandEncoder(); const pass = encoder.beginRenderPass({ colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: \"clear\", clearValue: { r: 0.1, g: 0.3, b: 0.6, a: 1.0 }, storeOp: \"store\", }] }); const bindGroup = device.createBindGroup({ label: \"bind group\", layout: pipeline.getBindGroupLayout(0), entries: [{ binding: 0, resource: { buffer: statesStorageBuffer }, }], }); pass.setPipeline(pipeline); pass.setBindGroup(0, bindGroup); pass.setVertexBuffer(0, vertexBuffer); pass.draw(grid_length * grid_length * 6); pass.end(); const commandBuffer = encoder.finish(); device.queue.submit([commandBuffer]); } render(0); Instead of one buffer, we use two buffer to allow switching between two. const statesStorageBuffer = [ device.createBuffer({ size: grid_length * grid_length * 4, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }), device.createBuffer({ size: grid_length * grid_length * 4, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }) ] device.queue.writeBuffer(statesStorageBuffer[0], 0, states.buffer); Then, when rendering, we need to bind the correct buffer. Since we will be swapping the buffers, we need to use the modulo operator to get the correct buffer. const bindGroup = device.createBindGroup({ label: \"bind group\", layout: pipeline.getBindGroupLayout(0), entries: [{ binding: 0, resource: { buffer: statesStorageBuffer[step % 2] }, }], }); And lastly, let step = 0; setInterval(() => { render(step); step++; step = step % 2; }, 1000); We will see the grid of cells updating every second. The second state is all the cells are dead, since we didn't update the states. Now let's write the actual computation shader. Now we have two buffers and will be switching between them. So after every rendering, we just use one as the previous state, and write the states to the other buffer, so that the next rendering will use the updated states. Previously, we could only read from the buffer, in order to write to the buffer in the wsgl, we need to specify read_write in the var declaration. @group(0) @binding(1) var<storage, read_write> states: array<u32>; Note that we don't need to change GPU usage, since that is how CPU will interact with the buffer, where as now, buffers are handled completely by GPU except for the first write. Then we can write the computation shader. @group(0) @binding(0) var<storage> states: array<u32>; @group(0) @binding(1) var<storage, read_write> next_states: array<u32>; @compute @workgroup_size(${grid_length}, ${grid_length}) fn computeMain(@builtin(global_invocation_id) cell: vec3u) { } The compute shader is executed in parallel, and the global_invocation_id is the global index of the thread. It is a vec3u . For example, for the first task executaed, it is 0, 0, 0 , then increase for the next task- though please note that they are computed parallelly. We can just use cell.x , cell.y as the index of the cell. That is, for task x, y, z , we deal with the state of the cell at x, y . const getComputationShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"computation shader\", code: ` @group(0) @binding(0) var<storage> states: array<u32>; @group(0) @binding(1) var<storage, read_write> next_states: array<u32>; @compute @workgroup_size(${workgroup_size}, ${workgroup_size}) fn computeMain(@builtin(global_invocation_id) cell: vec3u) { let index = cell.x * ${grid_length} + cell.y; var count = 0u; for(var i: i32 = -1; i < 2; i++) { for(var j: i32 = -1; j < 2; j++) { if(i == 0 && j == 0) { continue; } let x = i32(cell.x) + i; let y = i32(cell.y) + j; if(x >= 0 && x < ${grid_length} && y >= 0 && y < ${grid_length}) { count += states[x * ${grid_length} + y]; } } } if(states[index] == 1u) { if(count < 2u || count > 3u) { next_states[index] = 0u; } else { next_states[index] = 1u; } } else { if(count == 3u) { next_states[index] = 1u; } else { next_states[index] = 0u; } } } ` }); } Then the rest of the code, they are the same. const computationShaderModule = await getComputationShaderModule(device); const computationEncoder = device.createCommandEncoder(); const computationPass = computationEncoder.beginComputePass(); However, we now want two buffers be shared for the two pipelines, since they will create their respective bind group layout. We need to make sure that the two bind groups are compatible. const bindGroupLayout = device.createBindGroupLayout({ entries: [{ binding: 0, visibility: GPUShaderStage.FRAGMENT | GPUShaderStage.COMPUTE | GPUShaderStage.VERTEX, buffer: { type: \"read-only-storage\" } }, { binding: 1, visibility: GPUShaderStage.FRAGMENT | GPUShaderStage.COMPUTE, buffer: { type: \"storage\" } }] }); const pipelineLayout = device.createPipelineLayout({ bindGroupLayouts: [bindGroupLayout] }); const pipeline = device.createRenderPipeline({ label: \"pipeline\", layout: pipelineLayout, vertex: { module: shaderModule, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout] }, fragment: { module: shaderModule, entryPoint: \"fragmentMain\", targets: [{ format: canvasFormat }] } }); const computeShaderModule = await getComputationShaderModule(device); const computePipeline = device.createComputePipeline({ layout: pipelineLayout, compute: { module: computeShaderModule, entryPoint: \"computeMain\" } }); Then we can create the bind group. const bindGroup = device.createBindGroup({ label: \"bind group\", layout: bindGroupLayout, entries: [{ binding: 0, resource: { buffer: statesStorageBuffer[step % 2] }, }, { binding: 1, resource: { buffer: statesStorageBuffer[(step + 1) % 2] }, }] }); And lastly, before the encoder finishes, we need to dispatch the computation. const computePass = encoder.beginComputePass(); computePass.setPipeline(computePipeline); computePass.setBindGroup(0, bindGroup); const factor = Math.floor(grid_length / workgroup_size); computePass.dispatchWorkgroups(factor, factor) computePass.end(); Here for the workgroup, if the total computation is x by y by z , and the workgroup size is a by b by c , then the workgroup size is Math.ceil(x / a) , Math.ceil(y / b) , Math.ceil(z / c) . Every time, x by y by z cells are computed. Now, the complete code is as follows. const grid_length = 64; const workgroup_size = 8; const requestDevice = async (): Promise<[GPUAdapter, GPUDevice] | null> => { const adapter = await navigator.gpu.requestAdapter(); if (!adapter) { console.error('WebGPU not supported'); return null; } const device = await adapter.requestDevice(); console.log(device); return [adapter, device]; } const getContext = async (device: GPUDevice): Promise<[GPUCanvasContext, GPUTextureFormat]> => { const canvas = document.getElementById('app') as HTMLCanvasElement; const context = canvas.getContext(\"webgpu\")!; const canvasFormat = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device: device, format: canvasFormat, }); return [context, canvasFormat]; } const getShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"shader\", code: ` @group(0) @binding(0) var<storage> states: array<u32>; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) cell: u32, }; @vertex fn vertexMain(@location(0) pos: vec2f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { var output: VertexOutput; output.pos = vec4<f32>(pos, 0.0, 1.0); output.cell = vertexIndex / 6; return output; } @fragment fn fragmentMain(input: VertexOutput) -> vec4<f32> { if(states[input.cell] == 0u) { discard; } return vec4<f32>(1.0, 1.0, 0.0, 1.0); } ` }); } const getVertexBuffer = async (device: GPUDevice): Promise<[GPUBuffer, GPUVertexBufferLayout]> => { const vertexBuffer = device.createBuffer({ size: grid_length * grid_length * 4 * 12, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, mappedAtCreation: true, }); const vertices = new Float32Array(grid_length * grid_length * 12); // starts from -1 to 1 const step = 2 / grid_length; const padding = step / 6; for(let i = 0; i < grid_length; i++) { for(let j = 0; j < grid_length; j++) { const top_left_x = -1 + i * step; const top_left_y = 1 - j * step; const bottom_right_x = top_left_x + step; const bottom_right_y = top_left_y - step; const index = (i * grid_length + j) * 12; vertices[index] = top_left_x + padding; vertices[index + 1] = top_left_y - padding; vertices[index + 2] = bottom_right_x - padding; vertices[index + 3] = top_left_y - padding; vertices[index + 4] = top_left_x + padding; vertices[index + 5] = bottom_right_y + padding; vertices[index + 6] = top_left_x + padding; vertices[index + 7] = bottom_right_y + padding; vertices[index + 8] = bottom_right_x - padding; vertices[index + 9] = top_left_y - padding; vertices[index + 10] = bottom_right_x - padding; vertices[index + 11] = bottom_right_y + padding; } } new Float32Array(vertexBuffer.getMappedRange()).set(vertices); const vertexBufferLayout: GPUVertexBufferLayout = { arrayStride: 8, attributes: [{ format: \"float32x2\", offset: 0, shaderLocation: 0, }], }; vertexBuffer.unmap(); return [vertexBuffer, vertexBufferLayout]; } const getComputationShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"computation shader\", code: ` @group(0) @binding(0) var<storage> states: array<u32>; @group(0) @binding(1) var<storage, read_write> next_states: array<u32>; @compute @workgroup_size(${workgroup_size}, ${workgroup_size}) fn computeMain(@builtin(global_invocation_id) cell: vec3u) { let index = cell.x * ${grid_length} + cell.y; var count = 0u; for(var i: i32 = -1; i < 2; i++) { for(var j: i32 = -1; j < 2; j++) { if(i == 0 && j == 0) { continue; } let x = i32(cell.x) + i; let y = i32(cell.y) + j; if(x >= 0 && x < ${grid_length} && y >= 0 && y < ${grid_length}) { count += states[x * ${grid_length} + y]; } } } if(states[index] == 1u) { if(count < 2u || count > 3u) { next_states[index] = 0u; } else { next_states[index] = 1u; } } else { if(count == 3u) { next_states[index] = 1u; } else { next_states[index] = 0u; } } } ` }); } const main = async () => { const [_, device] = (await requestDevice())!; const [context, canvasFormat] = await getContext(device); const shaderModule = await getShaderModule(device); const [vertexBuffer, vertexBufferLayout] = await getVertexBuffer(device); const states = new Uint32Array(grid_length * grid_length); for(let i = 0; i < grid_length * grid_length; i++) { states[i] = Math.random() > 0.25 ? 0 : 1; } const statesStorageBuffer = [ device.createBuffer({ size: grid_length * grid_length * 4, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }), device.createBuffer({ size: grid_length * grid_length * 4, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }) ] device.queue.writeBuffer(statesStorageBuffer[0], 0, states.buffer); const bindGroupLayout = device.createBindGroupLayout({ entries: [{ binding: 0, visibility: GPUShaderStage.FRAGMENT | GPUShaderStage.COMPUTE | GPUShaderStage.VERTEX, buffer: { type: \"read-only-storage\" } }, { binding: 1, visibility: GPUShaderStage.FRAGMENT | GPUShaderStage.COMPUTE, buffer: { type: \"storage\" } }] }); const pipelineLayout = device.createPipelineLayout({ bindGroupLayouts: [bindGroupLayout] }); const pipeline = device.createRenderPipeline({ label: \"pipeline\", layout: pipelineLayout, vertex: { module: shaderModule, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout] }, fragment: { module: shaderModule, entryPoint: \"fragmentMain\", targets: [{ format: canvasFormat }] } }); const computeShaderModule = await getComputationShaderModule(device); const computePipeline = device.createComputePipeline({ layout: pipelineLayout, compute: { module: computeShaderModule, entryPoint: \"computeMain\" } }); const render = async (step: number) => { const encoder = device.createCommandEncoder(); const bindGroup = device.createBindGroup({ label: \"bind group\", layout: bindGroupLayout, entries: [{ binding: 0, resource: { buffer: statesStorageBuffer[step % 2] }, }, { binding: 1, resource: { buffer: statesStorageBuffer[(step + 1) % 2] }, }] }); const pass = encoder.beginRenderPass({ colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: \"clear\", clearValue: { r: 0.1, g: 0.3, b: 0.6, a: 1.0 }, storeOp: \"store\", }] }); pass.setPipeline(pipeline); pass.setBindGroup(0, bindGroup); pass.setVertexBuffer(0, vertexBuffer); pass.draw(grid_length * grid_length * 6); pass.end(); const computePass = encoder.beginComputePass(); computePass.setPipeline(computePipeline); computePass.setBindGroup(0, bindGroup); const factor = Math.floor(grid_length / workgroup_size); computePass.dispatchWorkgroups(factor, factor) computePass.end(); const commandBuffer = encoder.finish(); device.queue.submit([commandBuffer]); } let step = 0; setInterval(() => { render(step); step++; step = step % 2; }, 500); } main() Now you should see the game of life simulation running on the screen. Congratulations! You have successfully created a game of life simulation using WebGPU. If you are willing to observe the game of life simulation in a more detailed way, you can increase the grid length and workgroup size. However, keep in mind that the larger the grid length and workgroup size, the more computation is required, and it may slow down the simulation. However, GPU is powerful and can handle a large amount of computation. You can build a pure CPU version and see the great difference between the two. In the next chapter, we will learn about 3D models and how to render them using WebGPU.","title":"Life Game"},{"location":"02/#triangles-on-web-ch2-creating-the-life-game","text":"We will build a cellular automaton that simulates the game of life. The game of life is a zero-player game, meaning that its evolution is determined by its initial state, requiring no further input. One interacts with the Game of Life by creating an initial configuration and observing how it evolves. In the last chapter, we learnt about WebGPU basics, and now, we will use that knowledge to create a simple game of life simulation. We will use the GPU to perform the calculations and render the game of life.","title":"Triangles On Web Ch2 Creating the Life Game"},{"location":"02/#the-game-of-life","text":"The Game of Life is a cellular automaton devised by the British mathematician John Horton Conway in 1970. Basically, there is a gird of any size, and each node in the grid can either be empty or alive. The game evolves in steps, and at each step, the following rules are applied to each node: If the cell is alive, then it stays alive if it has either 2 or 3 live neighbors, otherwise it dies. If the cell is empty, then it springs to life only in the case that it has 3 live neighbors, otherwise it remains empty. By neighboring, we mean the 8 cells surrounding the cell.","title":"The Game of Life"},{"location":"02/#clean-up-the-code","text":"Before we start, let's clean up the code from the last chapter. It's just a bit refactoring. const requestDevice = async (): Promise<[GPUAdapter, GPUDevice] | null> => { const adapter = await navigator.gpu.requestAdapter(); if (!adapter) { console.error('WebGPU not supported'); return null; } const device = await adapter.requestDevice(); console.log(device); return [adapter, device]; } const getContext = async (device: GPUDevice): Promise<[GPUCanvasContext, GPUTextureFormat]> => { const canvas = document.getElementById('app') as HTMLCanvasElement; const context = canvas.getContext(\"webgpu\")!; const canvasFormat = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device: device, format: canvasFormat, }); return [context, canvasFormat]; } const getShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"shader\", code: ` @vertex fn vertexMain(@location(0) pos: vec2f) -> @builtin(position) vec4f { return vec4f(pos, 0, 1); } @fragment fn fragmentMain() -> @location(0) vec4<f32> { return vec4<f32>(1.0, 1.0, 0.0, 1.0); } ` }); } const getBuffer = async (device: GPUDevice): Promise<[GPUBuffer, GPUVertexBufferLayout]> => { const vertices = [ 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, ] const vertexBuffer = device.createBuffer({ size: vertices.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, mappedAtCreation: true, }); new Float32Array(vertexBuffer.getMappedRange()).set(vertices); const vertexBufferLayout: GPUVertexBufferLayout = { arrayStride: 8, attributes: [{ format: \"float32x2\", offset: 0, shaderLocation: 0, }], }; vertexBuffer.unmap(); return [vertexBuffer, vertexBufferLayout]; } const main = async () => { const [_, device] = (await requestDevice())!; const [context, canvasFormat] = await getContext(device); const shaderModule = await getShaderModule(device); const [vertexBuffer, vertexBufferLayout] = await getBuffer(device); const pipeline = device.createRenderPipeline({ label: \"Cell pipeline\", layout: \"auto\", vertex: { module: shaderModule, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout] }, fragment: { module: shaderModule, entryPoint: \"fragmentMain\", targets: [{ format: canvasFormat }] } }); const encoder = device.createCommandEncoder(); const pass = encoder.beginRenderPass({ colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: \"clear\", clearValue: { r: 0.1, g: 0.3, b: 0.8, a: 1.0 }, storeOp: \"store\", }] }); pass.setPipeline(pipeline); pass.setVertexBuffer(0, vertexBuffer); pass.draw(6); pass.end(); const commandBuffer = encoder.finish(); device.queue.submit([commandBuffer]); } main()","title":"Clean Up the Code"},{"location":"02/#draw-the-grid","text":"We will start by drawing all the cells on the screen, which forms a grid. First we generate all the points for each grid cell. A grid is a 2D array of cells, and each cell is a square. We will draw the grid as a set of triangles. Each cell will be represented by two triangles. const grid_length = 64; const getBuffer = async (device: GPUDevice): Promise<[GPUBuffer, GPUVertexBufferLayout]> => { const vertexBuffer = device.createBuffer({ size: grid_length * grid_length * 4 * 12, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, mappedAtCreation: true, }); const vertices = new Float32Array(grid_length * grid_length * 12); // starts from -1 to 1 const step = 2 / grid_length; const padding = step / 4; for(let i = 0; i < grid_length; i++) { for(let j = 0; j < grid_length; j++) { const top_left_x = -1 + i * step; const top_left_y = 1 - j * step; const bottom_right_x = top_left_x + step; const bottom_right_y = top_left_y - step; const index = (i * grid_length + j) * 12; vertices[index] = top_left_x + padding; vertices[index + 1] = top_left_y - padding; vertices[index + 2] = bottom_right_x - padding; vertices[index + 3] = top_left_y - padding; vertices[index + 4] = top_left_x + padding; vertices[index + 5] = bottom_right_y + padding; vertices[index + 6] = top_left_x + padding; vertices[index + 7] = bottom_right_y + padding; vertices[index + 8] = bottom_right_x - padding; vertices[index + 9] = top_left_y - padding; vertices[index + 10] = bottom_right_x - padding; vertices[index + 11] = bottom_right_y + padding; } } new Float32Array(vertexBuffer.getMappedRange()).set(vertices); const vertexBufferLayout: GPUVertexBufferLayout = { arrayStride: 8, attributes: [{ format: \"float32x2\", offset: 0, shaderLocation: 0, }], }; vertexBuffer.unmap(); return [vertexBuffer, vertexBufferLayout]; } Then change the draw pass to the correct number, pass.draw(grid_length * grid_length * 6); Now, you should see a grid of cells on the screen.","title":"Draw the Grid"},{"location":"02/#handle-the-states","text":"Now we need to deal with the states of each cell. Here is our first issue- how can we pass the states of each cell to the GPU? Previously, we learnt about passing data to GPU as shader parameters. But in this case, we can't do that since every cell needs access to the state of all other cells. We can't pass the state of all cells as a parameter to the shader. The solution is a bind group. A bind group is a collection of resources that are bound to the pipeline. We can directly create an array and bind it to a buffer, then pass it to the shader. We can simply use u32 to represent the state of each cell. We will use 0 to represent an empty cell and 1 to represent a live cell, please note that this is definitely not the most economical way to represent the state of a cell, but it's the simplest way for now. To create a bind group, first we initialize the data for it. Here, we will use a random pattern, where for each cell, there is a 25% chance that it will be alive. const states = new Uint32Array(grid_length * grid_length); for(let i = 0; i < grid_length * grid_length; i++) { states[i] = Math.random() < 0.25 ? 1 : 0; } Then we create a buffer and bind group for it. The buffer for vertex shader input is vertex buffer, and for now, we just need a buffer that stores the states of each cell, so we use a storage buffer. const statesStorageBuffer = device.createBuffer({ size: grid_length * grid_length * 4, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }); Previously, we used the map function to get the mapped range of the buffer, but this time, we can use device.queue.writeBuffer , which is another way to write data to the buffer. For mapped range, it is a complete map and thus suitable for constantly updating, small data, while writeBuffer is suitable for writing data once. device.queue.writeBuffer(statesStorageBuffer, 0, states.buffer); The second parameter is an offset, obviously, it should be a zero here. To access the buffer in the shader, we need to create a bind group layout. A bind group layout is a description of the resources that a bind group will contain. const bindGroup = device.createBindGroup({ label: \"bind group\", layout: pipeline.getBindGroupLayout(0), entries: [{ binding: 0, resource: { buffer: statesStorageBuffer }, }], }); Here we create a bind group with a single entry, which is a buffer that stores the states of each cell. Typically, you need to manually create a layout. But we created pipeline with layout: \"auto\" , so we can just use pipeline.getBindGroupLayout(0) to get the layout. It will look for the shader and create the layout for you. Then we need to update the render pass to bind the bind group. pass.setBindGroup(0, bindGroup); So we create a bind group, which contains a buffer that stores the states of each cell, and bind it to the pipeline. To access the buffer in the shader, we need to update the shader code. const getShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"shader\", code: ` @group(0) @binding(0) var<storage> states: array<u32>; @vertex fn vertexMain(@location(0) pos: vec2f) -> @builtin(position) vec4f { return vec4f(pos, 0, 1); } @fragment fn fragmentMain() -> @location(0) vec4<f32> { return vec4<f32>(1.0, 1.0, 0.0, 1.0); } ` }); } Here, the var<storage> is used to declare a storage buffer, and array<u32> is used to declare an array of u32 . The @group(0) @binding(0) is used to bind the buffer to the bind group. Now we can access the buffer in the shader. However, there is another issue. How can we knew the index of the cell to be rendered? We just can just pass the index of the cell to the vertex shader. To do so, we can add a new parameter to the vertex shader. @group(0) @binding(0) var<storage> states: array<u32>; @vertex fn vertexMain(@location(0) pos: vec2f, @location(1) cell: vec2f) -> @builtin(position) vec4f { return vec4f(pos, 0, 1); } Then we need to update the vertex buffer layout. const vertexBufferLayout: GPUVertexBufferLayout = { arrayStride: 12, attributes: [{ format: \"float32x2\", offset: 0, shaderLocation: 0, }, { format: \"float32x2\", offset: 8, shaderLocation: 1, }], }; However, a simpler way would be using vertex index. The vertex index is the index of the vertex in the vertex buffer. We can use the vertex index to calculate the cell index. @group(0) @binding(0) var<storage> states: array<u32>; @vertex fn vertexMain(@location(0) pos: vec2f, @builtin(vertex_index) vertexIndex: u32) -> @builtin(position) vec4f { let cell_index = vertexIndex / 6; return vec4f(pos, 0, 1); } But the problem is, vertex shader can only take parameters and return position, so we can not discard certain vertices in vertex shader. We need to use the fragment shader to do that. But how we can pass the state to the fragment shader? Actually, you can just use a struct. So long that the returned value of the vertex shader is a struct with a field of type @builtin(position) vec4f , the fragment shader can access the struct. struct VertexOutput { @builtin(position) pos: vec4<f32>, @location(0) @interpolate(flat) cell: u32, }; @group(0) @binding(0) var<storage> states: array<u32>; @vertex fn vertexMain(@location(0) pos: vec2f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let cell_index = vertexIndex / 6; var output: VertexOutput; output.pos = vec4f(gridPos, 0, 1); output.cell = cell; return output; } Please note that, fragment shader is responsible for rendering every pixel, but vertex shader output only the vertex. When the vertices are passed, they are first ensemble into a primitive, then the fragment shader is called for each pixel in the primitive. This is why we need a @interpolate(flat) for the cell. This tells the pipeline that when dealing with the value of pixels that are not vertices, it should use the flatten value of the primitive- that is, the value of the vertex. Previously, we passed no parameters to the fragment shader. Actually, it just accepts the output from the vertex shader, so if you want the position, if vertex only return position, then you can just use pos in the fragment shader. @fragment fn fragmentMain(@builtin(position) pos: vec4f) -> vec4<f32> { return vec4<f32>(1.0, 1.0, 0.0, 1.0); } But now, since we have a struct, we can access the field of the struct in the fragment shader. In the fragment, you can discard some nodes based on the state. The discarded nodes will not be rendered. However, for example, if in a triangle, one vertex is discarded, the whole triangle will be discarded. @fragment fn fragmentMain(input: VertexOutput) -> @location(0) vec4<f32> { if(states[input.cell] == 0u) { discard; } return vec4<f32>(1.0, 1.0, 0.0, 1.0); } Now, you should only see part of the grid, where the cells are alive initially.","title":"Handle the States"},{"location":"02/#update-the-states","text":"However, our cells are not alive- we have to update the states of each cell. To run computation on GPU, instead of vertex shader and fragment shader, we can use compute shader. A compute shader is a shader stage that is used to perform general-purpose computation on the GPU. It is used to perform calculations that are not necessarily related to rendering. For computation shader, we need to create a new shader module. const getComputationShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"computation shader\", code: ` @group(0) @binding(0) var<storage> states: array<u32>; @compute @workgroup_size(${workgroup_size}, ${workgroup_size}) fn computeMain() { } ` }); } We have something unique for the compute shader- @workgroup_size . The workgroup size is the number of threads in a workgroup. The workgroup size is a 3D vector, and the typical maximum size is 256x256x64, may vary depending on the device, in my Mac, it's only capable of up to 16 . If you want to do, for example, computation over grid_length by grid_length cells, if it exceeds the workgroup size, you can make the workgroup size smaller. When the actual computation is done, it's done by a workgroup_size times workgroup_size every time. It slides over the grid. global_invocation_id is still the actual index of the cell over the whole grid. By omitting the parameters, it is the same as setting that dimension to 1. Then the same pipeline and pass, const computationShaderModule = await getComputationShaderModule(device); const computationPipeline = device.createComputePipeline({ label: \"computation pipeline\", layout: \"auto\", compute: { module: computationShaderModule, entryPoint: \"computeMain\", } }); const computationEncoder = device.createCommandEncoder(); const computationPass = computationEncoder.beginComputePass(); Before going further, we need to introduce a two-buffer system for updating states. Basically, since rendering is a long-lasting process, we can't update the states while rendering. So we need to use two buffers, one for the current state and one for the next state. After each frame, we swap the buffers. Now we extract our code that is responsible for rendering, we isolate them and pass an extra parameter, the step. const render = async (step: number) => { const encoder = device.createCommandEncoder(); const pass = encoder.beginRenderPass({ colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: \"clear\", clearValue: { r: 0.1, g: 0.3, b: 0.6, a: 1.0 }, storeOp: \"store\", }] }); const bindGroup = device.createBindGroup({ label: \"bind group\", layout: pipeline.getBindGroupLayout(0), entries: [{ binding: 0, resource: { buffer: statesStorageBuffer }, }], }); pass.setPipeline(pipeline); pass.setBindGroup(0, bindGroup); pass.setVertexBuffer(0, vertexBuffer); pass.draw(grid_length * grid_length * 6); pass.end(); const commandBuffer = encoder.finish(); device.queue.submit([commandBuffer]); } render(0); Instead of one buffer, we use two buffer to allow switching between two. const statesStorageBuffer = [ device.createBuffer({ size: grid_length * grid_length * 4, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }), device.createBuffer({ size: grid_length * grid_length * 4, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }) ] device.queue.writeBuffer(statesStorageBuffer[0], 0, states.buffer); Then, when rendering, we need to bind the correct buffer. Since we will be swapping the buffers, we need to use the modulo operator to get the correct buffer. const bindGroup = device.createBindGroup({ label: \"bind group\", layout: pipeline.getBindGroupLayout(0), entries: [{ binding: 0, resource: { buffer: statesStorageBuffer[step % 2] }, }], }); And lastly, let step = 0; setInterval(() => { render(step); step++; step = step % 2; }, 1000); We will see the grid of cells updating every second. The second state is all the cells are dead, since we didn't update the states. Now let's write the actual computation shader. Now we have two buffers and will be switching between them. So after every rendering, we just use one as the previous state, and write the states to the other buffer, so that the next rendering will use the updated states. Previously, we could only read from the buffer, in order to write to the buffer in the wsgl, we need to specify read_write in the var declaration. @group(0) @binding(1) var<storage, read_write> states: array<u32>; Note that we don't need to change GPU usage, since that is how CPU will interact with the buffer, where as now, buffers are handled completely by GPU except for the first write. Then we can write the computation shader. @group(0) @binding(0) var<storage> states: array<u32>; @group(0) @binding(1) var<storage, read_write> next_states: array<u32>; @compute @workgroup_size(${grid_length}, ${grid_length}) fn computeMain(@builtin(global_invocation_id) cell: vec3u) { } The compute shader is executed in parallel, and the global_invocation_id is the global index of the thread. It is a vec3u . For example, for the first task executaed, it is 0, 0, 0 , then increase for the next task- though please note that they are computed parallelly. We can just use cell.x , cell.y as the index of the cell. That is, for task x, y, z , we deal with the state of the cell at x, y . const getComputationShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"computation shader\", code: ` @group(0) @binding(0) var<storage> states: array<u32>; @group(0) @binding(1) var<storage, read_write> next_states: array<u32>; @compute @workgroup_size(${workgroup_size}, ${workgroup_size}) fn computeMain(@builtin(global_invocation_id) cell: vec3u) { let index = cell.x * ${grid_length} + cell.y; var count = 0u; for(var i: i32 = -1; i < 2; i++) { for(var j: i32 = -1; j < 2; j++) { if(i == 0 && j == 0) { continue; } let x = i32(cell.x) + i; let y = i32(cell.y) + j; if(x >= 0 && x < ${grid_length} && y >= 0 && y < ${grid_length}) { count += states[x * ${grid_length} + y]; } } } if(states[index] == 1u) { if(count < 2u || count > 3u) { next_states[index] = 0u; } else { next_states[index] = 1u; } } else { if(count == 3u) { next_states[index] = 1u; } else { next_states[index] = 0u; } } } ` }); } Then the rest of the code, they are the same. const computationShaderModule = await getComputationShaderModule(device); const computationEncoder = device.createCommandEncoder(); const computationPass = computationEncoder.beginComputePass(); However, we now want two buffers be shared for the two pipelines, since they will create their respective bind group layout. We need to make sure that the two bind groups are compatible. const bindGroupLayout = device.createBindGroupLayout({ entries: [{ binding: 0, visibility: GPUShaderStage.FRAGMENT | GPUShaderStage.COMPUTE | GPUShaderStage.VERTEX, buffer: { type: \"read-only-storage\" } }, { binding: 1, visibility: GPUShaderStage.FRAGMENT | GPUShaderStage.COMPUTE, buffer: { type: \"storage\" } }] }); const pipelineLayout = device.createPipelineLayout({ bindGroupLayouts: [bindGroupLayout] }); const pipeline = device.createRenderPipeline({ label: \"pipeline\", layout: pipelineLayout, vertex: { module: shaderModule, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout] }, fragment: { module: shaderModule, entryPoint: \"fragmentMain\", targets: [{ format: canvasFormat }] } }); const computeShaderModule = await getComputationShaderModule(device); const computePipeline = device.createComputePipeline({ layout: pipelineLayout, compute: { module: computeShaderModule, entryPoint: \"computeMain\" } }); Then we can create the bind group. const bindGroup = device.createBindGroup({ label: \"bind group\", layout: bindGroupLayout, entries: [{ binding: 0, resource: { buffer: statesStorageBuffer[step % 2] }, }, { binding: 1, resource: { buffer: statesStorageBuffer[(step + 1) % 2] }, }] }); And lastly, before the encoder finishes, we need to dispatch the computation. const computePass = encoder.beginComputePass(); computePass.setPipeline(computePipeline); computePass.setBindGroup(0, bindGroup); const factor = Math.floor(grid_length / workgroup_size); computePass.dispatchWorkgroups(factor, factor) computePass.end(); Here for the workgroup, if the total computation is x by y by z , and the workgroup size is a by b by c , then the workgroup size is Math.ceil(x / a) , Math.ceil(y / b) , Math.ceil(z / c) . Every time, x by y by z cells are computed. Now, the complete code is as follows. const grid_length = 64; const workgroup_size = 8; const requestDevice = async (): Promise<[GPUAdapter, GPUDevice] | null> => { const adapter = await navigator.gpu.requestAdapter(); if (!adapter) { console.error('WebGPU not supported'); return null; } const device = await adapter.requestDevice(); console.log(device); return [adapter, device]; } const getContext = async (device: GPUDevice): Promise<[GPUCanvasContext, GPUTextureFormat]> => { const canvas = document.getElementById('app') as HTMLCanvasElement; const context = canvas.getContext(\"webgpu\")!; const canvasFormat = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device: device, format: canvasFormat, }); return [context, canvasFormat]; } const getShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"shader\", code: ` @group(0) @binding(0) var<storage> states: array<u32>; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) cell: u32, }; @vertex fn vertexMain(@location(0) pos: vec2f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { var output: VertexOutput; output.pos = vec4<f32>(pos, 0.0, 1.0); output.cell = vertexIndex / 6; return output; } @fragment fn fragmentMain(input: VertexOutput) -> vec4<f32> { if(states[input.cell] == 0u) { discard; } return vec4<f32>(1.0, 1.0, 0.0, 1.0); } ` }); } const getVertexBuffer = async (device: GPUDevice): Promise<[GPUBuffer, GPUVertexBufferLayout]> => { const vertexBuffer = device.createBuffer({ size: grid_length * grid_length * 4 * 12, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, mappedAtCreation: true, }); const vertices = new Float32Array(grid_length * grid_length * 12); // starts from -1 to 1 const step = 2 / grid_length; const padding = step / 6; for(let i = 0; i < grid_length; i++) { for(let j = 0; j < grid_length; j++) { const top_left_x = -1 + i * step; const top_left_y = 1 - j * step; const bottom_right_x = top_left_x + step; const bottom_right_y = top_left_y - step; const index = (i * grid_length + j) * 12; vertices[index] = top_left_x + padding; vertices[index + 1] = top_left_y - padding; vertices[index + 2] = bottom_right_x - padding; vertices[index + 3] = top_left_y - padding; vertices[index + 4] = top_left_x + padding; vertices[index + 5] = bottom_right_y + padding; vertices[index + 6] = top_left_x + padding; vertices[index + 7] = bottom_right_y + padding; vertices[index + 8] = bottom_right_x - padding; vertices[index + 9] = top_left_y - padding; vertices[index + 10] = bottom_right_x - padding; vertices[index + 11] = bottom_right_y + padding; } } new Float32Array(vertexBuffer.getMappedRange()).set(vertices); const vertexBufferLayout: GPUVertexBufferLayout = { arrayStride: 8, attributes: [{ format: \"float32x2\", offset: 0, shaderLocation: 0, }], }; vertexBuffer.unmap(); return [vertexBuffer, vertexBufferLayout]; } const getComputationShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"computation shader\", code: ` @group(0) @binding(0) var<storage> states: array<u32>; @group(0) @binding(1) var<storage, read_write> next_states: array<u32>; @compute @workgroup_size(${workgroup_size}, ${workgroup_size}) fn computeMain(@builtin(global_invocation_id) cell: vec3u) { let index = cell.x * ${grid_length} + cell.y; var count = 0u; for(var i: i32 = -1; i < 2; i++) { for(var j: i32 = -1; j < 2; j++) { if(i == 0 && j == 0) { continue; } let x = i32(cell.x) + i; let y = i32(cell.y) + j; if(x >= 0 && x < ${grid_length} && y >= 0 && y < ${grid_length}) { count += states[x * ${grid_length} + y]; } } } if(states[index] == 1u) { if(count < 2u || count > 3u) { next_states[index] = 0u; } else { next_states[index] = 1u; } } else { if(count == 3u) { next_states[index] = 1u; } else { next_states[index] = 0u; } } } ` }); } const main = async () => { const [_, device] = (await requestDevice())!; const [context, canvasFormat] = await getContext(device); const shaderModule = await getShaderModule(device); const [vertexBuffer, vertexBufferLayout] = await getVertexBuffer(device); const states = new Uint32Array(grid_length * grid_length); for(let i = 0; i < grid_length * grid_length; i++) { states[i] = Math.random() > 0.25 ? 0 : 1; } const statesStorageBuffer = [ device.createBuffer({ size: grid_length * grid_length * 4, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }), device.createBuffer({ size: grid_length * grid_length * 4, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }) ] device.queue.writeBuffer(statesStorageBuffer[0], 0, states.buffer); const bindGroupLayout = device.createBindGroupLayout({ entries: [{ binding: 0, visibility: GPUShaderStage.FRAGMENT | GPUShaderStage.COMPUTE | GPUShaderStage.VERTEX, buffer: { type: \"read-only-storage\" } }, { binding: 1, visibility: GPUShaderStage.FRAGMENT | GPUShaderStage.COMPUTE, buffer: { type: \"storage\" } }] }); const pipelineLayout = device.createPipelineLayout({ bindGroupLayouts: [bindGroupLayout] }); const pipeline = device.createRenderPipeline({ label: \"pipeline\", layout: pipelineLayout, vertex: { module: shaderModule, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout] }, fragment: { module: shaderModule, entryPoint: \"fragmentMain\", targets: [{ format: canvasFormat }] } }); const computeShaderModule = await getComputationShaderModule(device); const computePipeline = device.createComputePipeline({ layout: pipelineLayout, compute: { module: computeShaderModule, entryPoint: \"computeMain\" } }); const render = async (step: number) => { const encoder = device.createCommandEncoder(); const bindGroup = device.createBindGroup({ label: \"bind group\", layout: bindGroupLayout, entries: [{ binding: 0, resource: { buffer: statesStorageBuffer[step % 2] }, }, { binding: 1, resource: { buffer: statesStorageBuffer[(step + 1) % 2] }, }] }); const pass = encoder.beginRenderPass({ colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: \"clear\", clearValue: { r: 0.1, g: 0.3, b: 0.6, a: 1.0 }, storeOp: \"store\", }] }); pass.setPipeline(pipeline); pass.setBindGroup(0, bindGroup); pass.setVertexBuffer(0, vertexBuffer); pass.draw(grid_length * grid_length * 6); pass.end(); const computePass = encoder.beginComputePass(); computePass.setPipeline(computePipeline); computePass.setBindGroup(0, bindGroup); const factor = Math.floor(grid_length / workgroup_size); computePass.dispatchWorkgroups(factor, factor) computePass.end(); const commandBuffer = encoder.finish(); device.queue.submit([commandBuffer]); } let step = 0; setInterval(() => { render(step); step++; step = step % 2; }, 500); } main() Now you should see the game of life simulation running on the screen. Congratulations! You have successfully created a game of life simulation using WebGPU. If you are willing to observe the game of life simulation in a more detailed way, you can increase the grid length and workgroup size. However, keep in mind that the larger the grid length and workgroup size, the more computation is required, and it may slow down the simulation. However, GPU is powerful and can handle a large amount of computation. You can build a pure CPU version and see the great difference between the two. In the next chapter, we will learn about 3D models and how to render them using WebGPU.","title":"Update the States"},{"location":"03/","text":"Triangles on Web Ch3 Texture In the last part, we created the life game with WebGPU. In this part, we will add texture to the game. Texture is a 2D image that can be used in shaders. It can be used to colorize the triangles, or to store data. In this part, we will use texture to colorize the triangles. Texture Texture is simply a 2D image. It can be used in shaders to colorize the triangles. In WebGPU, texture is created by device.createTexture method. The texture is created with a TextureDescriptor object. Creating Texture Let's first look for a random image on the internet. I found a random image on this website . This is a grass texture by macrovector on Freepik. We need to load that firstly, as a bitmap. It is simple, use fetch to get the image, and use createImageBitmap to create a bitmap from the image. async function loadImageBitmap(url) { const res = await fetch(url); const blob = await res.blob(); return await createImageBitmap(blob); } Then load that bitmap into a texture, const texture = device.createTexture({ label: \"grass\", format: 'rgba8unorm', size: [bitmap.width, bitmap.height], usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST | GPUTextureUsage.RENDER_ATTACHMENT, }); TEXTURE_BINDING is the specified usage for the texture. COPY_DST has already been introduced. RENDER_ATTACHMENT means that the texture can be directly rendered to. Later we will use copyExternalImageToTexture , so these usages are necessary. Now, let's copy, device.queue.copyExternalImageToTexture( { source: bitmap, flipY: true }, { texture }, { width: bitmap.width, height: bitmap.height }, ); copyExternalImageToTexture copies the bitmap to the texture. flipY is necessary because the origin of the image is at the top-left corner, but the origin of the texture is at the bottom-left corner. However, for a grass texture, you can't see much difference. Using Texture Now we have the texture, let's use it in the shader. Texture is just a picture, and the way picture is presented is by fragment shader. The fragment shader choose a pixel in the texture, and use that pixel as the color of the triangle, so that the texture is applied to the triangle. However, there might not be direct match between the displayed primitive and the texture. For example, the texture might be smaller than the primitive, or the texture might be larger than the primitive. In addition that, in the fragment, we only operate on the vertices. The way to solve this is texture coordinations. Texture coordinations are the coordinations in the texture. The texture coordinations are in the range of [0, 1], and the texture coordinations are used to choose the pixel in the texture. For example, if the texture coordinations are (0, 0), then the pixel at the top-left corner of the texture is chosen. If the texture coordinations are (1, 1), then the pixel at the bottom-right corner of the texture is chosen. As for the pixel between the pixels, the texture is interpolated. This process is called texture sampling. The texture is sampled by the texture coordinations, and the color of the pixel is chosen. Previously, we have loaded variable texture with the picture. Now, we need to load the texture into the shader. We also need a sampler to sample the texture. They are all just special bindings. We also add the texture coordinations to the vertex shader, and use textureSample to let the fragment shader sample the texture. const getShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"shader\", code: ` @group(0) @binding(0) var<storage> states: array<u32>; @group(0) @binding(2) var grass_texture: texture_2d<f32>; @group(0) @binding(3) var grass_sampler: sampler; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) cell: u32, @location(1) texCoord: vec2<f32>, }; @vertex fn vertexMain(@location(0) pos: vec2f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { var output: VertexOutput; output.pos = vec4<f32>(pos, 0.0, 1.0); output.cell = vertexIndex / 6; output.texCoord = output.pos.xy * 0.5 + 0.5; return output; } @fragment fn fragmentMain(input: VertexOutput) -> @location(0) vec4<f32> { if(states[input.cell] == 0u) { discard; } return textureSample(grass_texture, grass_sampler, input.texCoord); } ` }); } Please note that the NDC is in the range of [-1, 1], so we need to convert it to [0, 1] by multiplying 0.5 and adding 0.5. The vec2<f32> is automatically interpolated by the rasterized, with the mode @interpolate(perspective) , which means that the texture coordinations are interpolated by the rasterize. Now, you can just regard it as a linear interpolation based on the coordinations. Now we need to pass the values with the bindings. Its the same as before, but we need to pass the texture and the sampler. const getBindGroupLayout = async (device: GPUDevice): Promise<GPUBindGroupLayout> => { const bindGroupLayout = device.createBindGroupLayout({ entries: [{ binding: 0, visibility: GPUShaderStage.FRAGMENT | GPUShaderStage.COMPUTE | GPUShaderStage.VERTEX, buffer: { type: \"read-only-storage\" } }, { binding: 1, visibility: GPUShaderStage.FRAGMENT | GPUShaderStage.COMPUTE, buffer: { type: \"storage\" } }, { binding: 2, visibility: GPUShaderStage.FRAGMENT, texture: {} }, { binding: 3, visibility: GPUShaderStage.FRAGMENT, sampler: {} }] }); return bindGroupLayout; } We have already created the texture, but not the sampler. The sampler is created by device.createSampler method. const getSampler = (device: GPUDevice): GPUSampler => { return device.createSampler({ magFilter: \"linear\", minFilter: \"linear\", }); } We just created a simple sampler with linear interpolation. There are also other options, like nearest for nearest neighbor interpolation. Finally, we just change the bind group creation to include the texture and the sampler. const bindGroup = device.createBindGroup({ label: \"bind group\", layout: bindGroupLayout, entries: [{ binding: 0, resource: { buffer: statesStorageBuffer[step % 2] }, }, { binding: 1, resource: { buffer: statesStorageBuffer[(step + 1) % 2] }, }, { binding: 2, resource: texture.createView(), }, { binding: 3, resource: sampler, }] }); Note that you need to pass a view of the texture, not the texture itself. Now we can see that each cell shows part of the grass texture. The texture is applied to the triangles. If you want each cell to show the full picture, you can change the texture coordinations to the following, @vertex fn vertexMain(@location(0) pos: vec2f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { var output: VertexOutput; output.pos = vec4<f32>(pos, 0.0, 1.0); output.cell = vertexIndex / 6; if(vertexIndex % 6 == 0) { output.texCoord = vec2<f32>(0.0, 0.0); } else if(vertexIndex % 6 == 1 || vertexIndex % 6 == 4) { output.texCoord = vec2<f32>(1.0, 0.0); } else if(vertexIndex % 6 == 2 || vertexIndex % 6 == 3) { output.texCoord = vec2<f32>(0.0, 1.0); } else { output.texCoord = vec2<f32>(1.0, 1.0); } return output; } Now, each cell shows the full picture, this is because the texture coordinations are set to the corners of the texture.","title":"Texture"},{"location":"03/#triangles-on-web-ch3-texture","text":"In the last part, we created the life game with WebGPU. In this part, we will add texture to the game. Texture is a 2D image that can be used in shaders. It can be used to colorize the triangles, or to store data. In this part, we will use texture to colorize the triangles.","title":"Triangles on Web Ch3 Texture"},{"location":"03/#texture","text":"Texture is simply a 2D image. It can be used in shaders to colorize the triangles. In WebGPU, texture is created by device.createTexture method. The texture is created with a TextureDescriptor object.","title":"Texture"},{"location":"03/#creating-texture","text":"Let's first look for a random image on the internet. I found a random image on this website . This is a grass texture by macrovector on Freepik. We need to load that firstly, as a bitmap. It is simple, use fetch to get the image, and use createImageBitmap to create a bitmap from the image. async function loadImageBitmap(url) { const res = await fetch(url); const blob = await res.blob(); return await createImageBitmap(blob); } Then load that bitmap into a texture, const texture = device.createTexture({ label: \"grass\", format: 'rgba8unorm', size: [bitmap.width, bitmap.height], usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST | GPUTextureUsage.RENDER_ATTACHMENT, }); TEXTURE_BINDING is the specified usage for the texture. COPY_DST has already been introduced. RENDER_ATTACHMENT means that the texture can be directly rendered to. Later we will use copyExternalImageToTexture , so these usages are necessary. Now, let's copy, device.queue.copyExternalImageToTexture( { source: bitmap, flipY: true }, { texture }, { width: bitmap.width, height: bitmap.height }, ); copyExternalImageToTexture copies the bitmap to the texture. flipY is necessary because the origin of the image is at the top-left corner, but the origin of the texture is at the bottom-left corner. However, for a grass texture, you can't see much difference.","title":"Creating Texture"},{"location":"03/#using-texture","text":"Now we have the texture, let's use it in the shader. Texture is just a picture, and the way picture is presented is by fragment shader. The fragment shader choose a pixel in the texture, and use that pixel as the color of the triangle, so that the texture is applied to the triangle. However, there might not be direct match between the displayed primitive and the texture. For example, the texture might be smaller than the primitive, or the texture might be larger than the primitive. In addition that, in the fragment, we only operate on the vertices. The way to solve this is texture coordinations. Texture coordinations are the coordinations in the texture. The texture coordinations are in the range of [0, 1], and the texture coordinations are used to choose the pixel in the texture. For example, if the texture coordinations are (0, 0), then the pixel at the top-left corner of the texture is chosen. If the texture coordinations are (1, 1), then the pixel at the bottom-right corner of the texture is chosen. As for the pixel between the pixels, the texture is interpolated. This process is called texture sampling. The texture is sampled by the texture coordinations, and the color of the pixel is chosen. Previously, we have loaded variable texture with the picture. Now, we need to load the texture into the shader. We also need a sampler to sample the texture. They are all just special bindings. We also add the texture coordinations to the vertex shader, and use textureSample to let the fragment shader sample the texture. const getShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"shader\", code: ` @group(0) @binding(0) var<storage> states: array<u32>; @group(0) @binding(2) var grass_texture: texture_2d<f32>; @group(0) @binding(3) var grass_sampler: sampler; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) cell: u32, @location(1) texCoord: vec2<f32>, }; @vertex fn vertexMain(@location(0) pos: vec2f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { var output: VertexOutput; output.pos = vec4<f32>(pos, 0.0, 1.0); output.cell = vertexIndex / 6; output.texCoord = output.pos.xy * 0.5 + 0.5; return output; } @fragment fn fragmentMain(input: VertexOutput) -> @location(0) vec4<f32> { if(states[input.cell] == 0u) { discard; } return textureSample(grass_texture, grass_sampler, input.texCoord); } ` }); } Please note that the NDC is in the range of [-1, 1], so we need to convert it to [0, 1] by multiplying 0.5 and adding 0.5. The vec2<f32> is automatically interpolated by the rasterized, with the mode @interpolate(perspective) , which means that the texture coordinations are interpolated by the rasterize. Now, you can just regard it as a linear interpolation based on the coordinations. Now we need to pass the values with the bindings. Its the same as before, but we need to pass the texture and the sampler. const getBindGroupLayout = async (device: GPUDevice): Promise<GPUBindGroupLayout> => { const bindGroupLayout = device.createBindGroupLayout({ entries: [{ binding: 0, visibility: GPUShaderStage.FRAGMENT | GPUShaderStage.COMPUTE | GPUShaderStage.VERTEX, buffer: { type: \"read-only-storage\" } }, { binding: 1, visibility: GPUShaderStage.FRAGMENT | GPUShaderStage.COMPUTE, buffer: { type: \"storage\" } }, { binding: 2, visibility: GPUShaderStage.FRAGMENT, texture: {} }, { binding: 3, visibility: GPUShaderStage.FRAGMENT, sampler: {} }] }); return bindGroupLayout; } We have already created the texture, but not the sampler. The sampler is created by device.createSampler method. const getSampler = (device: GPUDevice): GPUSampler => { return device.createSampler({ magFilter: \"linear\", minFilter: \"linear\", }); } We just created a simple sampler with linear interpolation. There are also other options, like nearest for nearest neighbor interpolation. Finally, we just change the bind group creation to include the texture and the sampler. const bindGroup = device.createBindGroup({ label: \"bind group\", layout: bindGroupLayout, entries: [{ binding: 0, resource: { buffer: statesStorageBuffer[step % 2] }, }, { binding: 1, resource: { buffer: statesStorageBuffer[(step + 1) % 2] }, }, { binding: 2, resource: texture.createView(), }, { binding: 3, resource: sampler, }] }); Note that you need to pass a view of the texture, not the texture itself. Now we can see that each cell shows part of the grass texture. The texture is applied to the triangles. If you want each cell to show the full picture, you can change the texture coordinations to the following, @vertex fn vertexMain(@location(0) pos: vec2f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { var output: VertexOutput; output.pos = vec4<f32>(pos, 0.0, 1.0); output.cell = vertexIndex / 6; if(vertexIndex % 6 == 0) { output.texCoord = vec2<f32>(0.0, 0.0); } else if(vertexIndex % 6 == 1 || vertexIndex % 6 == 4) { output.texCoord = vec2<f32>(1.0, 0.0); } else if(vertexIndex % 6 == 2 || vertexIndex % 6 == 3) { output.texCoord = vec2<f32>(0.0, 1.0); } else { output.texCoord = vec2<f32>(1.0, 1.0); } return output; } Now, each cell shows the full picture, this is because the texture coordinations are set to the corners of the texture.","title":"Using Texture"},{"location":"04/","text":"Triangles on Web Ch4 Orthographic Projection From this part, we will go into the world of 3D graphics. We will learn about the basic concepts of 3D rendering. But before building the simulation, we have a lot to learn- perspective and lighting, to be specific. In this part, we will learn about perspective. We will learn how to project 3D objects onto a 2D screen, and how to create a perspective camera. Of course, a simpler way would be to use orthographic projection, which we will also learn about. Perspective is a projection method that provide verisimilitude to the 3D world. It is the most common projection method in computer graphics. But previously, we learnt that NDC has three axis. So why do we need to project 3D objects onto a 2D screen? The z axis of NDC is depth, not the distance from the camera. So, instead of the z-axis, it is more like a layer of the 3D world. The smaller z-axis will get prioritized in things like depth test. Orthographic Projection Orthographic projection is a projection method that does not take perspective into account. That is, it does not make objects smaller as they go further away from the camera. This is useful for technical drawings, but not for realistic rendering. Orthographic projection is a projection, that is, in a mathematical sense, view_{point} = orthographic( model_{point}) Where veiw_{point} is the NDC point, and model_{point} is the 3D point. The orthographic function is the orthographic projection function. Now let's see how we should implement the orthographic function. How Orthographic Projection Works In orthographic projection, it is said that straight lines should remain straight, and parallel lines should remain parallel. Or alteratively, the transformation should be linear. The left is the actual 3D model, also called the world space. The right is the 2D screen, also called the screen space. Depending on the axis of the projected view, the orthographic projection can be divided into many types, The first panel is perspective projection, which we will learn about later, it is for comparison. The above image shows different kind of orthographic projections. But actually, there is only one kind of orthographic projection, if we treat their difference as parameters. That is, we can use a matrix to represent orthographic projection. Now let's do the math. Using Matrix for Orthographic Projection Before we do the math, we have two sets of axis to consider, The world axis, which is the actual 3D model. The screen axis, which is the actual 2D screen. x-axis is always horizontal, y-axis is always vertical. They will later be noted with subscripts w and s respectively. In orthographic projection, because during the transformation, the parallel lines should remain parallel, we can conclude that it is a linear transformation. That is, we can use a matrix to represent it. To actually get the matrix, we need to know how the bases of the world axis are projected onto the screen axis. The different choices of the bases will result in different orthographic projections. But typically, y-axis will remain the same, and z-axis will be projected. So, we can have the following matrix, r_s = M \\cdot r_w Where r_s is the screen space vector, r_w is the world space vector, and M is the orthographic projection matrix, varied based on the orthographic projection type. Some typical orthographic projections are shown below, Top View In top view, the x-axis is projected onto the x-axis, and the z-axis is projected onto the y-axis. \\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ \\end{bmatrix} This just puts z-axis onto the x-axis, then completely ignores the world x-axis. Front View In front view, the x-axis is projected onto the x-axis, and the y-axis is projected onto the y-axis. \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ \\end{bmatrix} This just puts the world axis onto the screen axis, while ignoring the z-axis. Isometric View In isometric view, the projected x-axis, y-axis and z-axis will have a 120 degree angle between each other. \\begin{bmatrix} \\cos(\\frac{\\pi}{6}) & 0 & -\\cos(\\frac{\\pi}{6}) \\\\ -\\sin(\\frac{\\pi}{6}) & 1 & -\\sin(\\frac{\\pi}{6}) \\\\ \\end{bmatrix} This is a bit more complicated, but it allows us to actually see the 3D object, instead of just one view. Oblique View In oblique view, the x-axis and y-axis stays, z-axis will be 45 degree to the x-axis, with a compression factor of \\frac{1}{2} . \\begin{bmatrix} 1 & 0 & \\frac{1}{2\\sqrt{2}} \\\\ 0 & 1 & \\frac{1}{2\\sqrt{2}} \\\\ \\end{bmatrix} This is the most traditional orthographic projection, and is used in technical drawings. Show a Cube with Orthographic Projection Now, let's recapitulate on how to setup WebGPU. First, get an adapter, then request a device. const requestDevice = async (): Promise<[GPUAdapter, GPUDevice] | null> => { const adapter = await navigator.gpu.requestAdapter(); if (!adapter) { console.error('WebGPU not supported'); return null; } const device = await adapter.requestDevice(); console.log(device); return [adapter, device]; } Then, we need to get the canvas and its context. const getContext = async (device: GPUDevice): Promise<[GPUCanvasContext, GPUTextureFormat]> => { const canvas = document.getElementById('app') as HTMLCanvasElement; const context = canvas.getContext(\"webgpu\")!; const canvasFormat = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device: device, format: canvasFormat, }); return [context, canvasFormat]; } Then, we need a shader module. Like we previously discussed, the transformation of Orthographic Projection is a point-wise operation, so we only need a vertex shader. We need to perform matrix multiplication in the shader. const get_shader = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"sm\", code: ` @vertex fn vertexMain(@location(0) position: vec3<f32>) -> vec4<f32> { var projection: mat3x2<f32> = mat3x2<f32>( vec2<f32>(1.0, 0.0), vec2<f32>(0.0, 1.0), vec2<f32>(0.3535533906, 0.3535533906) ); return vec4<f32>(projection * position, 0.0, 1.0); } @fragment fn fragmentMain() -> vec4<f32> { return vec4<f32>(0.0, 0.0, 1.0, 1.0); } `, }) } Here we used the oblique view matrix, which is the most traditional orthographic projection matrix. We also used a blue color for the cube. Then, the vertex buffer, const get_vertices = (device: GPUDevice): [GPUBuffer, GPUVertexBufferLayout] => { // cube // use triangle const vertices = new Float32Array([ // xy 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, // yz 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, // zx 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, ]).map((v) => v * 0.5 - 0.25); const layout: GPUVertexBufferLayout = { arrayStride: 3 * 4, attributes: [{ format: \"float32x3\", offset: 0, shaderLocation: 0, }] } const buffer = device.createBuffer({ size: vertices.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }) device.queue.writeBuffer(buffer, 0, vertices.buffer); return [buffer, layout]; } Then the render pipeline, const encoder = device.createCommandEncoder(); const renderPassDescriptor: GPURenderPassDescriptor = { colorAttachments: [{ view: ctx.getCurrentTexture().createView(), clearValue: { r: 0.1, g: 0.3, b: 0.2, a: 1.0 }, storeOp: \"store\", loadOp: \"clear\", }] } const pass = encoder.beginRenderPass(renderPassDescriptor); const pipelineLayout = device.createPipelineLayout({ bindGroupLayouts: [], }); const pipeline = device.createRenderPipeline({ layout: pipelineLayout, vertex: { module: shader, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout], }, fragment: { module: shader, entryPoint: \"fragmentMain\", targets: [{ format: format, }] }, }); Finally, let's set off the render, pass.setPipeline(pipeline); pass.setVertexBuffer(0, vertices); pass.draw(6 * 2 * 3); pass.end(); const command = encoder.finish(); device.queue.submit([command]); Now, you should see a cube-shaped object on the screen. Of course, you can also try other orthographic projections. However, there is a problem- the perspective matrix is used in every vertex and the same- but they need to be created for each render. The way to solve this is to use a uniform buffer. Using Uniform Buffer A uniform buffer is a buffer that is shared between the CPU and the GPU. It is used to store data that is shared between all vertices or fragments. Different from storage buffer, uniform buffer is read-only, and is used to store data that is constant during the rendering process. To use a uniform buffer, we need to create a buffer, and then bind it to the pipeline. const projectionMatrix = new Float32Array([ 1.0, 0.0, 0.0, 1.0, 0.3535533906, 0.3535533906, ]); const projectionBuffer = device.createBuffer({ size: 6 * 4, usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST, }); device.queue.writeBuffer( projectionBuffer, 0, projectionMatrix.buffer, ) const bindGroupLayout = device.createBindGroupLayout({ entries: [ { binding: 0, visibility: GPUShaderStage.VERTEX, buffer: { type: \"uniform\" } } ] }); const bindGroup = device.createBindGroup({ layout: bindGroupLayout, entries: [ { binding: 0, resource: { buffer: projectionBuffer, } } ] }); const pipelineLayout = device.createPipelineLayout({ bindGroupLayouts: [bindGroupLayout] }); pass.setBindGroup(0, bindGroup); @group(0) @binding(0) var<uniform> projection: mat3x2<f32>; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, }; @vertex fn vertexMain(@location(0) position: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let projected = projection * position; let final_position = vec4<f32>(projected, 0.0, 1.0); var output = VertexOutput(final_position, vertexIndex / 6); return output; } Now the projection matrix is shared between all vertices. Color We would like different colors for different faces. We can do this by passing the color as an attribute to the vertex shader. const get_shader = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"sm\", code: ` struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, }; @vertex fn vertexMain(@location(0) position: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let projection: mat3x2<f32> = mat3x2<f32>( vec2<f32>(1.0, 0.0), vec2<f32>(0.0, 1.0), vec2<f32>(0.3535533906, 0.3535533906) ); let projected = projection * position; let final_position = vec4<f32>(projected, 0.0, 1.0); var output = VertexOutput(final_position, vertexIndex / 6); return output; } @fragment fn fragmentMain(input: VertexOutput) -> vec4<f32> { if (input.face == 0u) { return vec4<f32>(1.0, 0.0, 0.0, 1.0); } else if (input.face == 1u) { return vec4<f32>(0.0, 1.0, 0.0, 1.0); } else if (input.face == 2u) { return vec4<f32>(0.0, 0.0, 1.0, 1.0); } else if (input.face == 3u) { return vec4<f32>(1.0, 1.0, 0.0, 1.0); } else if (input.face == 4u) { return vec4<f32>(1.0, 0.0, 1.0, 1.0); } else { return vec4<f32>(0.0, 1.0, 1.0, 1.0); } } `, }) } But as you do this, you'd find something horrible- we do not have depth system, and thus, the back faces overlap the front faces. Of course, an easy way to solve this just adding the original depth value to the fragment shader, then discard the fragment if the depth value is larger. However, this is so common and most GPU have a built-in depth test system. Manual Depth Test Here we will manually write the depth value to the depth texture. Later, we will learn about using default depth test system. Depth test is a test that is performed to determine whether a fragment should be drawn. It is based on the depth value of the fragment. In depth test, there is a buffer called the depth texture, which stores the depth value of each pixel. Vertex shader will create multiple primitives, which will be rasterized into fragments. When this happens, the depth value of the fragment will be compared with the depth value in the depth texture. If the depth value of the fragment is smaller, the fragment will be drawn. For example, if we have two overlapping triangles, when the front triangle is drawn, the depth value of the fragment will be written to the depth texture. When the back triangle is drawn, the depth value of the fragment will be compared with the depth value in the depth texture. If the depth value of the fragment is larger, the fragment will be discarded. Again, fragment is just a fancy name for pixel. From the above, you can tell that there needs to be a space where we store the depth value of each pixel. This is the depth texture. Let's create a depth texture. const get_depth_texture = (device: GPUDevice, size: { width: number, height: number }): GPUTexture => { return device.createTexture({ size: { width: size.width, height: size.height, depthOrArrayLayers: 1 }, format: \"depth24plus\", usage: GPUTextureUsage.RENDER_ATTACHMENT, }); } Please note one parameter, depthOrArrayLayers . When we are talking about 3D textures, this parameter will be the depth of the texture. But when we are talking about depth textures, this parameter will be the number of layers. However, when performing depth test, obviously, we only need a 2D layer for storing the depth value. Now we add a new attachment, the depthAndStencilAttachment . const renderPassDescriptor: GPURenderPassDescriptor = { colorAttachments: [{ view: ctx.getCurrentTexture().createView(), clearValue: { r: 0.1, g: 0.3, b: 0.2, a: 1.0 }, storeOp: \"store\", loadOp: \"clear\", }], depthStencilAttachment: { view: depthTexture.createView(), depthClearValue: 1.0, depthStoreOp: \"store\", depthLoadOp: \"clear\", } } // ... const pipeline = device.createRenderPipeline({ layout: pipelineLayout, vertex: { module: shader, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout], }, fragment: { module: shader, entryPoint: \"fragmentMain\", targets: [{ format: format, }] }, depthStencil: { format: \"depth24plus\", depthWriteEnabled: true, depthCompare: \"less\", } }); Remember what an attachment is? It is something that is attached to the render pass, or the target of the render pass. In this case, the depth texture is attached to the render pass. depthCompare is the comparison function. It is used to compare the depth value of the fragment with the depth value in the depth texture. If set to less , the smaller depth value will be drawn. depthWriteEnabled is used to determine whether the depth value of the fragment will be written to the depth texture. If set to false , the depth value of the fragment will not be written to the depth texture. Now you see a cube with different colors on different faces- but some color block are still wrongly overlapped. This is because we did the perspective ourselves- and left z-axis as 0. This is not the actual depth value, and the depth test system will not work. To solve this, we need to write the depth value to the depth texture, or you can correctly set z-axis. Since we haven't yet talked about clip space, we will write the depth value to the depth texture. First, we need to ask the vertex shader to output the depth value. Please note that the depth should be within 0 to 1, so normalize the depth value (although we did not use the part of the z that's smaller than one, you can't ignore that depth or else the rendering will be wrong, we still have to divide by 2 then add 0.5). struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, @location(1) depth: f32, }; @vertex fn vertexMain(@location(0) position: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let projected = projection * position; let final_position = vec4<f32>(projected, 0.0, 1.0); let depth = position.z * 0.5 + 0.5; var output = VertexOutput(final_position, vertexIndex / 6, depth); return output; } Instead of returning only the color attachment, which is defined by location (since there allows multiple color attachments), we also return the depth value, it is @builtin(frag_depth) . struct FragmentOutput { @location(0) color: vec4<f32>, @builtin(frag_depth) depth: f32, }; @fragment fn fragmentMain(input: VertexOutput) -> FragmentOutput { var output = FragmentOutput(vec4<f32>(1.0, 1.0, 1.0, 1.0), input.depth); if (input.face == 0u) { output.color = vec4<f32>(1.0, 0.0, 0.0, 1.0); } else if (input.face == 1u) { output.color = vec4<f32>(0.0, 1.0, 0.0, 1.0); } else if (input.face == 2u) { output.color = vec4<f32>(0.0, 0.0, 1.0, 1.0); } else if (input.face == 3u) { output.color = vec4<f32>(1.0, 1.0, 0.0, 1.0); } else if (input.face == 4u) { output.color = vec4<f32>(1.0, 0.0, 1.0, 1.0); } else { output.color = vec4<f32>(0.0, 1.0, 1.0, 1.0); } return output; } Now you should see a perfect cube. However, it definitely seems a bit off- since orthographic projection is not perspective projection, and although it has the same length in the front and in the back for a pair of parallel lines, your eyes believes that the lines that are further away should be shorter. Now, they are of the same length, so you perceive the cube as the farther side being larger. You can adjust the orthographic projection matrix to make the cube look more realistic. There might be a small margin of face that should have been drawn but is not. This is because of some floating point error when the value is compared. You can adjust the comparison function to less-equal to mitigate this. But if you want to eliminate this, you need some fine-tuning on the depth value- we are not doing that here since it doesn't affect the overall look too much. You can also set different comparison functions for the depth test, to see how other comparison functions work. Rotate the Cube Now, let's get the cube moving. We have a simple way- we will have a slider on the web page, then pass the angle to the vertex shader to ask it to rotate the cube. <input type=\"range\" id=\"angle\" min=\"-180\" max=\"180\" step=\"0.1\" value=\"0\" /> We need a render loop. const render = () => { const depthTexture = get_depth_texture(device, { width: ctx.canvas.width, height: ctx.canvas.height }); const encoder = device.createCommandEncoder(); const renderPassDescriptor: GPURenderPassDescriptor = { colorAttachments: [{ view: ctx.getCurrentTexture().createView(), clearValue: { r: 0.1, g: 0.3, b: 0.2, a: 1.0 }, storeOp: \"store\", loadOp: \"clear\", }], depthStencilAttachment: { view: depthTexture.createView(), depthClearValue: 1.0, depthStoreOp: \"store\", depthLoadOp: \"clear\", } } const pass = encoder.beginRenderPass(renderPassDescriptor); const bindGroupLayout = device.createBindGroupLayout({ entries: [ { binding: 0, visibility: GPUShaderStage.VERTEX, buffer: { type: \"uniform\" } } ] }); const bindGroup = device.createBindGroup({ layout: bindGroupLayout, entries: [ { binding: 0, resource: { buffer: projectionBuffer, } } ] }); const pipelineLayout = device.createPipelineLayout({ bindGroupLayouts: [bindGroupLayout] }); const pipeline = device.createRenderPipeline({ layout: pipelineLayout, vertex: { module: shader, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout], }, fragment: { module: shader, entryPoint: \"fragmentMain\", targets: [{ format: format, }] }, depthStencil: { format: \"depth24plus\", depthWriteEnabled: true, depthCompare: \"less-equal\", } }); pass.setPipeline(pipeline); pass.setBindGroup(0, bindGroup); pass.setVertexBuffer(0, vertices); pass.draw(6 * 2 * 3); pass.end(); const command = encoder.finish(); device.queue.submit([command]); } setInterval(render, 1000 / 60); We will use uniform buffer to pass the angle to the vertex shader. const angle = document.getElementById(\"angle\") as HTMLInputElement; const angleBuffer = new Float32Array([parseFloat(angle.value) * ( Math.PI / 180 )]); const angleBufferGPU = device.createBuffer({ size: 4, usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST, }); device.queue.writeBuffer( angleBufferGPU, 0, angleBuffer.buffer, ); const bindGroupLayout = device.createBindGroupLayout({ entries: [ { binding: 0, visibility: GPUShaderStage.VERTEX, buffer: { type: \"uniform\" } }, { binding: 1, visibility: GPUShaderStage.VERTEX, buffer: { type: \"uniform\" } } ] }); const bindGroup = device.createBindGroup({ layout: bindGroupLayout, entries: [ { binding: 0, resource: { buffer: projectionBuffer, } }, { binding: 1, resource: { buffer: angleBufferGPU, } } ] }); Now, just multiply a rotation matrix before sending position to the projection matrix. const get_shader = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"sm\", code: ` @group(0) @binding(0) var<uniform> projection: mat3x2<f32>; @group(0) @binding(1) var<uniform> angle: f32; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, @location(1) depth: f32, }; @vertex fn vertexMain(@location(0) position: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(cos(angle), 0.0, sin(angle)), vec3<f32>(0.0, 1.0, 0.0), vec3<f32>(-sin(angle), 0.0, cos(angle)) ); let rotated = rotation * position; let projected = projection * rotated; let final_position = vec4<f32>(projected, 0.0, 1.0); let depth = rotated.z * 0.5 + 0.5; var output = VertexOutput(final_position, vertexIndex / 6, depth); return output; } struct FragmentOutput { @location(0) color: vec4<f32>, @builtin(frag_depth) depth: f32, }; @fragment fn fragmentMain(input: VertexOutput) -> FragmentOutput { var output = FragmentOutput(vec4<f32>(1.0, 1.0, 1.0, 1.0), input.depth); if (input.face == 0u) { output.color = vec4<f32>(1.0, 0.0, 0.0, 1.0); } else if (input.face == 1u) { output.color = vec4<f32>(0.0, 1.0, 0.0, 1.0); } else if (input.face == 2u) { output.color = vec4<f32>(0.0, 0.0, 1.0, 1.0); } else if (input.face == 3u) { output.color = vec4<f32>(1.0, 1.0, 0.0, 1.0); } else if (input.face == 4u) { output.color = vec4<f32>(1.0, 0.0, 1.0, 1.0); } else { output.color = vec4<f32>(0.0, 1.0, 1.0, 1.0); } return output; } `, }) } Now you can rotate the cube with the slider. When doing rotation, it is better to adopt isometric view of the orthographic projection, since it is more intuitive.","title":"Orthographic Projection"},{"location":"04/#triangles-on-web-ch4-orthographic-projection","text":"From this part, we will go into the world of 3D graphics. We will learn about the basic concepts of 3D rendering. But before building the simulation, we have a lot to learn- perspective and lighting, to be specific. In this part, we will learn about perspective. We will learn how to project 3D objects onto a 2D screen, and how to create a perspective camera. Of course, a simpler way would be to use orthographic projection, which we will also learn about. Perspective is a projection method that provide verisimilitude to the 3D world. It is the most common projection method in computer graphics. But previously, we learnt that NDC has three axis. So why do we need to project 3D objects onto a 2D screen? The z axis of NDC is depth, not the distance from the camera. So, instead of the z-axis, it is more like a layer of the 3D world. The smaller z-axis will get prioritized in things like depth test.","title":"Triangles on Web Ch4 Orthographic Projection"},{"location":"04/#orthographic-projection","text":"Orthographic projection is a projection method that does not take perspective into account. That is, it does not make objects smaller as they go further away from the camera. This is useful for technical drawings, but not for realistic rendering. Orthographic projection is a projection, that is, in a mathematical sense, view_{point} = orthographic( model_{point}) Where veiw_{point} is the NDC point, and model_{point} is the 3D point. The orthographic function is the orthographic projection function. Now let's see how we should implement the orthographic function.","title":"Orthographic Projection"},{"location":"04/#how-orthographic-projection-works","text":"In orthographic projection, it is said that straight lines should remain straight, and parallel lines should remain parallel. Or alteratively, the transformation should be linear. The left is the actual 3D model, also called the world space. The right is the 2D screen, also called the screen space. Depending on the axis of the projected view, the orthographic projection can be divided into many types, The first panel is perspective projection, which we will learn about later, it is for comparison. The above image shows different kind of orthographic projections. But actually, there is only one kind of orthographic projection, if we treat their difference as parameters. That is, we can use a matrix to represent orthographic projection. Now let's do the math.","title":"How Orthographic Projection Works"},{"location":"04/#using-matrix-for-orthographic-projection","text":"Before we do the math, we have two sets of axis to consider, The world axis, which is the actual 3D model. The screen axis, which is the actual 2D screen. x-axis is always horizontal, y-axis is always vertical. They will later be noted with subscripts w and s respectively. In orthographic projection, because during the transformation, the parallel lines should remain parallel, we can conclude that it is a linear transformation. That is, we can use a matrix to represent it. To actually get the matrix, we need to know how the bases of the world axis are projected onto the screen axis. The different choices of the bases will result in different orthographic projections. But typically, y-axis will remain the same, and z-axis will be projected. So, we can have the following matrix, r_s = M \\cdot r_w Where r_s is the screen space vector, r_w is the world space vector, and M is the orthographic projection matrix, varied based on the orthographic projection type. Some typical orthographic projections are shown below,","title":"Using Matrix for Orthographic Projection"},{"location":"04/#top-view","text":"In top view, the x-axis is projected onto the x-axis, and the z-axis is projected onto the y-axis. \\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ \\end{bmatrix} This just puts z-axis onto the x-axis, then completely ignores the world x-axis.","title":"Top View"},{"location":"04/#front-view","text":"In front view, the x-axis is projected onto the x-axis, and the y-axis is projected onto the y-axis. \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ \\end{bmatrix} This just puts the world axis onto the screen axis, while ignoring the z-axis.","title":"Front View"},{"location":"04/#isometric-view","text":"In isometric view, the projected x-axis, y-axis and z-axis will have a 120 degree angle between each other. \\begin{bmatrix} \\cos(\\frac{\\pi}{6}) & 0 & -\\cos(\\frac{\\pi}{6}) \\\\ -\\sin(\\frac{\\pi}{6}) & 1 & -\\sin(\\frac{\\pi}{6}) \\\\ \\end{bmatrix} This is a bit more complicated, but it allows us to actually see the 3D object, instead of just one view.","title":"Isometric View"},{"location":"04/#oblique-view","text":"In oblique view, the x-axis and y-axis stays, z-axis will be 45 degree to the x-axis, with a compression factor of \\frac{1}{2} . \\begin{bmatrix} 1 & 0 & \\frac{1}{2\\sqrt{2}} \\\\ 0 & 1 & \\frac{1}{2\\sqrt{2}} \\\\ \\end{bmatrix} This is the most traditional orthographic projection, and is used in technical drawings.","title":"Oblique View"},{"location":"04/#show-a-cube-with-orthographic-projection","text":"Now, let's recapitulate on how to setup WebGPU. First, get an adapter, then request a device. const requestDevice = async (): Promise<[GPUAdapter, GPUDevice] | null> => { const adapter = await navigator.gpu.requestAdapter(); if (!adapter) { console.error('WebGPU not supported'); return null; } const device = await adapter.requestDevice(); console.log(device); return [adapter, device]; } Then, we need to get the canvas and its context. const getContext = async (device: GPUDevice): Promise<[GPUCanvasContext, GPUTextureFormat]> => { const canvas = document.getElementById('app') as HTMLCanvasElement; const context = canvas.getContext(\"webgpu\")!; const canvasFormat = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device: device, format: canvasFormat, }); return [context, canvasFormat]; } Then, we need a shader module. Like we previously discussed, the transformation of Orthographic Projection is a point-wise operation, so we only need a vertex shader. We need to perform matrix multiplication in the shader. const get_shader = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"sm\", code: ` @vertex fn vertexMain(@location(0) position: vec3<f32>) -> vec4<f32> { var projection: mat3x2<f32> = mat3x2<f32>( vec2<f32>(1.0, 0.0), vec2<f32>(0.0, 1.0), vec2<f32>(0.3535533906, 0.3535533906) ); return vec4<f32>(projection * position, 0.0, 1.0); } @fragment fn fragmentMain() -> vec4<f32> { return vec4<f32>(0.0, 0.0, 1.0, 1.0); } `, }) } Here we used the oblique view matrix, which is the most traditional orthographic projection matrix. We also used a blue color for the cube. Then, the vertex buffer, const get_vertices = (device: GPUDevice): [GPUBuffer, GPUVertexBufferLayout] => { // cube // use triangle const vertices = new Float32Array([ // xy 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, // yz 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, // zx 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, ]).map((v) => v * 0.5 - 0.25); const layout: GPUVertexBufferLayout = { arrayStride: 3 * 4, attributes: [{ format: \"float32x3\", offset: 0, shaderLocation: 0, }] } const buffer = device.createBuffer({ size: vertices.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }) device.queue.writeBuffer(buffer, 0, vertices.buffer); return [buffer, layout]; } Then the render pipeline, const encoder = device.createCommandEncoder(); const renderPassDescriptor: GPURenderPassDescriptor = { colorAttachments: [{ view: ctx.getCurrentTexture().createView(), clearValue: { r: 0.1, g: 0.3, b: 0.2, a: 1.0 }, storeOp: \"store\", loadOp: \"clear\", }] } const pass = encoder.beginRenderPass(renderPassDescriptor); const pipelineLayout = device.createPipelineLayout({ bindGroupLayouts: [], }); const pipeline = device.createRenderPipeline({ layout: pipelineLayout, vertex: { module: shader, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout], }, fragment: { module: shader, entryPoint: \"fragmentMain\", targets: [{ format: format, }] }, }); Finally, let's set off the render, pass.setPipeline(pipeline); pass.setVertexBuffer(0, vertices); pass.draw(6 * 2 * 3); pass.end(); const command = encoder.finish(); device.queue.submit([command]); Now, you should see a cube-shaped object on the screen. Of course, you can also try other orthographic projections. However, there is a problem- the perspective matrix is used in every vertex and the same- but they need to be created for each render. The way to solve this is to use a uniform buffer.","title":"Show a Cube with Orthographic Projection"},{"location":"04/#using-uniform-buffer","text":"A uniform buffer is a buffer that is shared between the CPU and the GPU. It is used to store data that is shared between all vertices or fragments. Different from storage buffer, uniform buffer is read-only, and is used to store data that is constant during the rendering process. To use a uniform buffer, we need to create a buffer, and then bind it to the pipeline. const projectionMatrix = new Float32Array([ 1.0, 0.0, 0.0, 1.0, 0.3535533906, 0.3535533906, ]); const projectionBuffer = device.createBuffer({ size: 6 * 4, usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST, }); device.queue.writeBuffer( projectionBuffer, 0, projectionMatrix.buffer, ) const bindGroupLayout = device.createBindGroupLayout({ entries: [ { binding: 0, visibility: GPUShaderStage.VERTEX, buffer: { type: \"uniform\" } } ] }); const bindGroup = device.createBindGroup({ layout: bindGroupLayout, entries: [ { binding: 0, resource: { buffer: projectionBuffer, } } ] }); const pipelineLayout = device.createPipelineLayout({ bindGroupLayouts: [bindGroupLayout] }); pass.setBindGroup(0, bindGroup); @group(0) @binding(0) var<uniform> projection: mat3x2<f32>; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, }; @vertex fn vertexMain(@location(0) position: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let projected = projection * position; let final_position = vec4<f32>(projected, 0.0, 1.0); var output = VertexOutput(final_position, vertexIndex / 6); return output; } Now the projection matrix is shared between all vertices.","title":"Using Uniform Buffer"},{"location":"04/#color","text":"We would like different colors for different faces. We can do this by passing the color as an attribute to the vertex shader. const get_shader = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"sm\", code: ` struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, }; @vertex fn vertexMain(@location(0) position: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let projection: mat3x2<f32> = mat3x2<f32>( vec2<f32>(1.0, 0.0), vec2<f32>(0.0, 1.0), vec2<f32>(0.3535533906, 0.3535533906) ); let projected = projection * position; let final_position = vec4<f32>(projected, 0.0, 1.0); var output = VertexOutput(final_position, vertexIndex / 6); return output; } @fragment fn fragmentMain(input: VertexOutput) -> vec4<f32> { if (input.face == 0u) { return vec4<f32>(1.0, 0.0, 0.0, 1.0); } else if (input.face == 1u) { return vec4<f32>(0.0, 1.0, 0.0, 1.0); } else if (input.face == 2u) { return vec4<f32>(0.0, 0.0, 1.0, 1.0); } else if (input.face == 3u) { return vec4<f32>(1.0, 1.0, 0.0, 1.0); } else if (input.face == 4u) { return vec4<f32>(1.0, 0.0, 1.0, 1.0); } else { return vec4<f32>(0.0, 1.0, 1.0, 1.0); } } `, }) } But as you do this, you'd find something horrible- we do not have depth system, and thus, the back faces overlap the front faces. Of course, an easy way to solve this just adding the original depth value to the fragment shader, then discard the fragment if the depth value is larger. However, this is so common and most GPU have a built-in depth test system.","title":"Color"},{"location":"04/#manual-depth-test","text":"Here we will manually write the depth value to the depth texture. Later, we will learn about using default depth test system. Depth test is a test that is performed to determine whether a fragment should be drawn. It is based on the depth value of the fragment. In depth test, there is a buffer called the depth texture, which stores the depth value of each pixel. Vertex shader will create multiple primitives, which will be rasterized into fragments. When this happens, the depth value of the fragment will be compared with the depth value in the depth texture. If the depth value of the fragment is smaller, the fragment will be drawn. For example, if we have two overlapping triangles, when the front triangle is drawn, the depth value of the fragment will be written to the depth texture. When the back triangle is drawn, the depth value of the fragment will be compared with the depth value in the depth texture. If the depth value of the fragment is larger, the fragment will be discarded. Again, fragment is just a fancy name for pixel. From the above, you can tell that there needs to be a space where we store the depth value of each pixel. This is the depth texture. Let's create a depth texture. const get_depth_texture = (device: GPUDevice, size: { width: number, height: number }): GPUTexture => { return device.createTexture({ size: { width: size.width, height: size.height, depthOrArrayLayers: 1 }, format: \"depth24plus\", usage: GPUTextureUsage.RENDER_ATTACHMENT, }); } Please note one parameter, depthOrArrayLayers . When we are talking about 3D textures, this parameter will be the depth of the texture. But when we are talking about depth textures, this parameter will be the number of layers. However, when performing depth test, obviously, we only need a 2D layer for storing the depth value. Now we add a new attachment, the depthAndStencilAttachment . const renderPassDescriptor: GPURenderPassDescriptor = { colorAttachments: [{ view: ctx.getCurrentTexture().createView(), clearValue: { r: 0.1, g: 0.3, b: 0.2, a: 1.0 }, storeOp: \"store\", loadOp: \"clear\", }], depthStencilAttachment: { view: depthTexture.createView(), depthClearValue: 1.0, depthStoreOp: \"store\", depthLoadOp: \"clear\", } } // ... const pipeline = device.createRenderPipeline({ layout: pipelineLayout, vertex: { module: shader, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout], }, fragment: { module: shader, entryPoint: \"fragmentMain\", targets: [{ format: format, }] }, depthStencil: { format: \"depth24plus\", depthWriteEnabled: true, depthCompare: \"less\", } }); Remember what an attachment is? It is something that is attached to the render pass, or the target of the render pass. In this case, the depth texture is attached to the render pass. depthCompare is the comparison function. It is used to compare the depth value of the fragment with the depth value in the depth texture. If set to less , the smaller depth value will be drawn. depthWriteEnabled is used to determine whether the depth value of the fragment will be written to the depth texture. If set to false , the depth value of the fragment will not be written to the depth texture. Now you see a cube with different colors on different faces- but some color block are still wrongly overlapped. This is because we did the perspective ourselves- and left z-axis as 0. This is not the actual depth value, and the depth test system will not work. To solve this, we need to write the depth value to the depth texture, or you can correctly set z-axis. Since we haven't yet talked about clip space, we will write the depth value to the depth texture. First, we need to ask the vertex shader to output the depth value. Please note that the depth should be within 0 to 1, so normalize the depth value (although we did not use the part of the z that's smaller than one, you can't ignore that depth or else the rendering will be wrong, we still have to divide by 2 then add 0.5). struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, @location(1) depth: f32, }; @vertex fn vertexMain(@location(0) position: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let projected = projection * position; let final_position = vec4<f32>(projected, 0.0, 1.0); let depth = position.z * 0.5 + 0.5; var output = VertexOutput(final_position, vertexIndex / 6, depth); return output; } Instead of returning only the color attachment, which is defined by location (since there allows multiple color attachments), we also return the depth value, it is @builtin(frag_depth) . struct FragmentOutput { @location(0) color: vec4<f32>, @builtin(frag_depth) depth: f32, }; @fragment fn fragmentMain(input: VertexOutput) -> FragmentOutput { var output = FragmentOutput(vec4<f32>(1.0, 1.0, 1.0, 1.0), input.depth); if (input.face == 0u) { output.color = vec4<f32>(1.0, 0.0, 0.0, 1.0); } else if (input.face == 1u) { output.color = vec4<f32>(0.0, 1.0, 0.0, 1.0); } else if (input.face == 2u) { output.color = vec4<f32>(0.0, 0.0, 1.0, 1.0); } else if (input.face == 3u) { output.color = vec4<f32>(1.0, 1.0, 0.0, 1.0); } else if (input.face == 4u) { output.color = vec4<f32>(1.0, 0.0, 1.0, 1.0); } else { output.color = vec4<f32>(0.0, 1.0, 1.0, 1.0); } return output; } Now you should see a perfect cube. However, it definitely seems a bit off- since orthographic projection is not perspective projection, and although it has the same length in the front and in the back for a pair of parallel lines, your eyes believes that the lines that are further away should be shorter. Now, they are of the same length, so you perceive the cube as the farther side being larger. You can adjust the orthographic projection matrix to make the cube look more realistic. There might be a small margin of face that should have been drawn but is not. This is because of some floating point error when the value is compared. You can adjust the comparison function to less-equal to mitigate this. But if you want to eliminate this, you need some fine-tuning on the depth value- we are not doing that here since it doesn't affect the overall look too much. You can also set different comparison functions for the depth test, to see how other comparison functions work.","title":"Manual Depth Test"},{"location":"04/#rotate-the-cube","text":"Now, let's get the cube moving. We have a simple way- we will have a slider on the web page, then pass the angle to the vertex shader to ask it to rotate the cube. <input type=\"range\" id=\"angle\" min=\"-180\" max=\"180\" step=\"0.1\" value=\"0\" /> We need a render loop. const render = () => { const depthTexture = get_depth_texture(device, { width: ctx.canvas.width, height: ctx.canvas.height }); const encoder = device.createCommandEncoder(); const renderPassDescriptor: GPURenderPassDescriptor = { colorAttachments: [{ view: ctx.getCurrentTexture().createView(), clearValue: { r: 0.1, g: 0.3, b: 0.2, a: 1.0 }, storeOp: \"store\", loadOp: \"clear\", }], depthStencilAttachment: { view: depthTexture.createView(), depthClearValue: 1.0, depthStoreOp: \"store\", depthLoadOp: \"clear\", } } const pass = encoder.beginRenderPass(renderPassDescriptor); const bindGroupLayout = device.createBindGroupLayout({ entries: [ { binding: 0, visibility: GPUShaderStage.VERTEX, buffer: { type: \"uniform\" } } ] }); const bindGroup = device.createBindGroup({ layout: bindGroupLayout, entries: [ { binding: 0, resource: { buffer: projectionBuffer, } } ] }); const pipelineLayout = device.createPipelineLayout({ bindGroupLayouts: [bindGroupLayout] }); const pipeline = device.createRenderPipeline({ layout: pipelineLayout, vertex: { module: shader, entryPoint: \"vertexMain\", buffers: [vertexBufferLayout], }, fragment: { module: shader, entryPoint: \"fragmentMain\", targets: [{ format: format, }] }, depthStencil: { format: \"depth24plus\", depthWriteEnabled: true, depthCompare: \"less-equal\", } }); pass.setPipeline(pipeline); pass.setBindGroup(0, bindGroup); pass.setVertexBuffer(0, vertices); pass.draw(6 * 2 * 3); pass.end(); const command = encoder.finish(); device.queue.submit([command]); } setInterval(render, 1000 / 60); We will use uniform buffer to pass the angle to the vertex shader. const angle = document.getElementById(\"angle\") as HTMLInputElement; const angleBuffer = new Float32Array([parseFloat(angle.value) * ( Math.PI / 180 )]); const angleBufferGPU = device.createBuffer({ size: 4, usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST, }); device.queue.writeBuffer( angleBufferGPU, 0, angleBuffer.buffer, ); const bindGroupLayout = device.createBindGroupLayout({ entries: [ { binding: 0, visibility: GPUShaderStage.VERTEX, buffer: { type: \"uniform\" } }, { binding: 1, visibility: GPUShaderStage.VERTEX, buffer: { type: \"uniform\" } } ] }); const bindGroup = device.createBindGroup({ layout: bindGroupLayout, entries: [ { binding: 0, resource: { buffer: projectionBuffer, } }, { binding: 1, resource: { buffer: angleBufferGPU, } } ] }); Now, just multiply a rotation matrix before sending position to the projection matrix. const get_shader = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"sm\", code: ` @group(0) @binding(0) var<uniform> projection: mat3x2<f32>; @group(0) @binding(1) var<uniform> angle: f32; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, @location(1) depth: f32, }; @vertex fn vertexMain(@location(0) position: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(cos(angle), 0.0, sin(angle)), vec3<f32>(0.0, 1.0, 0.0), vec3<f32>(-sin(angle), 0.0, cos(angle)) ); let rotated = rotation * position; let projected = projection * rotated; let final_position = vec4<f32>(projected, 0.0, 1.0); let depth = rotated.z * 0.5 + 0.5; var output = VertexOutput(final_position, vertexIndex / 6, depth); return output; } struct FragmentOutput { @location(0) color: vec4<f32>, @builtin(frag_depth) depth: f32, }; @fragment fn fragmentMain(input: VertexOutput) -> FragmentOutput { var output = FragmentOutput(vec4<f32>(1.0, 1.0, 1.0, 1.0), input.depth); if (input.face == 0u) { output.color = vec4<f32>(1.0, 0.0, 0.0, 1.0); } else if (input.face == 1u) { output.color = vec4<f32>(0.0, 1.0, 0.0, 1.0); } else if (input.face == 2u) { output.color = vec4<f32>(0.0, 0.0, 1.0, 1.0); } else if (input.face == 3u) { output.color = vec4<f32>(1.0, 1.0, 0.0, 1.0); } else if (input.face == 4u) { output.color = vec4<f32>(1.0, 0.0, 1.0, 1.0); } else { output.color = vec4<f32>(0.0, 1.0, 1.0, 1.0); } return output; } `, }) } Now you can rotate the cube with the slider. When doing rotation, it is better to adopt isometric view of the orthographic projection, since it is more intuitive.","title":"Rotate the Cube"},{"location":"05/","text":"Triangles on Web Ch5 Perspective Projection In this chapter, we learn about perspective projection, which is the most common way to render 3D objects. In this chapter, we will also learn about the clip space of the WebGPU, the camera system and the perspective projection. Clip Space We previously learnt that the normalized device coordinates (NDC) is adopted in the WebGPU, but how exactly does NDC work? During rendering, first off, the points are four dimensional, and the fourth dimension is the homogeneous coordinate, which is used to perform perspective division, a topic we will discuss later. Simply, the renderer transforms four-dimensional points to three-dimensional points by dividing the first three dimensions by the fourth dimension. That is, (x, y, z, w) to (\\frac{x}{w}, \\frac{y}{w}, \\frac{z}{w}) . After perspective division, the points are in the clip space. Which means, the points exceeds the range of zero to one on x or y axis will be removed, and the points with negative z value or z value larger than one (after these values are divided by w) will be removed. Then, the vertices are converted into primitives, and sends each pixel to the fragment shader for rendering. If you enabled depth testing, the render will take the z-axis as the depth by default- but caution that points with z smaller than zero or larger than one will be removed, so you need to get around the depth a bit. That is, you can change our previous code into this one with same result- instead of manually calculating the depth, we just use native z-axis as the depth. @group(0) @binding(0) var<uniform> projection: mat3x2<f32>; @group(0) @binding(1) var<uniform> angle: f32; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, @location(1) depth: f32, }; @vertex fn vertexMain(@location(0) position: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(cos(angle), 0.0, sin(angle)), vec3<f32>(0.0, 1.0, 0.0), vec3<f32>(-sin(angle), 0.0, cos(angle)) ); let rotated = rotation * position; let projected = projection * rotated; let final_position = vec4<f32>(projected, rotated.z * 0.25 + 0.25, 1.0); let depth = rotated.z * 0.5 + 0.5; var output = VertexOutput(final_position, vertexIndex / 6, depth); return output; } struct FragmentOutput { @location(0) color: vec4<f32>, // @builtin(frag_depth) depth: f32, }; @fragment fn fragmentMain(input: VertexOutput) -> FragmentOutput { var output = FragmentOutput(vec4<f32>(1.0, 1.0, 1.0, 1.0)); if (input.face == 0u) { output.color = vec4<f32>(1.0, 0.0, 0.0, 1.0); } else if (input.face == 1u) { output.color = vec4<f32>(0.0, 1.0, 0.0, 1.0); } else if (input.face == 2u) { output.color = vec4<f32>(0.0, 0.0, 1.0, 1.0); } else if (input.face == 3u) { output.color = vec4<f32>(1.0, 1.0, 0.0, 1.0); } else if (input.face == 4u) { output.color = vec4<f32>(1.0, 0.0, 1.0, 1.0); } else { output.color = vec4<f32>(0.0, 1.0, 1.0, 1.0); } return output; } Camera System A camera is something that captures the scene, and the camera system is a system that simulates the camera in the 3D world. In the previous chapter, we used the rotation matrix to rotate the object. Of course, you can also translate the object by adding a translation matrix. Of course, such things are useful, but more often, we take a different approach- albeit the same result- by moving the camera instead of the object. A camera usually has its position, direction and field of view. The position is the position of the camera, the direction is the direction the camera is facing, and the field of view is the angle of the camera's view, that is, how wide the camera can see. The field of view is usually measured in how large the screen the camera can see at the distance of one unit. After introducing the camera, instead of using the raw points to render, we use the points relative to the camera. This transformation is called the view transformation, which is actually three steps, translation, rotation and scaling. Translation It's simple, the translated position is simply the original position minus the camera's position. r_{\\text{translated}} = r_{\\text{original}} - r_{\\text{camera}} Rotation The rotation is a bit more complicated. On the simple side, you just need to rotated the r_{\\text{translated}} by the inverse of the camera's rotation. By complicated, it is that you need to determine a rotation matrix that inverts the camera's rotation matrix. This depends on how you describe the rotation. A typical way would be the Euler angles- which implies that the rotation is done three times that are orthogonal between each two processes. The most common way is the ZYX order, it contains three rotations. Please note that as the object rotates, there is something called local axis and global axis. The local axis is the axis of the object, which follows the same rotation as the object, while the global axis is the axis of the world, which is fixed. At first, the local axis is the same as the global axis, but as the object rotates, the local axis changes. The following Yaw, the object rotates around the local z-axis. Pitch, the object rotates around the local y-axis. Roll, the object rotates around the local x-axis. If the angles are respectively \\alpha , \\beta and \\gamma , the rotation matrix is the product of the three rotation matrices. If we suppose the rotation matrix is R , that is, for the camera, d_{\\text{camera}} = R \\cdot (0, 0, 1) Then for the object, the rotation matrix is R^{-1} , that is, r_{\\text{rotated}} = R^{-1} \\cdot r_{\\text{translated}} Scaling The scaling is simple, remember the field of view? By default, our field of view is two. So if we want a view of width f_w and of height f_h , r_{\\text{scaled}} = (\\frac{r_{\\text{rotated}}}{2 * f_w}, \\frac{r_{\\text{rotated}}}{2 * f_h}, r_{\\text{rotated}}) Perspective Projection Now we know what a three-dimensional point should be transformed based on a camera, now we need to project the three-dimensional point to the two-dimensional scene the camera is facing. We already did that via orthographic projection, but it just sees not real- the reason was already addressed by artists in the Renaissance, that is, the perspective. Again, we do not explain what is perspective- we only talks about the math of it. Perspective Division We know that parallel lines should merge into one point in the distance, and the point is called the vanishing point. If the parallel lines rotates along the direct-up axis, the vanishing point will move along the x-axis, this is called the view horizon. This is from the view of lines. However, in computer graphics, we usually use the point-wise view of perspective. That is, there is a global vanishing point, where it radiates straight lines. There also exists a screen before the vanishing point, and then the camera looks at the screen. For a point in the 3D space, it's 2D projection on the screen is the intersection of the screen and a straight line that goes through the point and the vanishing point. Let's suppose the horizon is at the distance of f , relative to the camera, and stays at y_{\\text{horizon}} on the screen, which is also relative to the camera. Now, let's consider a point (x, y, z) , relative to the camera, we need to project it onto the screen, where z is zero. That is, we want to ask for (x_s, y_s, 0) , a straight line determined by (x, y, z) and (x_s, y_s, 0) goes right through the (0, y_{\\text{horizon}}, f) . So, we have \\frac{x_s}{x} = \\frac{f}{f - z} and, \\frac{y_s - y_\\text{horizon}}{y - y_\\text{horizon}} = \\frac{f}{f - z} More commonly we take y_{\\text{horizon}} as zero, \\frac{y_s}{y} = \\frac{f}{f - z} This is called the perspective division, and the f is the focal length of the camera. Please note that, this only considers the further-smaller role along the z-axis rotation. Only x-axis and y-axis has a perspective division, the z-axis does not. If you also want z-axis to have a perspective division, there are many more complex methods, which are too much for this chapter and we won't talk about it. Using Homogeneous Coordinates for Perspective Division Now, as you many have seen, we need division in the perspective division. However, division is bad because we prefer multiplication or addition. A way to get around is to use homogeneous coordinates. We already talked about how homogeneous coordinates work in the previous chapter, (x, y, z, w) is the same as (\\frac{x}{w}, \\frac{y}{w}, \\frac{z}{w}) . So, instead of doing division, we can use a matrix that manipulates the fourth dimension to do the division for us. That is, if we take the vector relative to the vanish point as r_{\\text{vanish}} = r_{\\text{rotated}} - (0, y_{\\text{horizon}}, f) , we can use the following matrix to do the perspective division, r_{\\text{projected vanish}} = \\begin{bmatrix} -f & 0 & 0 & 0 \\\\ 0 & -f & 0 & 0 \\\\ 0 & 0 & -f & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\cdot r_{\\text{vanish}} Note that we ignores z-axis value all together- it is a fixed value. Because it represents the depth of the point. Technically, the projected z-axis should stay -f based on the coordinate system with vanishing point as origin- but it doesn't matter. If we expand the matrix multiplication, we get, (-f \\cdot x, -f \\cdot y, -f (z-f), z - f) = \\begin{bmatrix} -f & 0 & 0 & 0 \\\\ 0 & -f & 0 & 0 \\\\ 0 & 0 & -f & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\cdot (x, y, z - f, 1) After converting the homogeneous coordinates back to the Cartesian coordinates, we get, (-\\frac{f \\cdot x}{z - f}, -\\frac{f \\cdot y}{z - f}, -f) Which is exactly the perspective division we want. Show the Cube in Perspective Previously, we have, @group(0) @binding(0) var<uniform> projection: mat3x2<f32>; @group(0) @binding(1) var<uniform> angle: f32; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, }; @vertex fn vertexMain(@location(0) position: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(cos(angle), 0.0, sin(angle)), vec3<f32>(0.0, 1.0, 0.0), vec3<f32>(-sin(angle), 0.0, cos(angle)) ); let rotated = rotation * position; let projected = projection * rotated; let depth = rotated.z * 0.25 + 0.25; let final_position = vec4<f32>(projected, depth, 1.0); var output = VertexOutput(final_position, vertexIndex / 6); return output; } struct FragmentOutput { @location(0) color: vec4<f32>, }; @fragment fn fragmentMain(input: VertexOutput) -> FragmentOutput { var output = FragmentOutput(vec4<f32>(1.0, 1.0, 1.0, 1.0)); if (input.face == 0u) { output.color = vec4<f32>(1.0, 0.0, 0.0, 1.0); } else if (input.face == 1u) { output.color = vec4<f32>(0.0, 1.0, 0.0, 1.0); } else if (input.face == 2u) { output.color = vec4<f32>(0.0, 0.0, 1.0, 1.0); } else if (input.face == 3u) { output.color = vec4<f32>(1.0, 1.0, 0.0, 1.0); } else if (input.face == 4u) { output.color = vec4<f32>(1.0, 0.0, 1.0, 1.0); } else { output.color = vec4<f32>(0.0, 1.0, 1.0, 1.0); } return output; } Now, we need to change how let projected = projection * rotated; works, rotated is a three dimensional vector, and we need to convert it to a four dimensional vector. We can do this by adding a one at the end of the vector. let rotated = vec4<f32>(rotation * position, 1.0); Then, we need to change the projection matrix. We change the projection matrix into a four by four matrix, @group(0) @binding(0) var<uniform> projection: mat4x4<f32>; Then do our previously said transformation. Please note the depth calculation, where we use world z-axis (you haven't forgot world axis, right?) to calculate the depth. @group(0) @binding(0) var<uniform> projection: mat4x4<f32>; @group(0) @binding(1) var<uniform> angle: f32; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, }; @vertex fn vertexMain(@location(0) position: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(cos(angle), 0.0, sin(angle)), vec3<f32>(0.0, 1.0, 0.0), vec3<f32>(-sin(angle), 0.0, cos(angle)) ); let rotated = vec4<f32>(rotation * position, 1.0); var projected = projection * (rotated - vec4<f32>(0.0, 0.0, 2.0, 0.0)); let final_position = vec4<f32>(projected.xy, 1.0 - rotated.z, projected.w); var output = VertexOutput(final_position, vertexIndex / 6); return output; } struct FragmentOutput { @location(0) color: vec4<f32>, }; @fragment fn fragmentMain(input: VertexOutput) -> FragmentOutput { var output = FragmentOutput(vec4<f32>(1.0, 1.0, 1.0, 1.0)); if (input.face == 0u) { output.color = vec4<f32>(1.0, 0.0, 0.0, 1.0); } else if (input.face == 1u) { output.color = vec4<f32>(0.0, 1.0, 0.0, 1.0); } else if (input.face == 2u) { output.color = vec4<f32>(0.0, 0.0, 1.0, 1.0); } else if (input.face == 3u) { output.color = vec4<f32>(1.0, 1.0, 0.0, 1.0); } else if (input.face == 4u) { output.color = vec4<f32>(1.0, 0.0, 1.0, 1.0); } else { output.color = vec4<f32>(0.0, 1.0, 1.0, 1.0); } return output; } Then change the projection matrix, const f = 2; const projectionMatrix = new Float32Array([ -f, 0, 0, 0, 0, -f, 0, 0, 0, 0, 0, -f, 0, 0, 1, 0, ]); const projectionBuffer = device.createBuffer({ size: 4 * 4 * 4, usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST, }); Now you should see a very realistic, nice cube in perspective. Adding with our previous chapter, you can rotate the cube. You can also move the cube with, let rotated = vec4<f32>(rotation * (position - vec3f(0.4, 0.4, 0.0)), 1.0);","title":"Perspective Projection"},{"location":"05/#triangles-on-web-ch5-perspective-projection","text":"In this chapter, we learn about perspective projection, which is the most common way to render 3D objects. In this chapter, we will also learn about the clip space of the WebGPU, the camera system and the perspective projection.","title":"Triangles on Web Ch5 Perspective Projection"},{"location":"05/#clip-space","text":"We previously learnt that the normalized device coordinates (NDC) is adopted in the WebGPU, but how exactly does NDC work? During rendering, first off, the points are four dimensional, and the fourth dimension is the homogeneous coordinate, which is used to perform perspective division, a topic we will discuss later. Simply, the renderer transforms four-dimensional points to three-dimensional points by dividing the first three dimensions by the fourth dimension. That is, (x, y, z, w) to (\\frac{x}{w}, \\frac{y}{w}, \\frac{z}{w}) . After perspective division, the points are in the clip space. Which means, the points exceeds the range of zero to one on x or y axis will be removed, and the points with negative z value or z value larger than one (after these values are divided by w) will be removed. Then, the vertices are converted into primitives, and sends each pixel to the fragment shader for rendering. If you enabled depth testing, the render will take the z-axis as the depth by default- but caution that points with z smaller than zero or larger than one will be removed, so you need to get around the depth a bit. That is, you can change our previous code into this one with same result- instead of manually calculating the depth, we just use native z-axis as the depth. @group(0) @binding(0) var<uniform> projection: mat3x2<f32>; @group(0) @binding(1) var<uniform> angle: f32; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, @location(1) depth: f32, }; @vertex fn vertexMain(@location(0) position: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(cos(angle), 0.0, sin(angle)), vec3<f32>(0.0, 1.0, 0.0), vec3<f32>(-sin(angle), 0.0, cos(angle)) ); let rotated = rotation * position; let projected = projection * rotated; let final_position = vec4<f32>(projected, rotated.z * 0.25 + 0.25, 1.0); let depth = rotated.z * 0.5 + 0.5; var output = VertexOutput(final_position, vertexIndex / 6, depth); return output; } struct FragmentOutput { @location(0) color: vec4<f32>, // @builtin(frag_depth) depth: f32, }; @fragment fn fragmentMain(input: VertexOutput) -> FragmentOutput { var output = FragmentOutput(vec4<f32>(1.0, 1.0, 1.0, 1.0)); if (input.face == 0u) { output.color = vec4<f32>(1.0, 0.0, 0.0, 1.0); } else if (input.face == 1u) { output.color = vec4<f32>(0.0, 1.0, 0.0, 1.0); } else if (input.face == 2u) { output.color = vec4<f32>(0.0, 0.0, 1.0, 1.0); } else if (input.face == 3u) { output.color = vec4<f32>(1.0, 1.0, 0.0, 1.0); } else if (input.face == 4u) { output.color = vec4<f32>(1.0, 0.0, 1.0, 1.0); } else { output.color = vec4<f32>(0.0, 1.0, 1.0, 1.0); } return output; }","title":"Clip Space"},{"location":"05/#camera-system","text":"A camera is something that captures the scene, and the camera system is a system that simulates the camera in the 3D world. In the previous chapter, we used the rotation matrix to rotate the object. Of course, you can also translate the object by adding a translation matrix. Of course, such things are useful, but more often, we take a different approach- albeit the same result- by moving the camera instead of the object. A camera usually has its position, direction and field of view. The position is the position of the camera, the direction is the direction the camera is facing, and the field of view is the angle of the camera's view, that is, how wide the camera can see. The field of view is usually measured in how large the screen the camera can see at the distance of one unit. After introducing the camera, instead of using the raw points to render, we use the points relative to the camera. This transformation is called the view transformation, which is actually three steps, translation, rotation and scaling.","title":"Camera System"},{"location":"05/#translation","text":"It's simple, the translated position is simply the original position minus the camera's position. r_{\\text{translated}} = r_{\\text{original}} - r_{\\text{camera}}","title":"Translation"},{"location":"05/#rotation","text":"The rotation is a bit more complicated. On the simple side, you just need to rotated the r_{\\text{translated}} by the inverse of the camera's rotation. By complicated, it is that you need to determine a rotation matrix that inverts the camera's rotation matrix. This depends on how you describe the rotation. A typical way would be the Euler angles- which implies that the rotation is done three times that are orthogonal between each two processes. The most common way is the ZYX order, it contains three rotations. Please note that as the object rotates, there is something called local axis and global axis. The local axis is the axis of the object, which follows the same rotation as the object, while the global axis is the axis of the world, which is fixed. At first, the local axis is the same as the global axis, but as the object rotates, the local axis changes. The following Yaw, the object rotates around the local z-axis. Pitch, the object rotates around the local y-axis. Roll, the object rotates around the local x-axis. If the angles are respectively \\alpha , \\beta and \\gamma , the rotation matrix is the product of the three rotation matrices. If we suppose the rotation matrix is R , that is, for the camera, d_{\\text{camera}} = R \\cdot (0, 0, 1) Then for the object, the rotation matrix is R^{-1} , that is, r_{\\text{rotated}} = R^{-1} \\cdot r_{\\text{translated}}","title":"Rotation"},{"location":"05/#scaling","text":"The scaling is simple, remember the field of view? By default, our field of view is two. So if we want a view of width f_w and of height f_h , r_{\\text{scaled}} = (\\frac{r_{\\text{rotated}}}{2 * f_w}, \\frac{r_{\\text{rotated}}}{2 * f_h}, r_{\\text{rotated}})","title":"Scaling"},{"location":"05/#perspective-projection","text":"Now we know what a three-dimensional point should be transformed based on a camera, now we need to project the three-dimensional point to the two-dimensional scene the camera is facing. We already did that via orthographic projection, but it just sees not real- the reason was already addressed by artists in the Renaissance, that is, the perspective. Again, we do not explain what is perspective- we only talks about the math of it.","title":"Perspective Projection"},{"location":"05/#perspective-division","text":"We know that parallel lines should merge into one point in the distance, and the point is called the vanishing point. If the parallel lines rotates along the direct-up axis, the vanishing point will move along the x-axis, this is called the view horizon. This is from the view of lines. However, in computer graphics, we usually use the point-wise view of perspective. That is, there is a global vanishing point, where it radiates straight lines. There also exists a screen before the vanishing point, and then the camera looks at the screen. For a point in the 3D space, it's 2D projection on the screen is the intersection of the screen and a straight line that goes through the point and the vanishing point. Let's suppose the horizon is at the distance of f , relative to the camera, and stays at y_{\\text{horizon}} on the screen, which is also relative to the camera. Now, let's consider a point (x, y, z) , relative to the camera, we need to project it onto the screen, where z is zero. That is, we want to ask for (x_s, y_s, 0) , a straight line determined by (x, y, z) and (x_s, y_s, 0) goes right through the (0, y_{\\text{horizon}}, f) . So, we have \\frac{x_s}{x} = \\frac{f}{f - z} and, \\frac{y_s - y_\\text{horizon}}{y - y_\\text{horizon}} = \\frac{f}{f - z} More commonly we take y_{\\text{horizon}} as zero, \\frac{y_s}{y} = \\frac{f}{f - z} This is called the perspective division, and the f is the focal length of the camera. Please note that, this only considers the further-smaller role along the z-axis rotation. Only x-axis and y-axis has a perspective division, the z-axis does not. If you also want z-axis to have a perspective division, there are many more complex methods, which are too much for this chapter and we won't talk about it.","title":"Perspective Division"},{"location":"05/#using-homogeneous-coordinates-for-perspective-division","text":"Now, as you many have seen, we need division in the perspective division. However, division is bad because we prefer multiplication or addition. A way to get around is to use homogeneous coordinates. We already talked about how homogeneous coordinates work in the previous chapter, (x, y, z, w) is the same as (\\frac{x}{w}, \\frac{y}{w}, \\frac{z}{w}) . So, instead of doing division, we can use a matrix that manipulates the fourth dimension to do the division for us. That is, if we take the vector relative to the vanish point as r_{\\text{vanish}} = r_{\\text{rotated}} - (0, y_{\\text{horizon}}, f) , we can use the following matrix to do the perspective division, r_{\\text{projected vanish}} = \\begin{bmatrix} -f & 0 & 0 & 0 \\\\ 0 & -f & 0 & 0 \\\\ 0 & 0 & -f & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\cdot r_{\\text{vanish}} Note that we ignores z-axis value all together- it is a fixed value. Because it represents the depth of the point. Technically, the projected z-axis should stay -f based on the coordinate system with vanishing point as origin- but it doesn't matter. If we expand the matrix multiplication, we get, (-f \\cdot x, -f \\cdot y, -f (z-f), z - f) = \\begin{bmatrix} -f & 0 & 0 & 0 \\\\ 0 & -f & 0 & 0 \\\\ 0 & 0 & -f & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} \\cdot (x, y, z - f, 1) After converting the homogeneous coordinates back to the Cartesian coordinates, we get, (-\\frac{f \\cdot x}{z - f}, -\\frac{f \\cdot y}{z - f}, -f) Which is exactly the perspective division we want.","title":"Using Homogeneous Coordinates for Perspective Division"},{"location":"05/#show-the-cube-in-perspective","text":"Previously, we have, @group(0) @binding(0) var<uniform> projection: mat3x2<f32>; @group(0) @binding(1) var<uniform> angle: f32; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, }; @vertex fn vertexMain(@location(0) position: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(cos(angle), 0.0, sin(angle)), vec3<f32>(0.0, 1.0, 0.0), vec3<f32>(-sin(angle), 0.0, cos(angle)) ); let rotated = rotation * position; let projected = projection * rotated; let depth = rotated.z * 0.25 + 0.25; let final_position = vec4<f32>(projected, depth, 1.0); var output = VertexOutput(final_position, vertexIndex / 6); return output; } struct FragmentOutput { @location(0) color: vec4<f32>, }; @fragment fn fragmentMain(input: VertexOutput) -> FragmentOutput { var output = FragmentOutput(vec4<f32>(1.0, 1.0, 1.0, 1.0)); if (input.face == 0u) { output.color = vec4<f32>(1.0, 0.0, 0.0, 1.0); } else if (input.face == 1u) { output.color = vec4<f32>(0.0, 1.0, 0.0, 1.0); } else if (input.face == 2u) { output.color = vec4<f32>(0.0, 0.0, 1.0, 1.0); } else if (input.face == 3u) { output.color = vec4<f32>(1.0, 1.0, 0.0, 1.0); } else if (input.face == 4u) { output.color = vec4<f32>(1.0, 0.0, 1.0, 1.0); } else { output.color = vec4<f32>(0.0, 1.0, 1.0, 1.0); } return output; } Now, we need to change how let projected = projection * rotated; works, rotated is a three dimensional vector, and we need to convert it to a four dimensional vector. We can do this by adding a one at the end of the vector. let rotated = vec4<f32>(rotation * position, 1.0); Then, we need to change the projection matrix. We change the projection matrix into a four by four matrix, @group(0) @binding(0) var<uniform> projection: mat4x4<f32>; Then do our previously said transformation. Please note the depth calculation, where we use world z-axis (you haven't forgot world axis, right?) to calculate the depth. @group(0) @binding(0) var<uniform> projection: mat4x4<f32>; @group(0) @binding(1) var<uniform> angle: f32; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, }; @vertex fn vertexMain(@location(0) position: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(cos(angle), 0.0, sin(angle)), vec3<f32>(0.0, 1.0, 0.0), vec3<f32>(-sin(angle), 0.0, cos(angle)) ); let rotated = vec4<f32>(rotation * position, 1.0); var projected = projection * (rotated - vec4<f32>(0.0, 0.0, 2.0, 0.0)); let final_position = vec4<f32>(projected.xy, 1.0 - rotated.z, projected.w); var output = VertexOutput(final_position, vertexIndex / 6); return output; } struct FragmentOutput { @location(0) color: vec4<f32>, }; @fragment fn fragmentMain(input: VertexOutput) -> FragmentOutput { var output = FragmentOutput(vec4<f32>(1.0, 1.0, 1.0, 1.0)); if (input.face == 0u) { output.color = vec4<f32>(1.0, 0.0, 0.0, 1.0); } else if (input.face == 1u) { output.color = vec4<f32>(0.0, 1.0, 0.0, 1.0); } else if (input.face == 2u) { output.color = vec4<f32>(0.0, 0.0, 1.0, 1.0); } else if (input.face == 3u) { output.color = vec4<f32>(1.0, 1.0, 0.0, 1.0); } else if (input.face == 4u) { output.color = vec4<f32>(1.0, 0.0, 1.0, 1.0); } else { output.color = vec4<f32>(0.0, 1.0, 1.0, 1.0); } return output; } Then change the projection matrix, const f = 2; const projectionMatrix = new Float32Array([ -f, 0, 0, 0, 0, -f, 0, 0, 0, 0, 0, -f, 0, 0, 1, 0, ]); const projectionBuffer = device.createBuffer({ size: 4 * 4 * 4, usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST, }); Now you should see a very realistic, nice cube in perspective. Adding with our previous chapter, you can rotate the cube. You can also move the cube with, let rotated = vec4<f32>(rotation * (position - vec3f(0.4, 0.4, 0.0)), 1.0);","title":"Show the Cube in Perspective"},{"location":"06/","text":"Triangles on Web Ch6 Lighting Previously, we got the shape of our model right- with perspective projection. Now, we make the color right with lighting. There are many lighting models- each tells how the fragment color is computed. The most common is Phong lighting model. It has 3 components: ambient, diffuse, and specular, which we will implement in this chapter. Ambient Ambient simply refers to the, well, ambient. It is the light that presents everywhere. Ambient light will cause the object to have a constant light shad on it. And thus it renders the object with a constant color. If we take a simple model, that the ambient light is white and the color is commensurate to the ambient light, then the color with ambient light taken into account is: color = I \\cdot albedo I = k_{\\text{ambient}} \\cdot I_{\\text{ambient}} Where I_{\\text{ambient}} is the intensity of the ambient light. In computer graphics, the intensity of light usually just use a unit of one, without any unit. I is the intensity of the light considering all factors, only ambient light in this case. albedo is just a fancy word for the color of the object if it is not illuminated by any light. k_{\\text{ambient}} is a factor that denotes the reflectivity of the object. You can take arbitrary values, but usually, it is taken as 1.0. Normally, we can take I_{\\text{ambient}} = 0.3 . Diffuse However, there is not only ambient light that is scattered everywhere- usually, there are also directed light sources. The light will shad on the object, reflected off its surface and reach the camera (more detailed lighting will just be ignored, we only consider reflection and the first reflection). In physics, we know that if a light reflects off a surface, there are two cases- if the surface is smooth, it is a specular reflection, where the reflection is sharp and clear, with reflected light following the law of reflection. If the surface is rough, because each surface can be viewed as smaller smooth surfaces of many different angles, the light will be reflected in all directions, and the reflection is called diffuse reflection. In Phong lighting model, both reflections are considered, albeit this is not the case in real life. Diffuse reflection follows the Lambert cosine law. The law states that the intensity of the light reflected off a surface is proportional to the cosine of the angle between the light source and the surface normal. That is, if the unit normal vector is \\vec{n} , a light with \\vec{l} and intensity I_{\\text{diffuse}} , the intensity of the light reflected off the surface is: I_{\\text{diffuse}} = k_{\\text{diffuse}} \\cdot I_{\\text{diffuse}} \\cdot \\max(0, \\vec{n} \\cdot \\vec{l}) Specular We also consider the specular reflection. The specular reflection denotes the light that is directly reflected to the camera. If we have a light that is reflected off a surface with the intensity I_{\\text{specular}} , the intensity of the light perceived by the camera is determined by the phong reflection model, with, I_{\\text{specular}} = k_{\\text{specular}} \\cdot I_{\\text{specular}} \\cdot \\max(0, \\vec{r} \\cdot \\vec{v})^{\\alpha} Where \\vec{r} is the reflection of the light, \\vec{v} is the vector from the fragment to the camera, and \\alpha is the shininess of the object, or say, how rough the highlight is. The higher the value, the more concentrated the highlight is, and vice versa. Putting it all together Now, we have all the components of the Phong lighting model. We can put it all together to get the final color of the fragment. First off, for a point at \\vec{p} (the world axis), the normal vector of the primitive at this point being \\vec{n} , the camera position being \\vec{v} , and the light position being \\vec{l} . Now, let's first consider how we can get all the vectors. \\vec{p} is directly given by the model. \\vec{n} can be calculated by the cross product of the two edges of the triangle, and then normalized. So in vertex shader, we pass both the normal vector and the position of the vertex. \\vec{v} is simply the camera position minus the position of the fragment. \\vec{l} is the light position minus the position of the fragment. This is determined by the light source we want. \\vec{r} is the reflection of the light, which can be calculated by the formula: \\vec{r} = 2 \\cdot \\vec{n} \\cdot (\\vec{n} \\cdot \\vec{l}) - \\vec{l} Now, we can calculate the intensity of the light by the formula: I_{\\text{light}} = I_{\\text{ambient}} + I_{\\text{diffuse}} + I_{\\text{specular}} And the final color of the fragment is: color = I_{\\text{light}} \\cdot albedo For each component of the light, I_{\\text{ambient}} = k_{\\text{ambient}} \\cdot I_{\\text{ambient}} I_{\\text{diffuse}} = k_{\\text{diffuse}} \\cdot I_{\\text{diffuse}} \\cdot \\max(0, \\vec{n} \\cdot \\vec{l}) I_{\\text{specular}} = k_{\\text{specular}} \\cdot I_{\\text{specular}} \\cdot \\max(0, \\vec{r} \\cdot \\vec{v})^{\\alpha} Implementation Now let's add a point light source to our cube. First, we need to calculate the normal vector, then pass it to the vertex shader. const get_shader = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"sm\", code: ` @group(0) @binding(0) var<uniform> projection: mat4x4<f32>; @group(0) @binding(1) var<uniform> angle: f32; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, }; @vertex fn vertexMain(@location(0) position: vec3f, @location(1) norm: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(1.0, 0.0, 0.0), vec3<f32>(0.0, cos(angle), sin(angle)), vec3<f32>(0.0, -sin(angle), cos(angle)), ); let rotated = vec4<f32>(rotation * (position - vec3f(0.4, 0.4, 0.0)), 1.0); var projected = projection * (rotated - vec4<f32>(0.0, 0.0, ${f}, 0.0)); let final_position = vec4<f32>(projected.xy, 1.0 - rotated.z, projected.w); var output = VertexOutput(final_position, vertexIndex / 6); return output; } struct FragmentOutput { @location(0) color: vec4<f32>, }; @fragment fn fragmentMain(input: VertexOutput) -> FragmentOutput { var output = FragmentOutput(vec4<f32>(1.0, 1.0, 1.0, 1.0)); if (input.face == 0u) { output.color = vec4<f32>(1.0, 0.0, 0.0, 1.0); } else if (input.face == 1u) { output.color = vec4<f32>(0.0, 1.0, 0.0, 1.0); } else if (input.face == 2u) { output.color = vec4<f32>(0.0, 0.0, 1.0, 1.0); } else if (input.face == 3u) { output.color = vec4<f32>(1.0, 1.0, 0.0, 1.0); } else if (input.face == 4u) { output.color = vec4<f32>(1.0, 0.0, 1.0, 1.0); } else { output.color = vec4<f32>(0.0, 1.0, 1.0, 1.0); } return output; } `, }) } const get_vertices = (device: GPUDevice): [GPUBuffer, GPUVertexBufferLayout] => { // cube // use triangle const vertices = new Float32Array([ // xy 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, // yz 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, // zx 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, ]).map((v) => v * 0.5 - 0.25); const primitiveCount = vertices.length / 9; const verticesWithNormals = new Float32Array(primitiveCount * 6 * 3); const cross = (a: number[], b: number[]): number[] => { return [ a[1] * b[2] - a[2] * b[1], a[2] * b[0] - a[0] * b[2], a[0] * b[1] - a[1] * b[0], ] } for (let i = 0; i < primitiveCount; i++) { const a = [vertices[i * 9 + 0], vertices[i * 9 + 1], vertices[i * 9 + 2]]; const b = [vertices[i * 9 + 3], vertices[i * 9 + 4], vertices[i * 9 + 5]]; const c = [vertices[i * 9 + 6], vertices[i * 9 + 7], vertices[i * 9 + 8]]; const ab = [b[0] - a[0], b[1] - a[1], b[2] - a[2]]; const ac = [c[0] - a[0], c[1] - a[1], c[2] - a[2]]; const normal = cross(ab, ac); const length = Math.sqrt(normal[0] * normal[0] + normal[1] * normal[1] + normal[2] * normal[2]); normal[0] /= length; normal[1] /= length; normal[2] /= length; for (let j = 0; j < 3; j++) { verticesWithNormals[i * 18 + j * 6 + 0] = vertices[i * 9 + j * 3 + 0]; verticesWithNormals[i * 18 + j * 6 + 1] = vertices[i * 9 + j * 3 + 1]; verticesWithNormals[i * 18 + j * 6 + 2] = vertices[i * 9 + j * 3 + 2]; verticesWithNormals[i * 18 + j * 6 + 3] = normal[0]; verticesWithNormals[i * 18 + j * 6 + 4] = normal[1]; verticesWithNormals[i * 18 + j * 6 + 5] = normal[2]; } } const layout: GPUVertexBufferLayout = { arrayStride: 3 * 4 * 2, attributes: [{ format: \"float32x3\", offset: 0, shaderLocation: 0, }, { format: \"float32x3\", offset: 3 * 4, shaderLocation: 1 }] } const buffer = device.createBuffer({ size: verticesWithNormals.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }) device.queue.writeBuffer(buffer, 0, verticesWithNormals.buffer); return [buffer, layout]; } Then we simply put norm vector into the output of the vertex shader, asking a linear interpolation of the norm vector. In addition, we also need the real position of the vertex for calculating lighting. struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, @location(1) @interpolate(linear) real_position: vec3f, @location(2) @interpolate(linear) normal: vec3f, }; @vertex fn vertexMain(@location(0) position: vec3f, @location(1) norm: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(1.0, 0.0, 0.0), vec3<f32>(0.0, cos(angle), sin(angle)), vec3<f32>(0.0, -sin(angle), cos(angle)), ); let rotated = vec4<f32>(rotation * (position - vec3f(0.4, 0.4, 0.0)), 1.0); var projected = projection * (rotated - vec4<f32>(0.0, 0.0, ${f}, 0.0)); let final_position = vec4<f32>(projected.xy, 1.0 - rotated.z, projected.w); var output = VertexOutput(final_position, vertexIndex / 6, rotation * position, rotation * norm); return output; } Now, in the fragment shader, we have our normal vector and positional vector. We can calculate the light intensity by the formula above. First, the albedo, @fragment fn fragmentMain(input: VertexOutput) -> @location(0) vec4<f32> { var output = vec4<f32>(0.3, 0.3, 0.3, 1.0); return output; } Then calculate all the vectors. Here we use a point light source. @group(0) @binding(0) var<uniform> projection: mat4x4<f32>; @group(0) @binding(1) var<uniform> angle: f32; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, @location(1) @interpolate(linear) real_position: vec3f, @location(2) @interpolate(linear) normal: vec3f, }; @vertex fn vertexMain(@location(0) position: vec3f, @location(1) norm: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(1.0, 0.0, 0.0), vec3<f32>(0.0, cos(angle), sin(angle)), vec3<f32>(0.0, -sin(angle), cos(angle)), ); let rotated = vec4<f32>(rotation * (position - vec3f(0.3, 0.3, 0.0)), 1.0); var projected = projection * (rotated - vec4<f32>(0.0, 0.0, ${f}, 0.0)); let final_position = vec4<f32>(projected.xy, 1.0 - rotated.z, projected.w); var output = VertexOutput(final_position, vertexIndex / 6, rotated.xyz, rotation * norm); return output; } @fragment fn fragmentMain(input: VertexOutput) -> @location(0) vec4<f32> { let light_source = vec3<f32>(-1.0, 0.0, -5.0); let l = normalize(light_source - input.real_position); let n = normalize(input.normal); let r = reflect(-l, n); let camera = vec3<f32>(0.0, 0.0, -1.0); let v = normalize(camera - input.real_position); let spec = pow(max(dot(r, v), 0.0), 32.0) * 0.8; let diff = max(dot(-n, l), 0.0) * 0.4; let amb = 0.2; let albedo = vec3<f32>(1.0, 1.0, 1.0); let color = albedo * min(amb + diff + spec, 1.0); return vec4<f32>(color, 1.0); } Please note that, we have not yet set the position of the camera previously- since we did not do the scaling based on the field of view, so you can basically choose any position of the camera. Here we choose (0, 0, -1). Also, some negative-positive signs were changed based since now l is the vector points to the source from the fragment. Now, you should see a cube with lighting. However- you may encounter an issue, that some primitives are properly lit, while some are not. This is because the normal vector has two directions. When creating hte vertices, you should make sure that the normal vector is pointing outwards. Previously shown vertices and cross product are correct.","title":"Lighting"},{"location":"06/#triangles-on-web-ch6-lighting","text":"Previously, we got the shape of our model right- with perspective projection. Now, we make the color right with lighting. There are many lighting models- each tells how the fragment color is computed. The most common is Phong lighting model. It has 3 components: ambient, diffuse, and specular, which we will implement in this chapter.","title":"Triangles on Web Ch6 Lighting"},{"location":"06/#ambient","text":"Ambient simply refers to the, well, ambient. It is the light that presents everywhere. Ambient light will cause the object to have a constant light shad on it. And thus it renders the object with a constant color. If we take a simple model, that the ambient light is white and the color is commensurate to the ambient light, then the color with ambient light taken into account is: color = I \\cdot albedo I = k_{\\text{ambient}} \\cdot I_{\\text{ambient}} Where I_{\\text{ambient}} is the intensity of the ambient light. In computer graphics, the intensity of light usually just use a unit of one, without any unit. I is the intensity of the light considering all factors, only ambient light in this case. albedo is just a fancy word for the color of the object if it is not illuminated by any light. k_{\\text{ambient}} is a factor that denotes the reflectivity of the object. You can take arbitrary values, but usually, it is taken as 1.0. Normally, we can take I_{\\text{ambient}} = 0.3 .","title":"Ambient"},{"location":"06/#diffuse","text":"However, there is not only ambient light that is scattered everywhere- usually, there are also directed light sources. The light will shad on the object, reflected off its surface and reach the camera (more detailed lighting will just be ignored, we only consider reflection and the first reflection). In physics, we know that if a light reflects off a surface, there are two cases- if the surface is smooth, it is a specular reflection, where the reflection is sharp and clear, with reflected light following the law of reflection. If the surface is rough, because each surface can be viewed as smaller smooth surfaces of many different angles, the light will be reflected in all directions, and the reflection is called diffuse reflection. In Phong lighting model, both reflections are considered, albeit this is not the case in real life. Diffuse reflection follows the Lambert cosine law. The law states that the intensity of the light reflected off a surface is proportional to the cosine of the angle between the light source and the surface normal. That is, if the unit normal vector is \\vec{n} , a light with \\vec{l} and intensity I_{\\text{diffuse}} , the intensity of the light reflected off the surface is: I_{\\text{diffuse}} = k_{\\text{diffuse}} \\cdot I_{\\text{diffuse}} \\cdot \\max(0, \\vec{n} \\cdot \\vec{l})","title":"Diffuse"},{"location":"06/#specular","text":"We also consider the specular reflection. The specular reflection denotes the light that is directly reflected to the camera. If we have a light that is reflected off a surface with the intensity I_{\\text{specular}} , the intensity of the light perceived by the camera is determined by the phong reflection model, with, I_{\\text{specular}} = k_{\\text{specular}} \\cdot I_{\\text{specular}} \\cdot \\max(0, \\vec{r} \\cdot \\vec{v})^{\\alpha} Where \\vec{r} is the reflection of the light, \\vec{v} is the vector from the fragment to the camera, and \\alpha is the shininess of the object, or say, how rough the highlight is. The higher the value, the more concentrated the highlight is, and vice versa.","title":"Specular"},{"location":"06/#putting-it-all-together","text":"Now, we have all the components of the Phong lighting model. We can put it all together to get the final color of the fragment. First off, for a point at \\vec{p} (the world axis), the normal vector of the primitive at this point being \\vec{n} , the camera position being \\vec{v} , and the light position being \\vec{l} . Now, let's first consider how we can get all the vectors. \\vec{p} is directly given by the model. \\vec{n} can be calculated by the cross product of the two edges of the triangle, and then normalized. So in vertex shader, we pass both the normal vector and the position of the vertex. \\vec{v} is simply the camera position minus the position of the fragment. \\vec{l} is the light position minus the position of the fragment. This is determined by the light source we want. \\vec{r} is the reflection of the light, which can be calculated by the formula: \\vec{r} = 2 \\cdot \\vec{n} \\cdot (\\vec{n} \\cdot \\vec{l}) - \\vec{l} Now, we can calculate the intensity of the light by the formula: I_{\\text{light}} = I_{\\text{ambient}} + I_{\\text{diffuse}} + I_{\\text{specular}} And the final color of the fragment is: color = I_{\\text{light}} \\cdot albedo For each component of the light, I_{\\text{ambient}} = k_{\\text{ambient}} \\cdot I_{\\text{ambient}} I_{\\text{diffuse}} = k_{\\text{diffuse}} \\cdot I_{\\text{diffuse}} \\cdot \\max(0, \\vec{n} \\cdot \\vec{l}) I_{\\text{specular}} = k_{\\text{specular}} \\cdot I_{\\text{specular}} \\cdot \\max(0, \\vec{r} \\cdot \\vec{v})^{\\alpha}","title":"Putting it all together"},{"location":"06/#implementation","text":"Now let's add a point light source to our cube. First, we need to calculate the normal vector, then pass it to the vertex shader. const get_shader = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"sm\", code: ` @group(0) @binding(0) var<uniform> projection: mat4x4<f32>; @group(0) @binding(1) var<uniform> angle: f32; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, }; @vertex fn vertexMain(@location(0) position: vec3f, @location(1) norm: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(1.0, 0.0, 0.0), vec3<f32>(0.0, cos(angle), sin(angle)), vec3<f32>(0.0, -sin(angle), cos(angle)), ); let rotated = vec4<f32>(rotation * (position - vec3f(0.4, 0.4, 0.0)), 1.0); var projected = projection * (rotated - vec4<f32>(0.0, 0.0, ${f}, 0.0)); let final_position = vec4<f32>(projected.xy, 1.0 - rotated.z, projected.w); var output = VertexOutput(final_position, vertexIndex / 6); return output; } struct FragmentOutput { @location(0) color: vec4<f32>, }; @fragment fn fragmentMain(input: VertexOutput) -> FragmentOutput { var output = FragmentOutput(vec4<f32>(1.0, 1.0, 1.0, 1.0)); if (input.face == 0u) { output.color = vec4<f32>(1.0, 0.0, 0.0, 1.0); } else if (input.face == 1u) { output.color = vec4<f32>(0.0, 1.0, 0.0, 1.0); } else if (input.face == 2u) { output.color = vec4<f32>(0.0, 0.0, 1.0, 1.0); } else if (input.face == 3u) { output.color = vec4<f32>(1.0, 1.0, 0.0, 1.0); } else if (input.face == 4u) { output.color = vec4<f32>(1.0, 0.0, 1.0, 1.0); } else { output.color = vec4<f32>(0.0, 1.0, 1.0, 1.0); } return output; } `, }) } const get_vertices = (device: GPUDevice): [GPUBuffer, GPUVertexBufferLayout] => { // cube // use triangle const vertices = new Float32Array([ // xy 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, // yz 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, // zx 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, ]).map((v) => v * 0.5 - 0.25); const primitiveCount = vertices.length / 9; const verticesWithNormals = new Float32Array(primitiveCount * 6 * 3); const cross = (a: number[], b: number[]): number[] => { return [ a[1] * b[2] - a[2] * b[1], a[2] * b[0] - a[0] * b[2], a[0] * b[1] - a[1] * b[0], ] } for (let i = 0; i < primitiveCount; i++) { const a = [vertices[i * 9 + 0], vertices[i * 9 + 1], vertices[i * 9 + 2]]; const b = [vertices[i * 9 + 3], vertices[i * 9 + 4], vertices[i * 9 + 5]]; const c = [vertices[i * 9 + 6], vertices[i * 9 + 7], vertices[i * 9 + 8]]; const ab = [b[0] - a[0], b[1] - a[1], b[2] - a[2]]; const ac = [c[0] - a[0], c[1] - a[1], c[2] - a[2]]; const normal = cross(ab, ac); const length = Math.sqrt(normal[0] * normal[0] + normal[1] * normal[1] + normal[2] * normal[2]); normal[0] /= length; normal[1] /= length; normal[2] /= length; for (let j = 0; j < 3; j++) { verticesWithNormals[i * 18 + j * 6 + 0] = vertices[i * 9 + j * 3 + 0]; verticesWithNormals[i * 18 + j * 6 + 1] = vertices[i * 9 + j * 3 + 1]; verticesWithNormals[i * 18 + j * 6 + 2] = vertices[i * 9 + j * 3 + 2]; verticesWithNormals[i * 18 + j * 6 + 3] = normal[0]; verticesWithNormals[i * 18 + j * 6 + 4] = normal[1]; verticesWithNormals[i * 18 + j * 6 + 5] = normal[2]; } } const layout: GPUVertexBufferLayout = { arrayStride: 3 * 4 * 2, attributes: [{ format: \"float32x3\", offset: 0, shaderLocation: 0, }, { format: \"float32x3\", offset: 3 * 4, shaderLocation: 1 }] } const buffer = device.createBuffer({ size: verticesWithNormals.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }) device.queue.writeBuffer(buffer, 0, verticesWithNormals.buffer); return [buffer, layout]; } Then we simply put norm vector into the output of the vertex shader, asking a linear interpolation of the norm vector. In addition, we also need the real position of the vertex for calculating lighting. struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, @location(1) @interpolate(linear) real_position: vec3f, @location(2) @interpolate(linear) normal: vec3f, }; @vertex fn vertexMain(@location(0) position: vec3f, @location(1) norm: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(1.0, 0.0, 0.0), vec3<f32>(0.0, cos(angle), sin(angle)), vec3<f32>(0.0, -sin(angle), cos(angle)), ); let rotated = vec4<f32>(rotation * (position - vec3f(0.4, 0.4, 0.0)), 1.0); var projected = projection * (rotated - vec4<f32>(0.0, 0.0, ${f}, 0.0)); let final_position = vec4<f32>(projected.xy, 1.0 - rotated.z, projected.w); var output = VertexOutput(final_position, vertexIndex / 6, rotation * position, rotation * norm); return output; } Now, in the fragment shader, we have our normal vector and positional vector. We can calculate the light intensity by the formula above. First, the albedo, @fragment fn fragmentMain(input: VertexOutput) -> @location(0) vec4<f32> { var output = vec4<f32>(0.3, 0.3, 0.3, 1.0); return output; } Then calculate all the vectors. Here we use a point light source. @group(0) @binding(0) var<uniform> projection: mat4x4<f32>; @group(0) @binding(1) var<uniform> angle: f32; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, @location(1) @interpolate(linear) real_position: vec3f, @location(2) @interpolate(linear) normal: vec3f, }; @vertex fn vertexMain(@location(0) position: vec3f, @location(1) norm: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(1.0, 0.0, 0.0), vec3<f32>(0.0, cos(angle), sin(angle)), vec3<f32>(0.0, -sin(angle), cos(angle)), ); let rotated = vec4<f32>(rotation * (position - vec3f(0.3, 0.3, 0.0)), 1.0); var projected = projection * (rotated - vec4<f32>(0.0, 0.0, ${f}, 0.0)); let final_position = vec4<f32>(projected.xy, 1.0 - rotated.z, projected.w); var output = VertexOutput(final_position, vertexIndex / 6, rotated.xyz, rotation * norm); return output; } @fragment fn fragmentMain(input: VertexOutput) -> @location(0) vec4<f32> { let light_source = vec3<f32>(-1.0, 0.0, -5.0); let l = normalize(light_source - input.real_position); let n = normalize(input.normal); let r = reflect(-l, n); let camera = vec3<f32>(0.0, 0.0, -1.0); let v = normalize(camera - input.real_position); let spec = pow(max(dot(r, v), 0.0), 32.0) * 0.8; let diff = max(dot(-n, l), 0.0) * 0.4; let amb = 0.2; let albedo = vec3<f32>(1.0, 1.0, 1.0); let color = albedo * min(amb + diff + spec, 1.0); return vec4<f32>(color, 1.0); } Please note that, we have not yet set the position of the camera previously- since we did not do the scaling based on the field of view, so you can basically choose any position of the camera. Here we choose (0, 0, -1). Also, some negative-positive signs were changed based since now l is the vector points to the source from the fragment. Now, you should see a cube with lighting. However- you may encounter an issue, that some primitives are properly lit, while some are not. This is because the normal vector has two directions. When creating hte vertices, you should make sure that the normal vector is pointing outwards. Previously shown vertices and cross product are correct.","title":"Implementation"},{"location":"07/","text":"Triangles on Web Ch7 WebGPU Shading Language We have used WebGPU shading language (wgsl) frequently, but we haven't actually talked about it in detail. In this chapter, we will talk about the syntax and semantics of wgsl. We mainly talks about its difference from universal programming languages like C and JavaScript. Annotations The very first significant difference is annotations starting with @ . There are mainly two kinds of annotations, the function annotations and the parameters annotations. Function Annotations We have used only four of them so far, @vertex , @fragment , @compute and @workgroup_size . The first three are used to specify the type of the function, and the last one is used to specify the workgroup size of the compute shader. We have already introduced their usage in the previous chapters. Parameters Annotations Different from universal programming languages, shading language needs you to explicitly put the memory layout of the parameters and the return value of a function. We have already used @builtin and @location , now let's dive deeper into them. Let's consider the following shader function: struct VertexInput { @builtin(vertex_index) index: u32, @location(0) position: vec3f, @location(1) norm: vec3f, }; struct VertexOutput { @builtin(position) position: vec4f, @location(0) norm: vec3f, }; @vertex fn vertex_main(input: VertexInput) -> VertexOutput { // ... } struct in the shading language is more like that in C language- it specifies a memory layout. First let's look at the input- the input struct of a function in shading language consists of two parts, the locational fields and the builtin fields. The locational fields are specified by @location , which specifies the location of the field in the memory. For example, the first chunk of the memory is the normal vector, which is specified by @location(1) norm: vec3f . However, aside from the locational fields, there are some hidden field that is automatically filled by the WebGPU runtime. For example, the vertex_index is automatically filled by the WebGPU runtime. They are distinct from the locational parameters and handled by the WebGPU runtime, thus you just use @builtin to specify them, without the need to specify the location. The input layout is specified by the vertex buffer layout in the pipeline layout. Whereas the output layout of the vertex shader is determined by the WebGPU runtime- a builtin position field is required. As for the fragment shader, the input is just simply the output of the vertex shader, and the output is dependent on the attachments you defined. For color attachments, it is locational. For depth attachment, there is a builtin @builtin(frag_depth) . You can also tear apart teh struct and specify the location or builtin field directly in the function signature. For example: @vertex fn vertex_main( @builtin(vertex_index) index: u32, @location(0) position: vec3f, @location(1) norm: vec3f, ) -> VertexOutput { // ... } Types Types here functions the same but have many builtins for vectors and matrix, commonly used are, u32 , i32 , f32 , for unsigned integer, signed integer and float. The number signals the number of bits. vec2<T> , vec3<T> , vec4<T> , for vectors. Because f32 vector is used most frequently, you can use vec2f , vec3f , vec4f as shorthand. mat2x2<T> , mat3x3<T> , mat4x4<T> , for matrices. Because f32 matrix is used most frequently, you can use mat2x2f , mat3x3f , mat4x4f as shorthand. There are also mat2x3 , mat3x2 , mat3x4 , mat4x3 and mat2x4 , mat4x2 for other combinations. bool , bvec2 , bvec3 , bvec4 , for boolean values. sampler , sampler_comparison , sampler_shadow , for samplers. Builtin Functions There are many builtin functions in the shading language, math functions, like sin , cos , ceil , floor , clamp , exp , etc. type converters, like f32 . sampler functions, like textureSample . textureSample returns a vec4f for color, we have introduced it in the previous chapters. derivative functions, like dpdx or dpdy . Such functions are only valid in fragment shaders. How they calculate the derivatives is special- they take a two times two pixel block, then calculate the difference between the pixels, then divide by the size of the block, as is indicated by the @builtin(position) . vector functions, like dot , cross , normalize . Besides, since lighting calculation is frequently used, there are also reflect , refract and faceforward functions. matrix functions, like matrixCompMult , outerProduct , transpose , inverse . packing functions, like pack4x8snorm . Basically, it just takes a vector and pack it into a 32-bit integer. The snorm suffix means that the vector is normalized to the range of [-1, 1]. Such functions are used in the texture sampling process. unpacking functions, like unpack4x8snorm . It is the reverse of the packing functions. Control Flow The control flow is similar to that in C language, but there are some differences. There is no switch statement. There is no goto statement. You can use discard to abandon the current fragment. Trivial Here are some more trivial differences, There is no null value. There is no undefined value. ++ and -- operators are not supported, so are the += , -= , or alike. let for constant and var for variable.","title":"WGSL"},{"location":"07/#triangles-on-web-ch7-webgpu-shading-language","text":"We have used WebGPU shading language (wgsl) frequently, but we haven't actually talked about it in detail. In this chapter, we will talk about the syntax and semantics of wgsl. We mainly talks about its difference from universal programming languages like C and JavaScript.","title":"Triangles on Web Ch7 WebGPU Shading Language"},{"location":"07/#annotations","text":"The very first significant difference is annotations starting with @ . There are mainly two kinds of annotations, the function annotations and the parameters annotations.","title":"Annotations"},{"location":"07/#function-annotations","text":"We have used only four of them so far, @vertex , @fragment , @compute and @workgroup_size . The first three are used to specify the type of the function, and the last one is used to specify the workgroup size of the compute shader. We have already introduced their usage in the previous chapters.","title":"Function Annotations"},{"location":"07/#parameters-annotations","text":"Different from universal programming languages, shading language needs you to explicitly put the memory layout of the parameters and the return value of a function. We have already used @builtin and @location , now let's dive deeper into them. Let's consider the following shader function: struct VertexInput { @builtin(vertex_index) index: u32, @location(0) position: vec3f, @location(1) norm: vec3f, }; struct VertexOutput { @builtin(position) position: vec4f, @location(0) norm: vec3f, }; @vertex fn vertex_main(input: VertexInput) -> VertexOutput { // ... } struct in the shading language is more like that in C language- it specifies a memory layout. First let's look at the input- the input struct of a function in shading language consists of two parts, the locational fields and the builtin fields. The locational fields are specified by @location , which specifies the location of the field in the memory. For example, the first chunk of the memory is the normal vector, which is specified by @location(1) norm: vec3f . However, aside from the locational fields, there are some hidden field that is automatically filled by the WebGPU runtime. For example, the vertex_index is automatically filled by the WebGPU runtime. They are distinct from the locational parameters and handled by the WebGPU runtime, thus you just use @builtin to specify them, without the need to specify the location. The input layout is specified by the vertex buffer layout in the pipeline layout. Whereas the output layout of the vertex shader is determined by the WebGPU runtime- a builtin position field is required. As for the fragment shader, the input is just simply the output of the vertex shader, and the output is dependent on the attachments you defined. For color attachments, it is locational. For depth attachment, there is a builtin @builtin(frag_depth) . You can also tear apart teh struct and specify the location or builtin field directly in the function signature. For example: @vertex fn vertex_main( @builtin(vertex_index) index: u32, @location(0) position: vec3f, @location(1) norm: vec3f, ) -> VertexOutput { // ... }","title":"Parameters Annotations"},{"location":"07/#types","text":"Types here functions the same but have many builtins for vectors and matrix, commonly used are, u32 , i32 , f32 , for unsigned integer, signed integer and float. The number signals the number of bits. vec2<T> , vec3<T> , vec4<T> , for vectors. Because f32 vector is used most frequently, you can use vec2f , vec3f , vec4f as shorthand. mat2x2<T> , mat3x3<T> , mat4x4<T> , for matrices. Because f32 matrix is used most frequently, you can use mat2x2f , mat3x3f , mat4x4f as shorthand. There are also mat2x3 , mat3x2 , mat3x4 , mat4x3 and mat2x4 , mat4x2 for other combinations. bool , bvec2 , bvec3 , bvec4 , for boolean values. sampler , sampler_comparison , sampler_shadow , for samplers.","title":"Types"},{"location":"07/#builtin-functions","text":"There are many builtin functions in the shading language, math functions, like sin , cos , ceil , floor , clamp , exp , etc. type converters, like f32 . sampler functions, like textureSample . textureSample returns a vec4f for color, we have introduced it in the previous chapters. derivative functions, like dpdx or dpdy . Such functions are only valid in fragment shaders. How they calculate the derivatives is special- they take a two times two pixel block, then calculate the difference between the pixels, then divide by the size of the block, as is indicated by the @builtin(position) . vector functions, like dot , cross , normalize . Besides, since lighting calculation is frequently used, there are also reflect , refract and faceforward functions. matrix functions, like matrixCompMult , outerProduct , transpose , inverse . packing functions, like pack4x8snorm . Basically, it just takes a vector and pack it into a 32-bit integer. The snorm suffix means that the vector is normalized to the range of [-1, 1]. Such functions are used in the texture sampling process. unpacking functions, like unpack4x8snorm . It is the reverse of the packing functions.","title":"Builtin Functions"},{"location":"07/#control-flow","text":"The control flow is similar to that in C language, but there are some differences. There is no switch statement. There is no goto statement. You can use discard to abandon the current fragment.","title":"Control Flow"},{"location":"07/#trivial","text":"Here are some more trivial differences, There is no null value. There is no undefined value. ++ and -- operators are not supported, so are the += , -= , or alike. let for constant and var for variable.","title":"Trivial"},{"location":"08/","text":"Triangles on Web Ch8 OBJ Model Previously we manually set the vertices and indices of the triangle. However, in real-world applications, we usually use a 3D modeling software to create the models. The models are usually saved in a file format, and the most common file format is the OBJ file format. In this chapter, we will talk about the OBJ file format and how to load the models from the OBJ file format. Parsing OBJ File Of course, you don't want to parse the OBJ file manually. I'd recommend this library to parse the OBJ file. Actually, after writing this tutorial, I found that library was not, well, good enough for all cases. So I wrote one myself, called obj-model-parser . If you consider using that, you can find it here . If you are using my library, simply call the parseObj function, and you will get something that containing the following structure, { objects: [ { name: string | undefined; groups: { groupNames: string[] | undefined; componentsChunks: { polygons: (d.Polygon | d.Line | d.Face)[]; }; }[]; } ] } All you need is the polygons. Also, if you like this one (or maybe this tutorial), consider giving me a star on github , I'll appreciate that. Anyways, let's back to our tutorial. First, let's copy our previous lighting demo- we don't want to write them all over again. Then install this for loading the obj file. bun install obj-file-parser-ts Then let's just get a random obj file from the internet. I found this bird and download it. Please put it into the static assets, the public folder if you used vite. Do not change the file name because the mtl file has the information about the material based on the file name. Then we can write the code to load the obj file. import ObjFileParser from \"obj-file-parser\"; const objFile = await fetch(\"bird/12213_Bird_v1_l3.obj\").then((res) => res.text()); const model = new ObjFileParser(objFile).parse(); console.log(model); The parsed result is like, { models: [ { name: 'unit_cube', vertices: [ { x: 1.0, 2.0, 3.0 }, ... ], textureCoords: [ { u: 1.0, v: 2.0, w: 3.0 }, ... ], vertexNormals: [ { x: 1.0, y: 2.0, z: 3.0 }, ... ], faces: [ { material: 'brick', group: 'group1', smoothingGroup: 0, vertices: [ { vertexIndex: 1, textureCoordsIndex: 1, vertexNormalIndex: 1 }, ... ] } ] }, { ... } ], materialLibraries: [ 'mat_lib1.mtl', ... ] } materialLibraries: The material library file names. We will talk about textures later. models: The models in the obj file. Each model has vertices, textureCoords, vertexNormals, and faces. Each face consists of many vertices, and each of such vertices has the indices for vertices position, textureCoords, and vertexNormals. That is to say, we should first, change our code to, import ObjFileParser from \"obj-file-parser\"; const objFile = await fetch(\"bird/12213_Bird_v1_l3.obj\").then((res) => res.text()); const model = new ObjFileParser(objFile).parse().models[0]; Remember previously we have a get_vertices function that returns the vertices and normals? We just change that to, const get_vertices = (device: GPUDevice, model: ObjFileParser.ObjModel): [GPUBuffer, GPUVertexBufferLayout] => { const layout: GPUVertexBufferLayout = { arrayStride: 3 * 4 * 2, attributes: [{ format: \"float32x3\", offset: 0, shaderLocation: 0, }, { format: \"float32x3\", offset: 3 * 4, shaderLocation: 1 }] } const buffer = device.createBuffer({ size: verticesWithNormals.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }) device.queue.writeBuffer(buffer, 0, verticesWithNormals.buffer); return [buffer, layout]; } Show the Model Now we need to replace the verticesWithNormals with the data from the obj file. We can do this by, const get_vertices = (device: GPUDevice, model: ObjFileParser.ObjModel): [GPUBuffer, GPUVertexBufferLayout] => { const verticesWithNormals = new Float32Array(model.faces.length * 3 * 2 * 3); for (let i = 0; i < model.faces.length; i++) { const face = model.faces[i]; for (let j = 0; j < 3; j++) { const vertex = model.vertices[face.vertices[j].vertexIndex - 1]; const normal = model.vertexNormals[face.vertices[j].vertexNormalIndex - 1]; verticesWithNormals.set([vertex.x, vertex.y, vertex.z], (i * 3 + j) * 6); verticesWithNormals.set([normal.x, normal.y, normal.z], (i * 3 + j) * 6 + 3); } } const layout: GPUVertexBufferLayout = { arrayStride: 3 * 4 * 2, attributes: [{ format: \"float32x3\", offset: 0, shaderLocation: 0, }, { format: \"float32x3\", offset: 3 * 4, shaderLocation: 1 }] } const buffer = device.createBuffer({ size: verticesWithNormals.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }) device.queue.writeBuffer(buffer, 0, verticesWithNormals.buffer); return [buffer, layout]; } Please note that the index of obj file starts from 1, so we need to minus 1 to get the correct index. Lastly, change the count of the draw call to model.faces.length * 3 . Now you should see something on the screen- but if you used the same model as me, you will see that there are missing triangles- this is because the model uses square primitives. To fix this, there is no significant changes (thank god). Similar as before, we just break squares into two triangles. const get_vertices = (device: GPUDevice, model: ObjFileParser.ObjModel): [GPUBuffer, GPUVertexBufferLayout] => { const verticesWithNormals = new Float32Array(model.faces.length * 2 * 3 * 2 * 3); let groupOffset = model.faces.length * 2 * 3 * 3; for (let i = 0; i < model.faces.length; i++) { const face = model.faces[i]; const firstGroup = [ face.vertices[0], face.vertices[1], face.vertices[2] ] const secondGroup = [ face.vertices[0], face.vertices[2], face.vertices[3], ] for (let j = 0; j < 3; j++) { const vertex = model.vertices[firstGroup[j].vertexIndex - 1]; const normal = model.vertexNormals[firstGroup[j].vertexNormalIndex - 1]; verticesWithNormals.set([vertex.x, vertex.y, vertex.z], (i * 3 + j) * 6); verticesWithNormals.set([normal.x, normal.y, normal.z], (i * 3 + j) * 6 + 3); } for (let j = 0; j < 3; j++) { const vertex = model.vertices[secondGroup[j].vertexIndex - 1]; const normal = model.vertexNormals[secondGroup[j].vertexNormalIndex - 1]; verticesWithNormals.set([vertex.x, vertex.y, vertex.z], (i * 3 + j) * 6 + groupOffset); verticesWithNormals.set([normal.x, normal.y, normal.z], (i * 3 + j) * 6 + 3 + groupOffset); } } const layout: GPUVertexBufferLayout = { arrayStride: 3 * 4 * 2, attributes: [{ format: \"float32x3\", offset: 0, shaderLocation: 0, }, { format: \"float32x3\", offset: 3 * 4, shaderLocation: 1 }] } const buffer = device.createBuffer({ size: verticesWithNormals.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }) device.queue.writeBuffer(buffer, 0, verticesWithNormals.buffer); return [buffer, layout]; } Now you can see the model- but it's a bit too big. It is simple- we just move the camera a bit or scale- whichever you prefer. let rotated = vec4<f32>(rotation * 0.1 * (position - vec3f(0.0, 0.0, 0.5)), 1.0); ``` This will do the work. The `0.5` is for showing the whole model- or else the model will be clipped by the near plane. However, the axis doesn't seem right- currently it is a top view, we generally want a front view. We can just swap the `z` and `y` axis in the vertices reading to solve this- also flip the y. The following code will do the work, ```typescript for (let j = 0; j < 3; j++) { const vertex = model.vertices[firstGroup[j].vertexIndex - 1]; const normal = model.vertexNormals[firstGroup[j].vertexNormalIndex - 1]; verticesWithNormals.set([vertex.x, -vertex.z, vertex.y], (i * 3 + j) * 6); verticesWithNormals.set([normal.x, -normal.z, normal.y], (i * 3 + j) * 6 + 3); } for (let j = 0; j < 3; j++) { const vertex = model.vertices[secondGroup[j].vertexIndex - 1]; const normal = model.vertexNormals[secondGroup[j].vertexNormalIndex - 1]; verticesWithNormals.set([vertex.x, -vertex.z, vertex.y], (i * 3 + j) * 6 + groupOffset); verticesWithNormals.set([normal.x, -normal.z, normal.y], (i * 3 + j) * 6 + 3 + groupOffset); } Now you should see a bird model successfully loaded on the screen. Apply Texture We need to first, of course, load the texture. Texture resources are defined by mtl file, we also need to parse that. Sadly, I haven't found any appropriate library for parsing the mtl file, (there I found one but it is not working), so, whatever, guess we just have to parse it manually. Likely, like obj file, mtl file is also pure text. You can just open in whatever text editor and see the content. The content is like, # 3ds Max Wavefront OBJ Exporter v0.97b - (c)2007 guruware # File Created: 08.12.2011 13:29:39 newmtl 12213_bird Ns 10.0000 Ni 1.5000 d 1.0000 Tr 0.0000 Tf 1.0000 1.0000 1.0000 illum 2 Ka 1.0000 1.0000 1.0000 Kd 1.0000 1.0000 1.0000 Ks 0.0000 0.0000 0.0000 Ke 0.0000 0.0000 0.0000 map_Ka 12213_bird_diffuse.jpg map_Kd 12213_bird_diffuse.jpg map_bump Map__7_Normal Bump.jpg Let's explain the content. So basically, it's kind of like yaml. newmtl 12213_bird declares a new material, and the following lines under indentation are the properties of the material. Ns is the shininess of the material, Ni is the optical density, d is the dissolve factor, Tr is the transparency factor, Tf is the transmission filter, illum is the illumination model, Ka is the ambient color, Kd is the diffuse color, Ks is the specular color, Ke is the emission color. We don't really care about them- since we have only built one lighting model. What's important is only map_Kd - which are the ambient and diffuse textures. map_bump is the bump map- which is kind of too complex. Basically, such texture allows to simulate small bumps on the surface. We don't really care about it right now. So, that all said, we only need to find map_Kd and load them. We can do this by, const parseMtl = (mtl: string) => { const ret = [] for (const line of mtl.split(\"\\r\\n\")) { const tokens = line.trim().split(\" \"); if (tokens[0] === \"newmtl\") { ret.push({ name: tokens[1], map_Kd: \"\", }) } if (tokens[0] === \"map_Kd\") { ret[ret.length - 1].map_Kd = tokens.slice(1).join(\" \"); } } return ret; } Yeah, this code is shit, but it works whatever. Now let's check the faces properties of our model once again, faces: [ { material: 'brick', group: 'group1', smoothingGroup: 0, vertices: [ { vertexIndex: 1, textureCoordsIndex: 1, vertexNormalIndex: 1 }, ... ] } ] Now it's obvious- we just need to use the same material as this material field here. Since in our file, we only have one texture- so I'll just use load one. If you have multiple, do them one by one. Do you still remember how to load the texture? const get_texture = async (device: GPUDevice, url: string): Promise<GPUTexture> => { const img = await fetch(url).then((res) => res.blob()).then((blob) => createImageBitmap(blob)); const bitmap = await createImageBitmap(img); const texture = device.createTexture({ size: [bitmap.width, bitmap.height, 1], format: \"rgba8unorm\", usage: GPUTextureUsage.RENDER_ATTACHMENT | GPUTextureUsage.COPY_DST | GPUTextureUsage.TEXTURE_BINDING, }); device.queue.copyExternalImageToTexture({ source: bitmap }, { texture }, [bitmap.width, bitmap.height, 1]); return texture; } We also need a sampler, const get_sampler = (device: GPUDevice): GPUSampler => { return device.createSampler({ magFilter: \"linear\", minFilter: \"linear\", }) } How to pass the texture to the shader? We need to use the texture and sampler binding. const get_texture_bind_group = (device: GPUDevice, texture: GPUTexture, sampler: GPUSampler): [GPUBindGroup, GPUBindGroupLayout] => { const layout = device.createBindGroupLayout({ entries: [{ binding: 0, visibility: GPUShaderStage.FRAGMENT, texture: { sampleType: \"float\", } }, { binding: 1, visibility: GPUShaderStage.FRAGMENT, sampler: {} }] }); const bindGroup = device.createBindGroup({ layout: layout, entries: [{ binding: 0, resource: texture.createView() }, { binding: 1, resource: sampler, }] }); return [bindGroup, layout]; } const pipelineLayout = device.createPipelineLayout({ bindGroupLayouts: [bindGroupLayout, textureBindGroupLayout] }); pass.setBindGroup(1, textureBindGroup); Now, for each vertex, we need to pass it's position, normal, and texture coordinates. const get_vertices = (device: GPUDevice, model: ObjFileParser.ObjModel): [GPUBuffer, GPUVertexBufferLayout] => { const verticesWithNormals = new Float32Array(model.faces.length * 2 * 3 * 3 * 3); let groupOffset = model.faces.length * 3 * 3 * 3; for (let i = 0; i < model.faces.length; i++) { const face = model.faces[i]; const firstGroup = [ face.vertices[0], face.vertices[1], face.vertices[2] ] const secondGroup = [ face.vertices[0], face.vertices[2], face.vertices[3], ] for (let j = 0; j < 3; j++) { const vertex = model.vertices[firstGroup[j].vertexIndex - 1]; const normal = model.vertexNormals[firstGroup[j].vertexNormalIndex - 1]; const tex = model.textureCoords[firstGroup[j].textureCoordsIndex - 1]; verticesWithNormals.set([vertex.x, -vertex.z, vertex.y], (i * 3 + j) * 9); verticesWithNormals.set([normal.x, -normal.z, normal.y], (i * 3 + j) * 9 + 3); verticesWithNormals.set([tex.u, tex.v, tex.w], (i * 3 + j) * 9 + 6); } for (let j = 0; j < 3; j++) { const vertex = model.vertices[secondGroup[j].vertexIndex - 1]; const normal = model.vertexNormals[secondGroup[j].vertexNormalIndex - 1]; const tex = model.textureCoords[secondGroup[j].textureCoordsIndex - 1]; verticesWithNormals.set([vertex.x, -vertex.z, vertex.y], groupOffset + (i * 3 + j) * 9); verticesWithNormals.set([normal.x, -normal.z, normal.y], groupOffset + (i * 3 + j) * 9 + 3); verticesWithNormals.set([tex.u, tex.v, tex.w], groupOffset + (i * 3 + j) * 9 + 6); } } const layout: GPUVertexBufferLayout = { arrayStride: 3 * 4 * 3, attributes: [{ format: \"float32x3\", offset: 0, shaderLocation: 0, }, { format: \"float32x3\", offset: 3 * 4, shaderLocation: 1 }, { format: \"float32x3\", offset: 3 * 4 * 2, shaderLocation: 2, }] } const buffer = device.createBuffer({ size: verticesWithNormals.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }) device.queue.writeBuffer(buffer, 0, verticesWithNormals.buffer); return [buffer, layout]; } Now, we need to change the shader to use the texture. @group(0) @binding(0) var<uniform> projection: mat4x4<f32>; @group(0) @binding(1) var<uniform> angle: f32; @group(1) @binding(0) var texture: texture_2d<f32>; @group(1) @binding(1) var tx_sampler: sampler; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, @location(1) @interpolate(linear) real_position: vec3f, @location(2) @interpolate(linear) normal: vec3f, @location(3) @interpolate(linear) tex_coords: vec2f, }; @vertex fn vertexMain(@location(0) position: vec3f, @location(1) norm: vec3f, @location(2) tex: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(cos(angle), 0.0, sin(angle)), vec3<f32>(0.0, 1.0, 0.0), vec3<f32>(-sin(angle), 0.0, cos(angle)), ); let rotated = vec4<f32>(rotation * 0.1 * (position - vec3f(0.0, 0.0, 0.5)), 1.0); var projected = projection * (rotated - vec4<f32>(0.0, 0.0, ${f}, 0.0)); let final_position = vec4<f32>(projected.xy, 1.0 - rotated.z, projected.w); var output = VertexOutput(final_position, vertexIndex / 6, rotated.xyz, rotation * norm, tex.xy); return output; } @fragment fn fragmentMain(input: VertexOutput) -> @location(0) vec4<f32> { let light_source = vec3<f32>(-5.0, -2.0, -9.0); let l = normalize(light_source - input.real_position); let n = normalize(input.normal); let r = reflect(-l, n); let camera = vec3<f32>(0.0, 0.0, -1.0); let v = normalize(camera - input.real_position); let spec = pow(max(dot(r, v), 0.0), 32.0) * 0.8; let diff = max(dot(-n, l), 0.0) * 0.4; let amb = 0.2; let albedo = textureSample(texture, tx_sampler, input.tex_coords).xyz; let color = albedo * min(amb + diff + spec, 1.0); return vec4<f32>(color, 1.0); } However, now the bird is colored- but the wrong way. This is because we didn't enable flipY - like we previously said in the texture chapter, the texture coordinate system is different from the screen coordinate system. const get_texture = async (device: GPUDevice, url: string): Promise<GPUTexture> => { const img = await fetch(url).then((res) => res.blob()).then((blob) => createImageBitmap(blob)); const bitmap = await createImageBitmap(img); const texture = device.createTexture({ size: [bitmap.width, bitmap.height, 1], format: \"rgba8unorm\", usage: GPUTextureUsage.COPY_DST | GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.RENDER_ATTACHMENT, }); device.queue.copyExternalImageToTexture({ source: bitmap, flipY: true }, { texture }, [bitmap.width, bitmap.height, 1]); return texture; } Now the bird looks nice, albeit looks like of like metal. We can modify the parameters of our lighting model to make it look more like a bird, by reducing alpha for example. But to achieve the best result, you should use the same lighting model as the mtl file suggests, which, is far too complex for this tutorial.","title":"3D Models"},{"location":"08/#triangles-on-web-ch8-obj-model","text":"Previously we manually set the vertices and indices of the triangle. However, in real-world applications, we usually use a 3D modeling software to create the models. The models are usually saved in a file format, and the most common file format is the OBJ file format. In this chapter, we will talk about the OBJ file format and how to load the models from the OBJ file format.","title":"Triangles on Web Ch8 OBJ Model"},{"location":"08/#parsing-obj-file","text":"Of course, you don't want to parse the OBJ file manually. I'd recommend this library to parse the OBJ file. Actually, after writing this tutorial, I found that library was not, well, good enough for all cases. So I wrote one myself, called obj-model-parser . If you consider using that, you can find it here . If you are using my library, simply call the parseObj function, and you will get something that containing the following structure, { objects: [ { name: string | undefined; groups: { groupNames: string[] | undefined; componentsChunks: { polygons: (d.Polygon | d.Line | d.Face)[]; }; }[]; } ] } All you need is the polygons. Also, if you like this one (or maybe this tutorial), consider giving me a star on github , I'll appreciate that. Anyways, let's back to our tutorial. First, let's copy our previous lighting demo- we don't want to write them all over again. Then install this for loading the obj file. bun install obj-file-parser-ts Then let's just get a random obj file from the internet. I found this bird and download it. Please put it into the static assets, the public folder if you used vite. Do not change the file name because the mtl file has the information about the material based on the file name. Then we can write the code to load the obj file. import ObjFileParser from \"obj-file-parser\"; const objFile = await fetch(\"bird/12213_Bird_v1_l3.obj\").then((res) => res.text()); const model = new ObjFileParser(objFile).parse(); console.log(model); The parsed result is like, { models: [ { name: 'unit_cube', vertices: [ { x: 1.0, 2.0, 3.0 }, ... ], textureCoords: [ { u: 1.0, v: 2.0, w: 3.0 }, ... ], vertexNormals: [ { x: 1.0, y: 2.0, z: 3.0 }, ... ], faces: [ { material: 'brick', group: 'group1', smoothingGroup: 0, vertices: [ { vertexIndex: 1, textureCoordsIndex: 1, vertexNormalIndex: 1 }, ... ] } ] }, { ... } ], materialLibraries: [ 'mat_lib1.mtl', ... ] } materialLibraries: The material library file names. We will talk about textures later. models: The models in the obj file. Each model has vertices, textureCoords, vertexNormals, and faces. Each face consists of many vertices, and each of such vertices has the indices for vertices position, textureCoords, and vertexNormals. That is to say, we should first, change our code to, import ObjFileParser from \"obj-file-parser\"; const objFile = await fetch(\"bird/12213_Bird_v1_l3.obj\").then((res) => res.text()); const model = new ObjFileParser(objFile).parse().models[0]; Remember previously we have a get_vertices function that returns the vertices and normals? We just change that to, const get_vertices = (device: GPUDevice, model: ObjFileParser.ObjModel): [GPUBuffer, GPUVertexBufferLayout] => { const layout: GPUVertexBufferLayout = { arrayStride: 3 * 4 * 2, attributes: [{ format: \"float32x3\", offset: 0, shaderLocation: 0, }, { format: \"float32x3\", offset: 3 * 4, shaderLocation: 1 }] } const buffer = device.createBuffer({ size: verticesWithNormals.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }) device.queue.writeBuffer(buffer, 0, verticesWithNormals.buffer); return [buffer, layout]; }","title":"Parsing OBJ File"},{"location":"08/#show-the-model","text":"Now we need to replace the verticesWithNormals with the data from the obj file. We can do this by, const get_vertices = (device: GPUDevice, model: ObjFileParser.ObjModel): [GPUBuffer, GPUVertexBufferLayout] => { const verticesWithNormals = new Float32Array(model.faces.length * 3 * 2 * 3); for (let i = 0; i < model.faces.length; i++) { const face = model.faces[i]; for (let j = 0; j < 3; j++) { const vertex = model.vertices[face.vertices[j].vertexIndex - 1]; const normal = model.vertexNormals[face.vertices[j].vertexNormalIndex - 1]; verticesWithNormals.set([vertex.x, vertex.y, vertex.z], (i * 3 + j) * 6); verticesWithNormals.set([normal.x, normal.y, normal.z], (i * 3 + j) * 6 + 3); } } const layout: GPUVertexBufferLayout = { arrayStride: 3 * 4 * 2, attributes: [{ format: \"float32x3\", offset: 0, shaderLocation: 0, }, { format: \"float32x3\", offset: 3 * 4, shaderLocation: 1 }] } const buffer = device.createBuffer({ size: verticesWithNormals.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }) device.queue.writeBuffer(buffer, 0, verticesWithNormals.buffer); return [buffer, layout]; } Please note that the index of obj file starts from 1, so we need to minus 1 to get the correct index. Lastly, change the count of the draw call to model.faces.length * 3 . Now you should see something on the screen- but if you used the same model as me, you will see that there are missing triangles- this is because the model uses square primitives. To fix this, there is no significant changes (thank god). Similar as before, we just break squares into two triangles. const get_vertices = (device: GPUDevice, model: ObjFileParser.ObjModel): [GPUBuffer, GPUVertexBufferLayout] => { const verticesWithNormals = new Float32Array(model.faces.length * 2 * 3 * 2 * 3); let groupOffset = model.faces.length * 2 * 3 * 3; for (let i = 0; i < model.faces.length; i++) { const face = model.faces[i]; const firstGroup = [ face.vertices[0], face.vertices[1], face.vertices[2] ] const secondGroup = [ face.vertices[0], face.vertices[2], face.vertices[3], ] for (let j = 0; j < 3; j++) { const vertex = model.vertices[firstGroup[j].vertexIndex - 1]; const normal = model.vertexNormals[firstGroup[j].vertexNormalIndex - 1]; verticesWithNormals.set([vertex.x, vertex.y, vertex.z], (i * 3 + j) * 6); verticesWithNormals.set([normal.x, normal.y, normal.z], (i * 3 + j) * 6 + 3); } for (let j = 0; j < 3; j++) { const vertex = model.vertices[secondGroup[j].vertexIndex - 1]; const normal = model.vertexNormals[secondGroup[j].vertexNormalIndex - 1]; verticesWithNormals.set([vertex.x, vertex.y, vertex.z], (i * 3 + j) * 6 + groupOffset); verticesWithNormals.set([normal.x, normal.y, normal.z], (i * 3 + j) * 6 + 3 + groupOffset); } } const layout: GPUVertexBufferLayout = { arrayStride: 3 * 4 * 2, attributes: [{ format: \"float32x3\", offset: 0, shaderLocation: 0, }, { format: \"float32x3\", offset: 3 * 4, shaderLocation: 1 }] } const buffer = device.createBuffer({ size: verticesWithNormals.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }) device.queue.writeBuffer(buffer, 0, verticesWithNormals.buffer); return [buffer, layout]; } Now you can see the model- but it's a bit too big. It is simple- we just move the camera a bit or scale- whichever you prefer. let rotated = vec4<f32>(rotation * 0.1 * (position - vec3f(0.0, 0.0, 0.5)), 1.0); ``` This will do the work. The `0.5` is for showing the whole model- or else the model will be clipped by the near plane. However, the axis doesn't seem right- currently it is a top view, we generally want a front view. We can just swap the `z` and `y` axis in the vertices reading to solve this- also flip the y. The following code will do the work, ```typescript for (let j = 0; j < 3; j++) { const vertex = model.vertices[firstGroup[j].vertexIndex - 1]; const normal = model.vertexNormals[firstGroup[j].vertexNormalIndex - 1]; verticesWithNormals.set([vertex.x, -vertex.z, vertex.y], (i * 3 + j) * 6); verticesWithNormals.set([normal.x, -normal.z, normal.y], (i * 3 + j) * 6 + 3); } for (let j = 0; j < 3; j++) { const vertex = model.vertices[secondGroup[j].vertexIndex - 1]; const normal = model.vertexNormals[secondGroup[j].vertexNormalIndex - 1]; verticesWithNormals.set([vertex.x, -vertex.z, vertex.y], (i * 3 + j) * 6 + groupOffset); verticesWithNormals.set([normal.x, -normal.z, normal.y], (i * 3 + j) * 6 + 3 + groupOffset); } Now you should see a bird model successfully loaded on the screen.","title":"Show the Model"},{"location":"08/#apply-texture","text":"We need to first, of course, load the texture. Texture resources are defined by mtl file, we also need to parse that. Sadly, I haven't found any appropriate library for parsing the mtl file, (there I found one but it is not working), so, whatever, guess we just have to parse it manually. Likely, like obj file, mtl file is also pure text. You can just open in whatever text editor and see the content. The content is like, # 3ds Max Wavefront OBJ Exporter v0.97b - (c)2007 guruware # File Created: 08.12.2011 13:29:39 newmtl 12213_bird Ns 10.0000 Ni 1.5000 d 1.0000 Tr 0.0000 Tf 1.0000 1.0000 1.0000 illum 2 Ka 1.0000 1.0000 1.0000 Kd 1.0000 1.0000 1.0000 Ks 0.0000 0.0000 0.0000 Ke 0.0000 0.0000 0.0000 map_Ka 12213_bird_diffuse.jpg map_Kd 12213_bird_diffuse.jpg map_bump Map__7_Normal Bump.jpg Let's explain the content. So basically, it's kind of like yaml. newmtl 12213_bird declares a new material, and the following lines under indentation are the properties of the material. Ns is the shininess of the material, Ni is the optical density, d is the dissolve factor, Tr is the transparency factor, Tf is the transmission filter, illum is the illumination model, Ka is the ambient color, Kd is the diffuse color, Ks is the specular color, Ke is the emission color. We don't really care about them- since we have only built one lighting model. What's important is only map_Kd - which are the ambient and diffuse textures. map_bump is the bump map- which is kind of too complex. Basically, such texture allows to simulate small bumps on the surface. We don't really care about it right now. So, that all said, we only need to find map_Kd and load them. We can do this by, const parseMtl = (mtl: string) => { const ret = [] for (const line of mtl.split(\"\\r\\n\")) { const tokens = line.trim().split(\" \"); if (tokens[0] === \"newmtl\") { ret.push({ name: tokens[1], map_Kd: \"\", }) } if (tokens[0] === \"map_Kd\") { ret[ret.length - 1].map_Kd = tokens.slice(1).join(\" \"); } } return ret; } Yeah, this code is shit, but it works whatever. Now let's check the faces properties of our model once again, faces: [ { material: 'brick', group: 'group1', smoothingGroup: 0, vertices: [ { vertexIndex: 1, textureCoordsIndex: 1, vertexNormalIndex: 1 }, ... ] } ] Now it's obvious- we just need to use the same material as this material field here. Since in our file, we only have one texture- so I'll just use load one. If you have multiple, do them one by one. Do you still remember how to load the texture? const get_texture = async (device: GPUDevice, url: string): Promise<GPUTexture> => { const img = await fetch(url).then((res) => res.blob()).then((blob) => createImageBitmap(blob)); const bitmap = await createImageBitmap(img); const texture = device.createTexture({ size: [bitmap.width, bitmap.height, 1], format: \"rgba8unorm\", usage: GPUTextureUsage.RENDER_ATTACHMENT | GPUTextureUsage.COPY_DST | GPUTextureUsage.TEXTURE_BINDING, }); device.queue.copyExternalImageToTexture({ source: bitmap }, { texture }, [bitmap.width, bitmap.height, 1]); return texture; } We also need a sampler, const get_sampler = (device: GPUDevice): GPUSampler => { return device.createSampler({ magFilter: \"linear\", minFilter: \"linear\", }) } How to pass the texture to the shader? We need to use the texture and sampler binding. const get_texture_bind_group = (device: GPUDevice, texture: GPUTexture, sampler: GPUSampler): [GPUBindGroup, GPUBindGroupLayout] => { const layout = device.createBindGroupLayout({ entries: [{ binding: 0, visibility: GPUShaderStage.FRAGMENT, texture: { sampleType: \"float\", } }, { binding: 1, visibility: GPUShaderStage.FRAGMENT, sampler: {} }] }); const bindGroup = device.createBindGroup({ layout: layout, entries: [{ binding: 0, resource: texture.createView() }, { binding: 1, resource: sampler, }] }); return [bindGroup, layout]; } const pipelineLayout = device.createPipelineLayout({ bindGroupLayouts: [bindGroupLayout, textureBindGroupLayout] }); pass.setBindGroup(1, textureBindGroup); Now, for each vertex, we need to pass it's position, normal, and texture coordinates. const get_vertices = (device: GPUDevice, model: ObjFileParser.ObjModel): [GPUBuffer, GPUVertexBufferLayout] => { const verticesWithNormals = new Float32Array(model.faces.length * 2 * 3 * 3 * 3); let groupOffset = model.faces.length * 3 * 3 * 3; for (let i = 0; i < model.faces.length; i++) { const face = model.faces[i]; const firstGroup = [ face.vertices[0], face.vertices[1], face.vertices[2] ] const secondGroup = [ face.vertices[0], face.vertices[2], face.vertices[3], ] for (let j = 0; j < 3; j++) { const vertex = model.vertices[firstGroup[j].vertexIndex - 1]; const normal = model.vertexNormals[firstGroup[j].vertexNormalIndex - 1]; const tex = model.textureCoords[firstGroup[j].textureCoordsIndex - 1]; verticesWithNormals.set([vertex.x, -vertex.z, vertex.y], (i * 3 + j) * 9); verticesWithNormals.set([normal.x, -normal.z, normal.y], (i * 3 + j) * 9 + 3); verticesWithNormals.set([tex.u, tex.v, tex.w], (i * 3 + j) * 9 + 6); } for (let j = 0; j < 3; j++) { const vertex = model.vertices[secondGroup[j].vertexIndex - 1]; const normal = model.vertexNormals[secondGroup[j].vertexNormalIndex - 1]; const tex = model.textureCoords[secondGroup[j].textureCoordsIndex - 1]; verticesWithNormals.set([vertex.x, -vertex.z, vertex.y], groupOffset + (i * 3 + j) * 9); verticesWithNormals.set([normal.x, -normal.z, normal.y], groupOffset + (i * 3 + j) * 9 + 3); verticesWithNormals.set([tex.u, tex.v, tex.w], groupOffset + (i * 3 + j) * 9 + 6); } } const layout: GPUVertexBufferLayout = { arrayStride: 3 * 4 * 3, attributes: [{ format: \"float32x3\", offset: 0, shaderLocation: 0, }, { format: \"float32x3\", offset: 3 * 4, shaderLocation: 1 }, { format: \"float32x3\", offset: 3 * 4 * 2, shaderLocation: 2, }] } const buffer = device.createBuffer({ size: verticesWithNormals.length * 4, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }) device.queue.writeBuffer(buffer, 0, verticesWithNormals.buffer); return [buffer, layout]; } Now, we need to change the shader to use the texture. @group(0) @binding(0) var<uniform> projection: mat4x4<f32>; @group(0) @binding(1) var<uniform> angle: f32; @group(1) @binding(0) var texture: texture_2d<f32>; @group(1) @binding(1) var tx_sampler: sampler; struct VertexOutput { @builtin(position) pos: vec4f, @location(0) @interpolate(flat) face: u32, @location(1) @interpolate(linear) real_position: vec3f, @location(2) @interpolate(linear) normal: vec3f, @location(3) @interpolate(linear) tex_coords: vec2f, }; @vertex fn vertexMain(@location(0) position: vec3f, @location(1) norm: vec3f, @location(2) tex: vec3f, @builtin(vertex_index) vertexIndex: u32) -> VertexOutput { let rotation = mat3x3<f32>( vec3<f32>(cos(angle), 0.0, sin(angle)), vec3<f32>(0.0, 1.0, 0.0), vec3<f32>(-sin(angle), 0.0, cos(angle)), ); let rotated = vec4<f32>(rotation * 0.1 * (position - vec3f(0.0, 0.0, 0.5)), 1.0); var projected = projection * (rotated - vec4<f32>(0.0, 0.0, ${f}, 0.0)); let final_position = vec4<f32>(projected.xy, 1.0 - rotated.z, projected.w); var output = VertexOutput(final_position, vertexIndex / 6, rotated.xyz, rotation * norm, tex.xy); return output; } @fragment fn fragmentMain(input: VertexOutput) -> @location(0) vec4<f32> { let light_source = vec3<f32>(-5.0, -2.0, -9.0); let l = normalize(light_source - input.real_position); let n = normalize(input.normal); let r = reflect(-l, n); let camera = vec3<f32>(0.0, 0.0, -1.0); let v = normalize(camera - input.real_position); let spec = pow(max(dot(r, v), 0.0), 32.0) * 0.8; let diff = max(dot(-n, l), 0.0) * 0.4; let amb = 0.2; let albedo = textureSample(texture, tx_sampler, input.tex_coords).xyz; let color = albedo * min(amb + diff + spec, 1.0); return vec4<f32>(color, 1.0); } However, now the bird is colored- but the wrong way. This is because we didn't enable flipY - like we previously said in the texture chapter, the texture coordinate system is different from the screen coordinate system. const get_texture = async (device: GPUDevice, url: string): Promise<GPUTexture> => { const img = await fetch(url).then((res) => res.blob()).then((blob) => createImageBitmap(blob)); const bitmap = await createImageBitmap(img); const texture = device.createTexture({ size: [bitmap.width, bitmap.height, 1], format: \"rgba8unorm\", usage: GPUTextureUsage.COPY_DST | GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.RENDER_ATTACHMENT, }); device.queue.copyExternalImageToTexture({ source: bitmap, flipY: true }, { texture }, [bitmap.width, bitmap.height, 1]); return texture; } Now the bird looks nice, albeit looks like of like metal. We can modify the parameters of our lighting model to make it look more like a bird, by reducing alpha for example. But to achieve the best result, you should use the same lighting model as the mtl file suggests, which, is far too complex for this tutorial.","title":"Apply Texture"},{"location":"09/","text":"Triangles on Web Ch9 B\u00e9zier Curve, B-spline Curve and Surface Previously, we took a polygonal approach for shapes. That is, we defined the shape with straight shapes, like straight lines and triangles. However, sometimes we may need inherently curve shapes, like in vector graphics design. In this chapter, we will learn about B\u00e9zier curve and B-spline curve, which are two popular curve representations. Each two kind of two curves combined together, we get a surface. Curve Math In a mathematical sense, a curve is a continuous mapping from a segment of the real points in space. That is, Curve(\\sigma): \\mathbb{R} \\rightarrow \\mathbb{R}^n Or a more accessible form, Curve(\\sigma) = (x(\\sigma), y(\\sigma), z(\\sigma)) \\space \\text{where} \\space \\sigma \\in [\\sigma_{min}, \\sigma_{max}] Where \\sigma is the parameter that defines the curve. For example, in a straight line, \\sigma can be the distance from the starting point. In a circle, \\sigma can be the angle. Due to the different nature of the \\sigma , we usually re-parameterize the curve. Re-parameterization is the process of changing the parameter \\sigma to another parameter \\tau . The re-parameterization is usually a function \\tau = rep(\\sigma) . Then the new curve is, Curve(\\tau) = (x(rep(\\tau)), y(rep(\\tau)), z(rep(\\tau))) \\space \\text{where} \\space \\tau \\in [\\tau_{min}, \\tau_{max}] We often take a unified parameter called the curve-length. In an intuitive sense, the curve-length is basically how long a point have traveled on the curve from an arbitrary starting point. The curve-length is denoted as s . In a more rigorous sense, the curve-length is defined as, s(\\sigma) = \\int_{\\sigma_{min}}^{\\sigma} \\sqrt{\\left(\\frac{dx}{d\\sigma}\\right)^2 + \\left(\\frac{dy}{d\\sigma}\\right)^2 + \\left(\\frac{dz}{d\\sigma}\\right)^2} d\\sigma So a curve can be represented as, Curve(s) = (x(s), y(s), z(s)) \\space \\text{where} \\space s \\in [s_{min}, s_{max}] Often, we prefer a normalized form. With the curve-length parameter, the unit of the parameter is unified, but it is not normalized. We can simply use a linear transformation from the curve-length to a new parameter t . t = \\frac{s - s_{min}}{s_{max} - s_{min}} In all, for any given curve with any given parameter \\sigma , we can always re-parameterize it to a normalized curve-length parameter t . B\u00e9zier Curve Then we need to consider how we can define a curve- you can't just let the user input the curve function- that is just, stupid. Currently, two popular methods are B\u00e9zier curve and B-spline curve. We will first talk about B\u00e9zier curve. The idea of B\u00e9zier curve is simple- instead of asking user the curve function, we ask the user to define a few control points- the points that controls the shape of the curve, then we define a good curve that roughly fits the straight lines between the control points. Let's suppose the control points are CP_i \\space \\text{where} i = 0, 1, ..., n , then B\u00e9zier curve is defined as, Curve(t) = \\sum_{i=0}^{n} CP_i \\cdot B_{n}^{i}(t) Where B_{n}^{i+1} is called the Bernstein polynomial. It is defined as, B_{n}^{i}(t) = \\binom{n}{i} t^i (1 - t)^{n - i} Where \\binom{n}{i} is the binomial coefficient. It is defined as, \\binom{n}{i} = \\frac{n!}{i! (n - i)!} The idea behind the B\u00e9zier curve is that, for a certain point at parameter t , the further the control point is, the less influence it has on the point. So we can write down, Curve(t) = \\sum_{i=0}^{n} CP_i \\cdot Weight_i(t) A natural assumption we can make is that, we want the sum of the weight stay the same throughout the curve. If we force the sum of the weight to be 1, we can write down, 1 = \\sum_{i=0}^{n} Weight_i(t) For B\u00e9zier curve, we can introduce the Bernstein polynomial in the following way, 1 = (1 - t + t)^n = \\sum_{i=0}^{n} B_{n}^{i}(t) So the weight is defined as Bernstein polynomial. B-spline Curve The B-spline is also based on the weight-control-point idea. In B\u00e9zier curve, since at any given point, the Bernstein weight is not zero, except for the starting and ending point, so if you make any adjustment to any control point, the whole curve will be affected. This is not always desirable. B-spline Curve address the issue by introducing a knot vector. The knot vector is a vector that defines the weight of the control points. The B-spline curve is defined as, Curve(t) = \\sum_{i=0}^{n-1} CP_i \\cdot N_{i, p}(t) Where N_{i, p}(t) is the B-spline basis function. It is defined as, N_{i, p}(t) = \\frac{t - t_i}{t_{i+p} - t_i} N_{i, p-1}(t) + \\frac{t_{i+p+1} - t}{t_{i+p+1} - t_{i+1}} N_{i+1, p-1}(t) Woa, that's scary. But don't be afraid, it is actually quite simple. The basic idea is that the instead of using the Bernstein polynomial, the B-spline function only spans a few control points, and evaluated zero outside the span. The number of points in the span is commensurate degree of the B-spline function, that is the p in the N_{i, p}(t) . Let's suppose we have some CP_i , and we want CP_i to have the greatest influence at t_{i+1} . Considering the weight restriction, we make the following restriction on the function we construct, N_{i, p}(t) , should be zero outside the span of CP_i . should be one at the corresponding t_{i+1} for a given CP_i . sum of all N_{i, p}(t) over i should be one. In the following, when encountering zero divided by zero, we can just take the limit as its value- it will always have one. However, actually, the values are always zero for such cases. For the zero degree, we only consider the sole point CP_i . It is easy to define the weights as, one for a certain segment of t and zero for the rest. N_{i, 0}(t) = \\begin{cases} 1 & \\text{if} \\space t_i \\leq t < t_{i+1} \\\\ 0 & \\text{otherwise} \\end{cases} So this is just a bunch of control points. Then, we increase the degree- the span. That is, we make one control point CP_i influence the next one control point, P_{i+1} . So we have, N_{i, 1}(t) = \\begin{cases} \\frac{t - t_i}{t_{i+1} - t_i} & \\text{if} \\space t_i \\leq t < t_{i+1} \\\\ \\frac{t_{i+1} - t}{t_{i+2} - t_{i+1}} & \\text{if} \\space t_{i+1} \\leq t < t_{i+2} \\\\ 0 & \\text{otherwise} \\end{cases} This function will form a triangle spike at t_{i+1} , spanning from t_{i} to t_{i+2} . The one-zero restriction is satisfied, we check the sum now. N_{i, 1}(t) + N_{i + 1, 1}(t) \\\\ =\\frac{ t - t_{i + 1}}{t_{i + 2} - t_{i + 1}} + \\frac{t_{i + 2} - t}{t_{i + 2} - t_{i + 1}} = 1 This sum is done for t_{i+1} < t < t_{i+2} . Then, we increase the degree again. We make the control point CP_i influence the next two control points. This time, instead of interpolating the t value, we will interpolate the degree function. Like the following, N_{i, 2}(t) = \\frac{t - t_i}{t_{i+1} - t_i} N_{i, 1}(t) + \\frac{t_{i+2} - t}{t_{i+2} - t_{i+1}} N_{i+1, 1}(t) We know that for N_{i, 1}(t) , is is one at t_{i+1} and zero at t_i and t_{i+2} . Similarly, N_{i+1, 1}(t) is one at t_{i+2} and zero at t_{i+1} and t_{i+3} . So for t_{i + 1} , the former term is always one, while the latter stays zero. In addition, if we sum all the N_{i, p}(t) over i , we only have three terms to consider, N_{i, 2}(t) = \\frac{t - t_i}{t_{i+1} - t_i} N_{i, 1}(t) + \\frac{t_{i+2} - t}{t_{i+2} - t_{i+1}} N_{i+1, 1}(t) \\\\ N_{i + 1, 2}(t) = \\frac{t - t_{i+1}}{t_{i+2} - t_{i+1}} N_{i+1, 1}(t) + \\frac{t_{i+3} - t}{t_{i+3} - t_{i+2}} N_{i+2, 1}(t) \\\\ N_{i + 2, 2}(t) = \\frac{t - t_{i+2}}{t_{i+3} - t_{i+2}} N_{i+2, 1}(t) + \\frac{t_{i+4} - t}{t_{i+4} - t_{i+3}} N_{i+3, 1}(t) \\\\ Adding up the three terms, we have, N_{i, 2}(t) + N_{i + 1, 2}(t) + N_{i + 2, 2}(t) = 1 The sum is done for t_{i+2} < t < t_{i+3} . So the remaining terms are zero. Note that, in this calculation, the only assumption is that, N_{i, 1} + N_{i + 1, 1} = 1 . If we take, N_{i, p} = \\frac{t - t_i}{t_{i+p} - t_i} N_{i, p-1} + \\frac{t_{i+p+1} - t}{t_{i+p+1} - t_{i+1}} N_{i+1, p-1} The same calculation can be done for any p , it is just more terms to consider. Here is a simple illustration for that. We can take N_{i, p} and N_{i+1, p} , they will produce N_{i, p+1} and N_{i+1, p+1} . If we iterate through, we simply get all the N_{i, p} . So we can conclude that the weight is always one. The one-zero restriction is more obviously, we just skip the calculation here. Producing B\u00e9zier Curve This is a programming tutorial, not purely math. So we need to implement Bezier and B-spline curve. Since their only difference is the weight function, we will only implement B\u00e9zier curve for reference. However, up until now, WebGPU does not directly support B\u00e9zier curve nor B-spline curve- since polygonal shapes are more common in computer graphics. However, we can perform CPU calculation and gather the lines, then draw the lines with WebGPU. That's enough rambling, let's implement the B\u00e9zier curve. Let's review how to set up WebGPU again- this will be, sadly, the last time we do this in the series. First, request an adapter, from which we then request a device. The adapter is obtained via navigator.gpu.requestAdapter() . const requestDevice = async (): Promise<[GPUAdapter, GPUDevice] | null> => { const adapter = await navigator.gpu.requestAdapter(); if (!adapter) { console.error('WebGPU not supported'); return null; } const device = await adapter.requestDevice(); console.log(device); return [adapter, device]; } Then, we set the canvas and the context. The context is obtained via canvas.getContext('webgpu') . const getContext = async (device: GPUDevice): Promise<[GPUCanvasContext, GPUTextureFormat]> => { const canvas = document.getElementById('app') as HTMLCanvasElement; const context = canvas.getContext(\"webgpu\")!; const canvasFormat = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device: device, format: canvasFormat, }); return [context, canvasFormat]; } Then, we set up the shader module. The vertex shader now takes a t parameter, and gives the position of the point on the curve. The fragment shader is the same as before. We suppose we will have a b\u00e9zier curve with, Curve(t) = (-0.5, -0.5) B_4^0(t) + (0.5, -0.5) B_4^1(t) + (0.5, 0.5) B_4^2(t) + (-0.5, 0.5) B_4^3(t) + (-0.5, -0.5) B_4^4(t) So we have the following code, const getShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"shader\", code: ` fn factorial(n: i32) -> i32 { var result: i32 = 1; for (var i: i32 = 1; i <= n; i = i + 1) { result = result * i; } return result; } fn comb(n: i32, k: i32) -> i32 { return factorial(n) / (factorial(k) * factorial(n - k)); } @vertex fn vertexMain(@location(0) t: f32) -> @builtin(position) vec4<f32> { // calculate all the bezier functions var b40 = f32(comb(4, 0)) * pow(1.0 - t, 4 - 0) * pow(t, 0); var b41 = f32(comb(4, 1)) * pow(1.0 - t, 4 - 1) * pow(t, 1); var b42 = f32(comb(4, 2)) * pow(1.0 - t, 4 - 2) * pow(t, 2); var b43 = f32(comb(4, 3)) * pow(1.0 - t, 4 - 3) * pow(t, 3); var b44 = f32(comb(4, 4)) * pow(1.0 - t, 4 - 4) * pow(t, 4); // control points are (-0.5, -0.5), (-0.5, 0.5), (0.5, 0.5), (0.5, -0.5), (-0.5, -0.5) var p0 = vec2<f32>(-0.5, -0.5); var p1 = vec2<f32>(-0.5, 0.5); var p2 = vec2<f32>(0.5, 0.5); var p3 = vec2<f32>(0.5, -0.5); var p4 = vec2<f32>(-0.5, -0.5); var res = vec2<f32>(0.0, 0.0); res = res + b40 * p0; res = res + b41 * p1; res = res + b42 * p2; res = res + b43 * p3; res = res + b44 * p4; return vec4<f32>(res, 0.0, 1.0); } @fragment fn fragmentMain() -> @location(0) vec4<f32> { return vec4<f32>(1.0, 1.0, 1.0, 1.0); } ` }); } Then the vertex buffer, const getVertexBuffer = async (device: GPUDevice): Promise<[GPUBuffer, GPUVertexBufferLayout]> => { const totalPoints = 100; const t = new Float32Array( Array.from( new Array(totalPoints - 1), (_, i) => i ).flatMap((i) => [i / totalPoints, (i + 1) / totalPoints]) ); const tBufferLayout: GPUVertexBufferLayout = { attributes: [ { shaderLocation: 0, offset: 0, format: \"float32\" } ], arrayStride: 4, stepMode: \"vertex\" }; const tBuffer = device.createBuffer({ size: t.byteLength, usage: GPUBufferUsage.VERTEX, mappedAtCreation: true }); new Float32Array(tBuffer.getMappedRange()).set(t); tBuffer.unmap(); return [tBuffer, tBufferLayout]; } Then, the encoder, render pass, pipeline- everything, const main = async () => { const [_, device] = (await requestDevice())!; const [context, format] = await getContext(device); const shaderModule = await getShaderModule(device); const [tBuffer, tBufferLayout] = await getVertexBuffer(device); const encoder = device.createCommandEncoder(); const renderPassDescriptor: GPURenderPassDescriptor = { colorAttachments: [{ view: context.getCurrentTexture().createView(), clearValue: { r: 0.1, g: 0.1, b: 0.1, a: 1.0 }, storeOp: \"store\", loadOp: \"clear\", }], } const passEncoder = encoder.beginRenderPass(renderPassDescriptor); const pipeline = device.createRenderPipeline({ vertex: { module: shaderModule, entryPoint: \"vertexMain\", buffers: [tBufferLayout] }, fragment: { module: shaderModule, entryPoint: \"fragmentMain\", targets: [{ format: format }] }, primitive: { topology: \"line-list\", }, layout: \"auto\" }); passEncoder.setPipeline(pipeline); passEncoder.setVertexBuffer(0, tBuffer); passEncoder.draw(200); passEncoder.end(); device.queue.submit([encoder.finish()]); } Now, you can see the B\u00e9zier curve on the screen. The curve is defined by the control points, and the curve is evaluated at the given parameter t . Surface We have understood two kinds of curves- now the surface is just a combination of two curves. The surface is defined as, Surface(u, v) = \\sum_{i=0}^{n} \\sum_{j=0}^{m} CP_{i, j} \\cdot Weight_{i, j}(u, v) The weight can simply be defined as the product of the weights of two base functions. That is, if you like, you can write that as, Surface(u, v) = \\sum_{i=0}^{n} Weight_{i}(u) \\sum_{j=0}^{m} CP_{i, j} \\cdot Weight_{j}(v) Please note that the result of the following part \\sum_{j=0}^{m} CP_{i, j} \\cdot Weight_{j}(v) is a point. So if we fix a v , we get a curve. Since u and v are interchangeable, we can conclude that if we fix a u , we also get a curve. If we choose both B\u00e9zier curve for u and v , we have a B\u00e9zier surface. If we choose B-spline curve for u and v , we have a B-spline surface. So, surface is just a combination of two curves- we have drawn the curves, we can draw the surface. But since drawing 3D things requires us to, ugh, again, deal with all the perspective projection, lighting, and so on, we will not draw the surface with code in this tutorial. Conclusion Okay, so this whole series ends. We have learned about the basic concepts of computer graphics, WebGPU, and some advanced topics like B\u00e9zier curve and B-spline curve. We have also learned about the shader language, WGSL, and how to load 3D models. Hope it is helpful for you.","title":"Curves and Surfaces"},{"location":"09/#triangles-on-web-ch9-bezier-curve-b-spline-curve-and-surface","text":"Previously, we took a polygonal approach for shapes. That is, we defined the shape with straight shapes, like straight lines and triangles. However, sometimes we may need inherently curve shapes, like in vector graphics design. In this chapter, we will learn about B\u00e9zier curve and B-spline curve, which are two popular curve representations. Each two kind of two curves combined together, we get a surface.","title":"Triangles on Web Ch9 B\u00e9zier Curve, B-spline Curve and Surface"},{"location":"09/#curve-math","text":"In a mathematical sense, a curve is a continuous mapping from a segment of the real points in space. That is, Curve(\\sigma): \\mathbb{R} \\rightarrow \\mathbb{R}^n Or a more accessible form, Curve(\\sigma) = (x(\\sigma), y(\\sigma), z(\\sigma)) \\space \\text{where} \\space \\sigma \\in [\\sigma_{min}, \\sigma_{max}] Where \\sigma is the parameter that defines the curve. For example, in a straight line, \\sigma can be the distance from the starting point. In a circle, \\sigma can be the angle. Due to the different nature of the \\sigma , we usually re-parameterize the curve. Re-parameterization is the process of changing the parameter \\sigma to another parameter \\tau . The re-parameterization is usually a function \\tau = rep(\\sigma) . Then the new curve is, Curve(\\tau) = (x(rep(\\tau)), y(rep(\\tau)), z(rep(\\tau))) \\space \\text{where} \\space \\tau \\in [\\tau_{min}, \\tau_{max}] We often take a unified parameter called the curve-length. In an intuitive sense, the curve-length is basically how long a point have traveled on the curve from an arbitrary starting point. The curve-length is denoted as s . In a more rigorous sense, the curve-length is defined as, s(\\sigma) = \\int_{\\sigma_{min}}^{\\sigma} \\sqrt{\\left(\\frac{dx}{d\\sigma}\\right)^2 + \\left(\\frac{dy}{d\\sigma}\\right)^2 + \\left(\\frac{dz}{d\\sigma}\\right)^2} d\\sigma So a curve can be represented as, Curve(s) = (x(s), y(s), z(s)) \\space \\text{where} \\space s \\in [s_{min}, s_{max}] Often, we prefer a normalized form. With the curve-length parameter, the unit of the parameter is unified, but it is not normalized. We can simply use a linear transformation from the curve-length to a new parameter t . t = \\frac{s - s_{min}}{s_{max} - s_{min}} In all, for any given curve with any given parameter \\sigma , we can always re-parameterize it to a normalized curve-length parameter t .","title":"Curve Math"},{"location":"09/#bezier-curve","text":"Then we need to consider how we can define a curve- you can't just let the user input the curve function- that is just, stupid. Currently, two popular methods are B\u00e9zier curve and B-spline curve. We will first talk about B\u00e9zier curve. The idea of B\u00e9zier curve is simple- instead of asking user the curve function, we ask the user to define a few control points- the points that controls the shape of the curve, then we define a good curve that roughly fits the straight lines between the control points. Let's suppose the control points are CP_i \\space \\text{where} i = 0, 1, ..., n , then B\u00e9zier curve is defined as, Curve(t) = \\sum_{i=0}^{n} CP_i \\cdot B_{n}^{i}(t) Where B_{n}^{i+1} is called the Bernstein polynomial. It is defined as, B_{n}^{i}(t) = \\binom{n}{i} t^i (1 - t)^{n - i} Where \\binom{n}{i} is the binomial coefficient. It is defined as, \\binom{n}{i} = \\frac{n!}{i! (n - i)!} The idea behind the B\u00e9zier curve is that, for a certain point at parameter t , the further the control point is, the less influence it has on the point. So we can write down, Curve(t) = \\sum_{i=0}^{n} CP_i \\cdot Weight_i(t) A natural assumption we can make is that, we want the sum of the weight stay the same throughout the curve. If we force the sum of the weight to be 1, we can write down, 1 = \\sum_{i=0}^{n} Weight_i(t) For B\u00e9zier curve, we can introduce the Bernstein polynomial in the following way, 1 = (1 - t + t)^n = \\sum_{i=0}^{n} B_{n}^{i}(t) So the weight is defined as Bernstein polynomial.","title":"B\u00e9zier Curve"},{"location":"09/#b-spline-curve","text":"The B-spline is also based on the weight-control-point idea. In B\u00e9zier curve, since at any given point, the Bernstein weight is not zero, except for the starting and ending point, so if you make any adjustment to any control point, the whole curve will be affected. This is not always desirable. B-spline Curve address the issue by introducing a knot vector. The knot vector is a vector that defines the weight of the control points. The B-spline curve is defined as, Curve(t) = \\sum_{i=0}^{n-1} CP_i \\cdot N_{i, p}(t) Where N_{i, p}(t) is the B-spline basis function. It is defined as, N_{i, p}(t) = \\frac{t - t_i}{t_{i+p} - t_i} N_{i, p-1}(t) + \\frac{t_{i+p+1} - t}{t_{i+p+1} - t_{i+1}} N_{i+1, p-1}(t) Woa, that's scary. But don't be afraid, it is actually quite simple. The basic idea is that the instead of using the Bernstein polynomial, the B-spline function only spans a few control points, and evaluated zero outside the span. The number of points in the span is commensurate degree of the B-spline function, that is the p in the N_{i, p}(t) . Let's suppose we have some CP_i , and we want CP_i to have the greatest influence at t_{i+1} . Considering the weight restriction, we make the following restriction on the function we construct, N_{i, p}(t) , should be zero outside the span of CP_i . should be one at the corresponding t_{i+1} for a given CP_i . sum of all N_{i, p}(t) over i should be one. In the following, when encountering zero divided by zero, we can just take the limit as its value- it will always have one. However, actually, the values are always zero for such cases. For the zero degree, we only consider the sole point CP_i . It is easy to define the weights as, one for a certain segment of t and zero for the rest. N_{i, 0}(t) = \\begin{cases} 1 & \\text{if} \\space t_i \\leq t < t_{i+1} \\\\ 0 & \\text{otherwise} \\end{cases} So this is just a bunch of control points. Then, we increase the degree- the span. That is, we make one control point CP_i influence the next one control point, P_{i+1} . So we have, N_{i, 1}(t) = \\begin{cases} \\frac{t - t_i}{t_{i+1} - t_i} & \\text{if} \\space t_i \\leq t < t_{i+1} \\\\ \\frac{t_{i+1} - t}{t_{i+2} - t_{i+1}} & \\text{if} \\space t_{i+1} \\leq t < t_{i+2} \\\\ 0 & \\text{otherwise} \\end{cases} This function will form a triangle spike at t_{i+1} , spanning from t_{i} to t_{i+2} . The one-zero restriction is satisfied, we check the sum now. N_{i, 1}(t) + N_{i + 1, 1}(t) \\\\ =\\frac{ t - t_{i + 1}}{t_{i + 2} - t_{i + 1}} + \\frac{t_{i + 2} - t}{t_{i + 2} - t_{i + 1}} = 1 This sum is done for t_{i+1} < t < t_{i+2} . Then, we increase the degree again. We make the control point CP_i influence the next two control points. This time, instead of interpolating the t value, we will interpolate the degree function. Like the following, N_{i, 2}(t) = \\frac{t - t_i}{t_{i+1} - t_i} N_{i, 1}(t) + \\frac{t_{i+2} - t}{t_{i+2} - t_{i+1}} N_{i+1, 1}(t) We know that for N_{i, 1}(t) , is is one at t_{i+1} and zero at t_i and t_{i+2} . Similarly, N_{i+1, 1}(t) is one at t_{i+2} and zero at t_{i+1} and t_{i+3} . So for t_{i + 1} , the former term is always one, while the latter stays zero. In addition, if we sum all the N_{i, p}(t) over i , we only have three terms to consider, N_{i, 2}(t) = \\frac{t - t_i}{t_{i+1} - t_i} N_{i, 1}(t) + \\frac{t_{i+2} - t}{t_{i+2} - t_{i+1}} N_{i+1, 1}(t) \\\\ N_{i + 1, 2}(t) = \\frac{t - t_{i+1}}{t_{i+2} - t_{i+1}} N_{i+1, 1}(t) + \\frac{t_{i+3} - t}{t_{i+3} - t_{i+2}} N_{i+2, 1}(t) \\\\ N_{i + 2, 2}(t) = \\frac{t - t_{i+2}}{t_{i+3} - t_{i+2}} N_{i+2, 1}(t) + \\frac{t_{i+4} - t}{t_{i+4} - t_{i+3}} N_{i+3, 1}(t) \\\\ Adding up the three terms, we have, N_{i, 2}(t) + N_{i + 1, 2}(t) + N_{i + 2, 2}(t) = 1 The sum is done for t_{i+2} < t < t_{i+3} . So the remaining terms are zero. Note that, in this calculation, the only assumption is that, N_{i, 1} + N_{i + 1, 1} = 1 . If we take, N_{i, p} = \\frac{t - t_i}{t_{i+p} - t_i} N_{i, p-1} + \\frac{t_{i+p+1} - t}{t_{i+p+1} - t_{i+1}} N_{i+1, p-1} The same calculation can be done for any p , it is just more terms to consider. Here is a simple illustration for that. We can take N_{i, p} and N_{i+1, p} , they will produce N_{i, p+1} and N_{i+1, p+1} . If we iterate through, we simply get all the N_{i, p} . So we can conclude that the weight is always one. The one-zero restriction is more obviously, we just skip the calculation here.","title":"B-spline Curve"},{"location":"09/#producing-bezier-curve","text":"This is a programming tutorial, not purely math. So we need to implement Bezier and B-spline curve. Since their only difference is the weight function, we will only implement B\u00e9zier curve for reference. However, up until now, WebGPU does not directly support B\u00e9zier curve nor B-spline curve- since polygonal shapes are more common in computer graphics. However, we can perform CPU calculation and gather the lines, then draw the lines with WebGPU. That's enough rambling, let's implement the B\u00e9zier curve. Let's review how to set up WebGPU again- this will be, sadly, the last time we do this in the series. First, request an adapter, from which we then request a device. The adapter is obtained via navigator.gpu.requestAdapter() . const requestDevice = async (): Promise<[GPUAdapter, GPUDevice] | null> => { const adapter = await navigator.gpu.requestAdapter(); if (!adapter) { console.error('WebGPU not supported'); return null; } const device = await adapter.requestDevice(); console.log(device); return [adapter, device]; } Then, we set the canvas and the context. The context is obtained via canvas.getContext('webgpu') . const getContext = async (device: GPUDevice): Promise<[GPUCanvasContext, GPUTextureFormat]> => { const canvas = document.getElementById('app') as HTMLCanvasElement; const context = canvas.getContext(\"webgpu\")!; const canvasFormat = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device: device, format: canvasFormat, }); return [context, canvasFormat]; } Then, we set up the shader module. The vertex shader now takes a t parameter, and gives the position of the point on the curve. The fragment shader is the same as before. We suppose we will have a b\u00e9zier curve with, Curve(t) = (-0.5, -0.5) B_4^0(t) + (0.5, -0.5) B_4^1(t) + (0.5, 0.5) B_4^2(t) + (-0.5, 0.5) B_4^3(t) + (-0.5, -0.5) B_4^4(t) So we have the following code, const getShaderModule = async (device: GPUDevice): Promise<GPUShaderModule> => { return device.createShaderModule({ label: \"shader\", code: ` fn factorial(n: i32) -> i32 { var result: i32 = 1; for (var i: i32 = 1; i <= n; i = i + 1) { result = result * i; } return result; } fn comb(n: i32, k: i32) -> i32 { return factorial(n) / (factorial(k) * factorial(n - k)); } @vertex fn vertexMain(@location(0) t: f32) -> @builtin(position) vec4<f32> { // calculate all the bezier functions var b40 = f32(comb(4, 0)) * pow(1.0 - t, 4 - 0) * pow(t, 0); var b41 = f32(comb(4, 1)) * pow(1.0 - t, 4 - 1) * pow(t, 1); var b42 = f32(comb(4, 2)) * pow(1.0 - t, 4 - 2) * pow(t, 2); var b43 = f32(comb(4, 3)) * pow(1.0 - t, 4 - 3) * pow(t, 3); var b44 = f32(comb(4, 4)) * pow(1.0 - t, 4 - 4) * pow(t, 4); // control points are (-0.5, -0.5), (-0.5, 0.5), (0.5, 0.5), (0.5, -0.5), (-0.5, -0.5) var p0 = vec2<f32>(-0.5, -0.5); var p1 = vec2<f32>(-0.5, 0.5); var p2 = vec2<f32>(0.5, 0.5); var p3 = vec2<f32>(0.5, -0.5); var p4 = vec2<f32>(-0.5, -0.5); var res = vec2<f32>(0.0, 0.0); res = res + b40 * p0; res = res + b41 * p1; res = res + b42 * p2; res = res + b43 * p3; res = res + b44 * p4; return vec4<f32>(res, 0.0, 1.0); } @fragment fn fragmentMain() -> @location(0) vec4<f32> { return vec4<f32>(1.0, 1.0, 1.0, 1.0); } ` }); } Then the vertex buffer, const getVertexBuffer = async (device: GPUDevice): Promise<[GPUBuffer, GPUVertexBufferLayout]> => { const totalPoints = 100; const t = new Float32Array( Array.from( new Array(totalPoints - 1), (_, i) => i ).flatMap((i) => [i / totalPoints, (i + 1) / totalPoints]) ); const tBufferLayout: GPUVertexBufferLayout = { attributes: [ { shaderLocation: 0, offset: 0, format: \"float32\" } ], arrayStride: 4, stepMode: \"vertex\" }; const tBuffer = device.createBuffer({ size: t.byteLength, usage: GPUBufferUsage.VERTEX, mappedAtCreation: true }); new Float32Array(tBuffer.getMappedRange()).set(t); tBuffer.unmap(); return [tBuffer, tBufferLayout]; } Then, the encoder, render pass, pipeline- everything, const main = async () => { const [_, device] = (await requestDevice())!; const [context, format] = await getContext(device); const shaderModule = await getShaderModule(device); const [tBuffer, tBufferLayout] = await getVertexBuffer(device); const encoder = device.createCommandEncoder(); const renderPassDescriptor: GPURenderPassDescriptor = { colorAttachments: [{ view: context.getCurrentTexture().createView(), clearValue: { r: 0.1, g: 0.1, b: 0.1, a: 1.0 }, storeOp: \"store\", loadOp: \"clear\", }], } const passEncoder = encoder.beginRenderPass(renderPassDescriptor); const pipeline = device.createRenderPipeline({ vertex: { module: shaderModule, entryPoint: \"vertexMain\", buffers: [tBufferLayout] }, fragment: { module: shaderModule, entryPoint: \"fragmentMain\", targets: [{ format: format }] }, primitive: { topology: \"line-list\", }, layout: \"auto\" }); passEncoder.setPipeline(pipeline); passEncoder.setVertexBuffer(0, tBuffer); passEncoder.draw(200); passEncoder.end(); device.queue.submit([encoder.finish()]); } Now, you can see the B\u00e9zier curve on the screen. The curve is defined by the control points, and the curve is evaluated at the given parameter t .","title":"Producing B\u00e9zier Curve"},{"location":"09/#surface","text":"We have understood two kinds of curves- now the surface is just a combination of two curves. The surface is defined as, Surface(u, v) = \\sum_{i=0}^{n} \\sum_{j=0}^{m} CP_{i, j} \\cdot Weight_{i, j}(u, v) The weight can simply be defined as the product of the weights of two base functions. That is, if you like, you can write that as, Surface(u, v) = \\sum_{i=0}^{n} Weight_{i}(u) \\sum_{j=0}^{m} CP_{i, j} \\cdot Weight_{j}(v) Please note that the result of the following part \\sum_{j=0}^{m} CP_{i, j} \\cdot Weight_{j}(v) is a point. So if we fix a v , we get a curve. Since u and v are interchangeable, we can conclude that if we fix a u , we also get a curve. If we choose both B\u00e9zier curve for u and v , we have a B\u00e9zier surface. If we choose B-spline curve for u and v , we have a B-spline surface. So, surface is just a combination of two curves- we have drawn the curves, we can draw the surface. But since drawing 3D things requires us to, ugh, again, deal with all the perspective projection, lighting, and so on, we will not draw the surface with code in this tutorial.","title":"Surface"},{"location":"09/#conclusion","text":"Okay, so this whole series ends. We have learned about the basic concepts of computer graphics, WebGPU, and some advanced topics like B\u00e9zier curve and B-spline curve. We have also learned about the shader language, WGSL, and how to load 3D models. Hope it is helpful for you.","title":"Conclusion"}]}